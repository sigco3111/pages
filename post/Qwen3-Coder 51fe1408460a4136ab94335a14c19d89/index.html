<!DOCTYPE html><html lang="ko" data-astro-cid-ztig7rse> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>상세</title><link rel="icon" href="/pages/favicon.svg" type="image/svg+xml"><link rel="icon" href="/pages/favicon-32x32.png" sizes="32x32"><link rel="apple-touch-icon" href="/pages/apple-touch-icon.png" sizes="180x180"><style>:root{color-scheme:light dark}body{margin:0;font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif}.wrap[data-astro-cid-ztig7rse]{max-width:860px;margin:0 auto;padding:20px}.topbar[data-astro-cid-ztig7rse]{position:sticky;top:0;backdrop-filter:blur(6px);background:color-mix(in oklab,canvas,transparent 35%);border-bottom:1px solid color-mix(in oklab,canvastext,transparent 90%);z-index:10}.topbar[data-astro-cid-ztig7rse] .inner[data-astro-cid-ztig7rse]{display:flex;align-items:center;gap:8px;padding:10px 20px;max-width:860px;margin:0 auto}.btn[data-astro-cid-ztig7rse]{appearance:none;border:1px solid color-mix(in oklab,canvastext,transparent 85%);background:transparent;color:inherit;border-radius:10px;padding:8px 12px;cursor:pointer;font-size:14px}.btn[data-astro-cid-ztig7rse].primary{background:#111827;color:#fff;border-color:#111827}@media (prefers-color-scheme: dark){.btn[data-astro-cid-ztig7rse].primary{background:#e5e7eb;color:#111827;border-color:#e5e7eb}}.hero[data-astro-cid-ztig7rse]{margin:14px 0 8px;display:none}.hero[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{width:100%;height:auto;border-radius:12px;display:block;background:#f3f4f6}article[data-astro-cid-ztig7rse]{line-height:1.72;font-size:16px}article[data-astro-cid-ztig7rse] :is(h1,h2,h3)[data-astro-cid-ztig7rse]{line-height:1.25;margin:24px 0 10px}article[data-astro-cid-ztig7rse] h1[data-astro-cid-ztig7rse]{font-size:28px}article[data-astro-cid-ztig7rse] h2[data-astro-cid-ztig7rse]{font-size:22px}article[data-astro-cid-ztig7rse] h3[data-astro-cid-ztig7rse]{font-size:18px}article[data-astro-cid-ztig7rse] p[data-astro-cid-ztig7rse]{margin:10px 0}article[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{max-width:100%;height:auto;border-radius:8px;background:#f3f4f6}article[data-astro-cid-ztig7rse] pre[data-astro-cid-ztig7rse]{overflow:auto;padding:14px;border:1px solid color-mix(in oklab,canvastext,transparent 90%);border-radius:10px;background:color-mix(in oklab,canvastext,transparent 96%)}article[data-astro-cid-ztig7rse] code[data-astro-cid-ztig7rse]:not(pre code){background:color-mix(in oklab,canvastext,transparent 94%);padding:2px 6px;border-radius:6px}article[data-astro-cid-ztig7rse] blockquote[data-astro-cid-ztig7rse]{border-left:3px solid #9CA3AF;margin:8px 0;padding:4px 12px;color:#6b7280}.actions[data-astro-cid-ztig7rse]{display:flex;gap:8px;flex-wrap:wrap;margin:12px 0 18px}
</style></head> <body class="container" style="padding:24px;max-width:900px" data-astro-cid-ztig7rse> <div class="topbar" data-astro-cid-ztig7rse> <div class="inner" data-astro-cid-ztig7rse> <a class="btn" href="/pages/" aria-label="홈으로" data-astro-cid-ztig7rse>← 홈</a> <a class="btn" id="source" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>원문 보기</a> </div> </div> <div class="wrap" data-astro-cid-ztig7rse> <h1 style="margin:10px 0 6px" data-astro-cid-ztig7rse></h1> <div class="hero" id="hero" data-astro-cid-ztig7rse><img alt="" id="heroImg" loading="eager" data-astro-cid-ztig7rse></div> <div class="actions" data-astro-cid-ztig7rse> <a class="btn primary" id="ctaSource" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>원문 바로가기</a> </div> <article data-astro-cid-ztig7rse> <h1 id="qwen3-coder">Qwen3-Coder</h1>
<p>발견일: 2025/07/24
원문 URL: <a href="https://github.com/QwenLM/Qwen3-Coder">https://github.com/QwenLM/Qwen3-Coder</a>
분류: 오픈소스
원문 Source: 🔗github
즐겨찾기: No</p>
<p><a href="https://opengraph.githubassets.com/feff673376d465a16cb6bb5a55fec3592b0fce95d6fd14a5ce8bb4ef2ce42ce3/QwenLM/Qwen3-Coder"></a></p>
<p>출처1 : <a href="https://github.com/QwenLM/Qwen3-Coder">https://github.com/QwenLM/Qwen3-Coder</a></p>
<p>출처2 : <a href="https://news.hada.io/topic?id=22129">https://news.hada.io/topic?id=22129</a></p>
<h2 id="설명">설명</h2>
<p><code>Qwen3-Coder</code>는 알리바바 클라우드의 Qwen 팀이 개발한 Qwen3 대형 언어 모델 시리즈의 코딩 특화 버전입니다. 이 모델은 코딩 및 에이전트 기반 작업에 최적화되어 있으며, 특히 <strong>Qwen3-Coder-480B-A35B-Instruct</strong>는 4800억 개 파라미터와 350억 개 활성 파라미터를 가진 Mixture-of-Experts(MoE) 모델로, 오픈소스 모델 중 에이전트 코딩, 브라우저 사용, 도구 활용에서 최상위 성능을 달성했습니다. Apache 2.0 라이선스 하에 오픈 웨이트로 제공되며, 256K 토큰의 긴 컨텍스트를 기본 지원하고 Yarn을 통해 최대 1M 토큰까지 확장 가능합니다.</p>
<h2 id="주요-특징">주요 특징</h2>
<h3 id="1-성능">1. 성능</h3>
<ul>
<li><strong>에이전트 코딩</strong>: Qwen3-Coder-480B-A35B-Instruct는 SWE-Bench Verified에서 Claude Sonnet 4와 유사한 성능(69.6% 검증 정확도, 단일 샷 67.0%)을 달성하며, 오픈소스 모델 중 최고 성능을 기록.</li>
<li><strong>코딩 언어 지원</strong>: 358개 프로그래밍 언어 지원(예: Python, C, C++, Java, JavaScript, Go 등).</li>
<li><strong>수학 및 일반 능력</strong>: Qwen3의 기본 모델에서 수학 및 일반 추론 능력을 유지.</li>
</ul>
<h3 id="2-긴-컨텍스트-처리">2. 긴 컨텍스트 처리</h3>
<ul>
<li><strong>기본</strong>: 256K 토큰 컨텍스트 지원.</li>
<li><strong>확장</strong>: Yarn을 통해 최대 1M 토큰 처리, 대규모 코드베이스 및 동적 데이터(예: 풀 리퀘스트) 처리에 최적화.</li>
</ul>
<h3 id="3-에이전트-기능">3. 에이전트 기능</h3>
<ul>
<li><strong>도구 호출</strong>: <code>qwen3coder_tool_parser.py</code>를 통해 최적화된 함수 호출 파서 제공, Qwen Code 및 CLINE과 통합.</li>
<li><strong>워크플로우 자동화</strong>: 풀 리퀘스트 처리, 리베이스 등 운영 작업 자동화 지원.</li>
</ul>
<h3 id="4-오픈소스">4. 오픈소스</h3>
<ul>
<li><strong>라이선스</strong>: Apache 2.0, 상업적 및 연구 용도에 자유롭게 사용 가능.</li>
<li><strong>배포</strong>: Hugging Face 및 ModelScope에서 모델 가중치 제공.</li>
</ul>
<h2 id="설치-및-사용">설치 및 사용</h2>
<h3 id="1-요구사항">1. 요구사항</h3>
<ul>
<li><strong>Python</strong>: 3.10 이상.</li>
<li><strong>PyTorch</strong>: 2.6 이상.</li>
<li><strong>Transformers</strong>: 최신 버전(4.51.0 이상) 권장.</li>
<li><strong>하드웨어</strong>: Qwen3-Coder-480B-A35B-Instruct는 고성능 GPU(예: A100) 필요, FP8 버전은 메모리 효율성 개선.</li>
</ul>
<h3 id="2-설치">2. 설치</h3>
<ol>
<li>
<p>저장소 클론:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">git</span><span style="color:#9ECBFF"> clone</span><span style="color:#F97583"> &#x3C;</span><span style="color:#9ECBFF">https://github.com/QwenLM/Qwen3-Coder.gi</span><span style="color:#E1E4E8">t</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#79B8FF">cd</span><span style="color:#9ECBFF"> Qwen3-Coder</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
</li>
<li>
<p>의존성 설치:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> transformers</span><span style="color:#9ECBFF"> torch</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
</li>
<li>
<p>모델 및 토크나이저 로드:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> transformers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">model_name </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> "Qwen/Qwen3-Coder-480B-A35B-Instruct"</span></span>
<span class="line"><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoModelForCausalLM.from_pretrained(model_name, </span><span style="color:#FFAB70">device_map</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">tokenizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
</li>
</ol>
<h3 id="3-사용-예시">3. 사용 예시</h3>
<h3 id="1-코드-작성">1) 코드 작성</h3>
<p>퀵소트 알고리즘 생성 예시:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">prompt </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> "write a quick sort algorithm."</span></span>
<span class="line"><span style="color:#E1E4E8">messages </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: prompt}]</span></span>
<span class="line"><span style="color:#E1E4E8">text </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer.apply_chat_template(messages, </span><span style="color:#FFAB70">tokenize</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">add_generation_prompt</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">model_inputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer([text], </span><span style="color:#FFAB70">return_tensors</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"pt"</span><span style="color:#E1E4E8">).to(model.device)</span></span>
<span class="line"><span style="color:#E1E4E8">generated_ids </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generate(</span><span style="color:#F97583">**</span><span style="color:#E1E4E8">model_inputs, </span><span style="color:#FFAB70">max_new_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">65536</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer.batch_decode(generated_ids, </span><span style="color:#FFAB70">skip_special_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(response)</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
<h3 id="2-코드-완성-fill-in-the-middle">2) 코드 완성 (Fill-in-the-Middle)</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">input_text </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> """def quicksort(arr):</span></span>
<span class="line"><span style="color:#9ECBFF">    if len(arr) middle = [x for x in arr if x == pivot]</span></span>
<span class="line"><span style="color:#9ECBFF">    right = [x for x in arr if x > pivot]</span></span>
<span class="line"><span style="color:#9ECBFF">    return quicksort(left) + middle + quicksort(right)"""</span></span>
<span class="line"><span style="color:#E1E4E8">messages </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"system"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"You are a code completion assistant."</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: input_text}</span></span>
<span class="line"><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">text </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer.apply_chat_template(messages, </span><span style="color:#FFAB70">tokenize</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">add_generation_prompt</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">model_inputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer([text], </span><span style="color:#FFAB70">return_tensors</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"pt"</span><span style="color:#E1E4E8">).to(model.device)</span></span>
<span class="line"><span style="color:#E1E4E8">generated_ids </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generate(model_inputs.input_ids, </span><span style="color:#FFAB70">max_new_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">do_sample</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">)[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">output_text </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer.decode(generated_ids[</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(model_inputs.input_ids[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]):], </span><span style="color:#FFAB70">skip_special_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Generated text: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">output_text</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
<h2 id="프로젝트-구조">프로젝트 구조</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Qwen3-Coder/</span></span>
<span class="line"><span>├── qwen3coder_tool_parser.py  # 함수 호출 파서</span></span>
<span class="line"><span>├── docs/                     # 문서</span></span>
<span class="line"><span>├── examples/                 # 예제 코드</span></span>
<span class="line"><span>├── tests/                    # 테스트 파일</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span></code></pre>
<h2 id="한계">한계</h2>
<ul>
<li><strong>비사고 모드</strong>: Qwen3-Coder-480B-A35B-Instruct는 비사고 모드만 지원하며, <code>...</code> 블록 생성 불가.</li>
<li><strong>리소스 요구</strong>: 대형 모델은 고성능 GPU 필요.</li>
<li><strong>API 비용</strong>: 다중 API 호출로 인해 토큰 사용량 증가 가능.</li>
</ul>
<h2 id="기여-방법">기여 방법</h2>
<ul>
<li><strong>문서</strong>: <code>CONTRIBUTING.md</code> 참조.</li>
<li><strong>이슈 및 PR</strong>: GitHub 이슈를 통해 피드백 제출 및 풀 리퀘스트로 기여.</li>
<li><strong>커뮤니티</strong>: Discord 및 WeChat 그룹을 통해 논의.</li>
</ul>
<h2 id="참고-자료">참고 자료</h2>
<ul>
<li><strong>공식 저장소</strong>: <a href="https://github.com/QwenLM/Qwen3-Coder">https://github.com/QwenLM/Qwen3-Coder</a></li>
<li><strong>Hugging Face</strong>: <a href="https://huggingface.co/Qwen">https://huggingface.co/Qwen</a></li>
<li><strong>ModelScope</strong>: <a href="https://modelscope.cn/">https://modelscope.cn</a></li>
<li><strong>기술 보고서</strong>: <a href="https://arxiv.org/abs/2505.09388">https://arxiv.org/abs/2505.09388</a></li>
<li><strong>블로그</strong>: <a href="https://qwenlm.github.io/">https://qwenlm.github.io</a></li>
</ul>
<h2 id="결론">결론</h2>
<p>Qwen3-Coder는 Qwen3 시리즈의 코딩 특화 모델로, 에이전트 코딩, 긴 컨텍스트 처리, 다중 언어 지원에서 탁월한 성능을 제공합니다. Qwen Code CLI와 통합되어 개발자 워크플로우를 효율화하며, 오픈소스 특성으로 커뮤니티 기여를 장려합니다. 대규모 코드베이스 관리와 복잡한 작업 자동화에 이상적이며, 지속적인 업데이트로 성능이 개선되고 있습니다.</p> </article> </div> <script type="module">
      // 목적: index.json에서 현재 글 메타/썸네일을 찾아 상세 화면에 반영한다.
      async function hydrateMeta() {
        try {
          const BASE = import.meta.env.BASE_URL;
          const slug = decodeURIComponent(location.pathname.replace(/.*\/post\//,'').replace(/\/?$/,''));
          const res = await fetch(`${BASE}index.json`);
          const data = await res.json();
          const items = (data && data.items) || [];
          const item = items.find((i) => i.slug === slug);
          if (!item) return;

          const hero = document.getElementById('hero');
          const heroImg = document.getElementById('heroImg');
          const source = document.getElementById('source');
          const cta = document.getElementById('ctaSource');
          if (item.thumbnail && hero && heroImg) {
            heroImg.setAttribute('src', item.thumbnail);
            hero.style.display = 'block';
          }
          if (item.source_url && source && cta) {
            source.setAttribute('href', item.source_url);
            cta.setAttribute('href', item.source_url);
            source.style.display='inline-block';
            cta.style.display='inline-block';
          }
        } catch {}
      }
      hydrateMeta();

      // 복사 버튼 제거됨 — 상단에 원문 보기 버튼만 유지
    </script> </body> </html>