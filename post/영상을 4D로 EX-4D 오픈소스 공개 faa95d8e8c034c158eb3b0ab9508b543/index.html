<!DOCTYPE html><html lang="ko" data-astro-cid-ztig7rse> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>ìƒì„¸</title><link rel="icon" href="/pages/favicon.svg" type="image/svg+xml"><link rel="icon" href="/pages/favicon-32x32.png" sizes="32x32"><link rel="apple-touch-icon" href="/pages/apple-touch-icon.png" sizes="180x180"><style>:root{color-scheme:light dark}body{margin:0;font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif}.wrap[data-astro-cid-ztig7rse]{max-width:860px;margin:0 auto;padding:20px}.topbar[data-astro-cid-ztig7rse]{position:sticky;top:0;backdrop-filter:blur(6px);background:color-mix(in oklab,canvas,transparent 35%);border-bottom:1px solid color-mix(in oklab,canvastext,transparent 90%);z-index:10}.topbar[data-astro-cid-ztig7rse] .inner[data-astro-cid-ztig7rse]{display:flex;align-items:center;gap:8px;padding:10px 20px;max-width:860px;margin:0 auto}.btn[data-astro-cid-ztig7rse]{appearance:none;border:1px solid color-mix(in oklab,canvastext,transparent 85%);background:transparent;color:inherit;border-radius:10px;padding:8px 12px;cursor:pointer;font-size:14px}.btn[data-astro-cid-ztig7rse].primary{background:#111827;color:#fff;border-color:#111827}@media (prefers-color-scheme: dark){.btn[data-astro-cid-ztig7rse].primary{background:#e5e7eb;color:#111827;border-color:#e5e7eb}}.hero[data-astro-cid-ztig7rse]{margin:14px 0 8px;display:none}.hero[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{width:100%;height:auto;border-radius:12px;display:block;background:#f3f4f6}article[data-astro-cid-ztig7rse]{line-height:1.72;font-size:16px}article[data-astro-cid-ztig7rse] :is(h1,h2,h3)[data-astro-cid-ztig7rse]{line-height:1.25;margin:24px 0 10px}article[data-astro-cid-ztig7rse] h1[data-astro-cid-ztig7rse]{font-size:28px}article[data-astro-cid-ztig7rse] h2[data-astro-cid-ztig7rse]{font-size:22px}article[data-astro-cid-ztig7rse] h3[data-astro-cid-ztig7rse]{font-size:18px}article[data-astro-cid-ztig7rse] p[data-astro-cid-ztig7rse]{margin:10px 0}article[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{max-width:100%;height:auto;border-radius:8px;background:#f3f4f6}article[data-astro-cid-ztig7rse] pre[data-astro-cid-ztig7rse]{overflow:auto;padding:14px;border:1px solid color-mix(in oklab,canvastext,transparent 90%);border-radius:10px;background:color-mix(in oklab,canvastext,transparent 96%)}article[data-astro-cid-ztig7rse] code[data-astro-cid-ztig7rse]:not(pre code){background:color-mix(in oklab,canvastext,transparent 94%);padding:2px 6px;border-radius:6px}article[data-astro-cid-ztig7rse] blockquote[data-astro-cid-ztig7rse]{border-left:3px solid #9CA3AF;margin:8px 0;padding:4px 12px;color:#6b7280}.actions[data-astro-cid-ztig7rse]{display:flex;gap:8px;flex-wrap:wrap;margin:12px 0 18px}
</style></head> <body class="container" style="padding:24px;max-width:900px" data-astro-cid-ztig7rse> <div class="topbar" data-astro-cid-ztig7rse> <div class="inner" data-astro-cid-ztig7rse> <a class="btn" href="/pages/" aria-label="í™ˆìœ¼ë¡œ" data-astro-cid-ztig7rse>â† í™ˆ</a> <a class="btn" id="source" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>ì›ë¬¸ ë³´ê¸°</a> </div> </div> <div class="wrap" data-astro-cid-ztig7rse> <h1 style="margin:10px 0 6px" data-astro-cid-ztig7rse></h1> <div class="hero" id="hero" data-astro-cid-ztig7rse><img alt="" id="heroImg" loading="eager" data-astro-cid-ztig7rse></div> <div class="actions" data-astro-cid-ztig7rse> <a class="btn primary" id="ctaSource" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>ì›ë¬¸ ë°”ë¡œê°€ê¸°</a> </div> <article data-astro-cid-ztig7rse> <h1 id="ì˜ìƒì„-4dë¡œ-ex-4d-ì˜¤í”ˆì†ŒìŠ¤-ê³µê°œ">ì˜ìƒì„ 4Dë¡œ EX-4D ì˜¤í”ˆì†ŒìŠ¤ ê³µê°œ</h1>
<p>ë°œê²¬ì¼: 2025/07/08
ì›ë¬¸ URL: <a href="https://github.com/tau-yihouxiang/EX-4D">https://github.com/tau-yihouxiang/EX-4D</a>
ë¶„ë¥˜: ì˜¤í”ˆì†ŒìŠ¤
ì›ë¬¸ Source: ğŸ”—github
ì¦ê²¨ì°¾ê¸°: No</p>
<p><a href="https://opengraph.githubassets.com/2f3fe93325ec8e6d2c7151c068a08c6b7ac6b334b4b5e5dce97b699b839e68ca/tau-yihouxiang/EX-4D"></a></p>
<h1 id="ex-4d-extreme-viewpoint-4d-video-synthesis-via-depth-watertight-mesh">EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh</h1>
<p>EX-4D: ì‹¬ë„ ë°©ìˆ˜ ë©”ì‹œë¥¼ í†µí•œ EXtreme Viewpoint 4D ë¹„ë””ì˜¤ í•©ì„±</p>
<p><img src="https://github.com/tau-yihouxiang/EX-4D/raw/main/docs/Logo.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2506.05554">ğŸ“„ Paper</a> | <a href="https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html">ğŸ¥ Homepage</a> | <a href="https://github.com/tau-yihouxiang/EX-4D">ğŸ’» Code</a>
<a href="https://arxiv.org/abs/2506.05554">ğŸ“„ ì¢…ì´</a> | <a href="https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html">ğŸ¥ í™ˆí˜ì´ì§€</a> | <a href="https://github.com/tau-yihouxiang/EX-4D">ğŸ’» ì½”ë“œ</a></p>
<h2 id="-highlights--í•˜ì´ë¼ì´íŠ¸">ğŸŒŸ Highlights ğŸŒŸ í•˜ì´ë¼ì´íŠ¸</h2>
<ul>
<li><strong>ğŸ¯ Extreme Viewpoint Synthesis</strong>: Generate high-quality 4D videos with camera movements ranging from -90Â° to 90Â°
<strong>ğŸ¯ ê·¹í•œì˜ ì‹œì  í•©ì„±</strong>: -90Â°ì—ì„œ 90Â° ë²”ìœ„ì˜ ì¹´ë©”ë¼ ì›€ì§ì„ìœ¼ë¡œ ê³ í’ˆì§ˆ 4D ë¹„ë””ì˜¤ ìƒì„±</li>
<li><strong>ğŸ”§ Depth Watertight Mesh</strong>: Novel geometric representation that models both visible and occluded regions
<strong>ğŸ”§ Depth Watertight Mesh</strong>: ê°€ì‹œ ì˜ì—­ê³¼ ê°€ë ¤ì§„ ì˜ì—­ì„ ëª¨ë‘ ëª¨ë¸ë§í•˜ëŠ” ìƒˆë¡œìš´ ê¸°í•˜í•™ì  í‘œí˜„</li>
<li><strong>âš¡ Lightweight Architecture</strong>: Only 1% trainable parameters (140M) of the 14B video diffusion backbone
<strong>âš¡ ê²½ëŸ‰ ì•„í‚¤í…ì²˜</strong>: 14B ë¹„ë””ì˜¤ í™•ì‚° ë°±ë³¸ì˜ 1% í•™ìŠµ ê°€ëŠ¥ ë§¤ê°œë³€ìˆ˜(140M)ë§Œ</li>
<li><strong>ğŸ­ No Multi-view Training</strong>: Innovative masking strategy eliminates the need for expensive multi-view datasets
<strong>ğŸ­ ë©€í‹°ë·° êµìœ¡ ì—†ìŒ</strong>: í˜ì‹ ì ì¸ ë§ˆìŠ¤í‚¹ ì „ëµìœ¼ë¡œ ê°’ë¹„ì‹¼ ë©€í‹°ë·° ë°ì´í„° ì„¸íŠ¸ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</li>
<li><strong>ğŸ† State-of-the-art Performance</strong>: Outperforms existing methods, especially on extreme camera angles
<strong>ğŸ† ìµœì²¨ë‹¨ ì„±ëŠ¥</strong>: íŠ¹íˆ ê·¹í•œì˜ ì¹´ë©”ë¼ ê°ë„ì—ì„œ ê¸°ì¡´ ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.</li>
</ul>
<h2 id="-demo-results--ë°ëª¨-ê²°ê³¼">ğŸ¬ Demo Results ğŸ¬ ë°ëª¨ ê²°ê³¼</h2>
<p><img src="https://github.com/tau-yihouxiang/EX-4D/raw/main/docs/teaser.png" alt=""></p>
<p><em>EX-4D transforms monocular videos into camera-controllable 4D experiences with physically consistent results under extreme viewpoints.</em>
<em>EX-4DëŠ” ë‹¨ì•ˆ ë¹„ë””ì˜¤ë¥¼ ì¹´ë©”ë¼ë¡œ ì œì–´í•  ìˆ˜ ìˆëŠ” 4D ê²½í—˜ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê·¹í•œì˜ ì‹œì ì—ì„œ ë¬¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ëœ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</em></p>
<h2 id="ï¸-framework-overview-ï¸-í”„ë ˆì„ì›Œí¬-ê°œìš”">ğŸ—ï¸ Framework Overview ğŸ—ï¸ í”„ë ˆì„ì›Œí¬ ê°œìš”</h2>
<p><img src="https://github.com/tau-yihouxiang/EX-4D/raw/main/docs/overview.png" alt=""></p>
<p>Our framework consists of three key components:
ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.</p>
<ol>
<li><strong>ğŸ”º Depth Watertight Mesh Construction</strong>: Creates a robust geometric prior that explicitly models both visible and occluded regions
<strong>ğŸ”º Depth Watertight Mesh Construction</strong>: ê°€ì‹œ ì˜ì—­ê³¼ ê°€ë ¤ì§„ ì˜ì—­ì„ ëª¨ë‘ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ê°•ë ¥í•œ ê¸°í•˜í•™ì  ì‚¬ì „ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤</li>
<li><strong>ğŸ­ Simulated Masking Strategy</strong>: Generates effective training data from monocular videos without multi-view datasets
<strong>ğŸ­ ì‹œë®¬ë ˆì´ì…˜ëœ ë§ˆìŠ¤í‚¹ ì „ëµ</strong>: ë‹¤ì¤‘ ë·° ë°ì´í„° ì„¸íŠ¸ ì—†ì´ ë‹¨ì•ˆ ë¹„ë””ì˜¤ì—ì„œ íš¨ê³¼ì ì¸ í›ˆë ¨ ë°ì´í„° ìƒì„±</li>
<li><strong>âš™ï¸ Lightweight LoRA Adapter</strong>: Efficiently integrates geometric information with pre-trained video diffusion models
<strong>âš™ï¸ ê²½ëŸ‰ LoRA ì–´ëŒ‘í„°</strong>: ê¸°í•˜í•™ì  ì •ë³´ë¥¼ ì‚¬ì „ í›ˆë ¨ëœ ë¹„ë””ì˜¤ í™•ì‚° ëª¨ë¸ê³¼ íš¨ìœ¨ì ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.</li>
</ol>
<h2 id="-quick-start--ë¹ ë¥¸-ì‹œì‘">ğŸš€ Quick Start ğŸš€ ë¹ ë¥¸ ì‹œì‘</h2>
<h3 id="installation-ì„¤ì¹˜">Installation ì„¤ì¹˜</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># Clone the repository</span></span>
<span class="line"><span>git clone https://github.com/tau-yihouxiang/EX-4D.git</span></span>
<span class="line"><span>cd EX-4D</span></span>
<span class="line"><span># Create conda environment</span></span>
<span class="line"><span>conda create -n ex4d python=3.10</span></span>
<span class="line"><span>conda activate ex4d</span></span>
<span class="line"><span># Install PyTorch (2.x recommended)</span></span>
<span class="line"><span>pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124</span></span>
<span class="line"><span># Install Nvdiffrast</span></span>
<span class="line"><span>pip install git+https://github.com/NVlabs/nvdiffrast.git</span></span>
<span class="line"><span># Install dependencies and diffsynth</span></span>
<span class="line"><span>pip install -e .</span></span>
<span class="line"><span># Install depthcrafter for depth estimation. (Follow DepthCrafter's installing instruction for checkpoints preparation.)</span></span>
<span class="line"><span>git clone https://github.com/Tencent/DepthCrafter.git</span></span>
<span class="line"><span></span></span></code></pre>
<h3 id="download-pretrained-model">Download Pretrained Model</h3>
<p>ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI</span></span>
<span class="line"><span>huggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D</span></span>
<span class="line"><span></span></span></code></pre>
<h3 id="example-usage-ì‚¬ìš©-ì˜ˆ">Example Usage ì‚¬ìš© ì˜ˆ</h3>
<h3 id="1-dw-mesh-reconstruction">1. DW-Mesh Reconstruction</h3>
<ol>
<li>DW-ë©”ì‰¬ ì¬êµ¬ì„±</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )</span></span>
<span class="line"><span>python recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh</span></span>
<span class="line"><span></span></span></code></pre>
<h3 id="2-ex-4d-generation-48gb-vram-required">2. EX-4D Generation (48GB VRAM required)</h3>
<ol start="2">
<li>EX-4D ì„¸ëŒ€(48GB VRAM í•„ìš”)</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>python generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4</span></span>
<span class="line"><span></span></span></code></pre>
<p><strong>Input Video</strong></p>
<p><img src="https://github.com/tau-yihouxiang/EX-4D/raw/main/examples/flower/input.gif" alt=""></p>
<p>âœ</p>
<p><strong>Output Video</strong></p>
<p><img src="https://github.com/tau-yihouxiang/EX-4D/raw/main/examples/flower/output.gif" alt=""></p>
<h3 id="user-study-results">User Study Results</h3>
<ul>
<li><strong>70.7%</strong> of participants preferred EX-4D over baseline methods</li>
<li>Superior performance in physical consistency and extreme viewpoint quality</li>
<li>Significant improvement as camera angles become more extreme</li>
</ul>
<h2 id="-applications">ğŸ¯ Applications</h2>
<ul>
<li><strong>ğŸ® Gaming</strong>: Create immersive 3D game cinematics from 2D footage</li>
<li><strong>ğŸ¬ Film Production</strong>: Generate novel camera angles for post-production</li>
<li><strong>ğŸ¥½ VR/AR</strong>: Create free-viewpoint video experiences</li>
<li><strong>ğŸ“± Social Media</strong>: Generate dynamic camera movements for content creation</li>
<li><strong>ğŸ¢ Architecture</strong>: Visualize spaces from multiple viewpoints</li>
</ul>
<h2 id="ï¸-limitations">âš ï¸ Limitations</h2>
<ul>
<li><strong>Depth Dependency</strong>: Performance relies on monocular depth estimation quality</li>
<li><strong>Computational Cost</strong>: Requires significant computation for high-resolution videos</li>
<li><strong>Reflective Surfaces</strong>: Challenges with reflective or transparent materials</li>
</ul>
<h2 id="-future-work">ğŸ”® Future Work</h2>
<ul>
<li>Real-time inference optimization (3DGS / 4DGS)</li>
<li>Support for higher resolutions (1K, 2K)</li>
<li>Neural mesh refinement techniques</li>
</ul>
<h2 id="-acknowledgments">ğŸ™ Acknowledgments</h2>
<p>We would like to thank the <a href="https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1">DiffSynth-Studio v1.1.1</a> project for providing the foundational diffusion framework.</p>
<h2 id="-citation">ğŸ“š Citation</h2>
<p>If you find our work useful, please consider citing:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>@misc{hu2025ex4dextremeviewpoint4d,</span></span>
<span class="line"><span>      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, </span></span>
<span class="line"><span>      author={Tao Hu and Haoyang Peng and Xiao Liu and Yuewen Ma},</span></span>
<span class="line"><span>      year={2025},</span></span>
<span class="line"><span>      eprint={2506.05554},</span></span>
<span class="line"><span>      archivePrefix={arXiv},</span></span>
<span class="line"><span>      primaryClass={cs.CV},</span></span>
<span class="line"><span>      url={https://arxiv.org/abs/2506.05554}, </span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span></code></pre> </article> </div> <script type="module">
      // ëª©ì : index.jsonì—ì„œ í˜„ì¬ ê¸€ ë©”íƒ€/ì¸ë„¤ì¼ì„ ì°¾ì•„ ìƒì„¸ í™”ë©´ì— ë°˜ì˜í•œë‹¤.
      async function hydrateMeta() {
        try {
          const BASE = import.meta.env.BASE_URL;
          const slug = decodeURIComponent(location.pathname.replace(/.*\/post\//,'').replace(/\/?$/,''));
          const res = await fetch(`${BASE}index.json`);
          const data = await res.json();
          const items = (data && data.items) || [];
          const item = items.find((i) => i.slug === slug);
          if (!item) return;

          const hero = document.getElementById('hero');
          const heroImg = document.getElementById('heroImg');
          const source = document.getElementById('source');
          const cta = document.getElementById('ctaSource');
          if (item.thumbnail && hero && heroImg) {
            heroImg.setAttribute('src', item.thumbnail);
            hero.style.display = 'block';
          }
          if (item.source_url && source && cta) {
            source.setAttribute('href', item.source_url);
            cta.setAttribute('href', item.source_url);
            source.style.display='inline-block';
            cta.style.display='inline-block';
          }
        } catch {}
      }
      hydrateMeta();

      // ë³µì‚¬ ë²„íŠ¼ ì œê±°ë¨ â€” ìƒë‹¨ì— ì›ë¬¸ ë³´ê¸° ë²„íŠ¼ë§Œ ìœ ì§€
    </script> </body> </html>