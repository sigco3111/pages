<!DOCTYPE html><html lang="ko" data-astro-cid-ztig7rse> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>상세</title><link rel="icon" href="/pages/favicon.svg" type="image/svg+xml"><link rel="icon" href="/pages/favicon-32x32.png" sizes="32x32"><link rel="apple-touch-icon" href="/pages/apple-touch-icon.png" sizes="180x180"><style>:root{color-scheme:light dark}body{margin:0;font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif}.wrap[data-astro-cid-ztig7rse]{max-width:860px;margin:0 auto;padding:20px}.topbar[data-astro-cid-ztig7rse]{position:sticky;top:0;backdrop-filter:blur(6px);background:color-mix(in oklab,canvas,transparent 35%);border-bottom:1px solid color-mix(in oklab,canvastext,transparent 90%);z-index:10}.topbar[data-astro-cid-ztig7rse] .inner[data-astro-cid-ztig7rse]{display:flex;align-items:center;gap:8px;padding:10px 20px;max-width:860px;margin:0 auto}.btn[data-astro-cid-ztig7rse]{appearance:none;border:1px solid color-mix(in oklab,canvastext,transparent 85%);background:transparent;color:inherit;border-radius:10px;padding:8px 12px;cursor:pointer;font-size:14px}.btn[data-astro-cid-ztig7rse].primary{background:#111827;color:#fff;border-color:#111827}@media (prefers-color-scheme: dark){.btn[data-astro-cid-ztig7rse].primary{background:#e5e7eb;color:#111827;border-color:#e5e7eb}}.hero[data-astro-cid-ztig7rse]{margin:14px 0 8px;display:none}.hero[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{width:100%;height:auto;border-radius:12px;display:block;background:#f3f4f6}article[data-astro-cid-ztig7rse]{line-height:1.72;font-size:16px}article[data-astro-cid-ztig7rse] :is(h1,h2,h3)[data-astro-cid-ztig7rse]{line-height:1.25;margin:24px 0 10px}article[data-astro-cid-ztig7rse] h1[data-astro-cid-ztig7rse]{font-size:28px}article[data-astro-cid-ztig7rse] h2[data-astro-cid-ztig7rse]{font-size:22px}article[data-astro-cid-ztig7rse] h3[data-astro-cid-ztig7rse]{font-size:18px}article[data-astro-cid-ztig7rse] p[data-astro-cid-ztig7rse]{margin:10px 0}article[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{max-width:100%;height:auto;border-radius:8px;background:#f3f4f6}article[data-astro-cid-ztig7rse] pre[data-astro-cid-ztig7rse]{overflow:auto;padding:14px;border:1px solid color-mix(in oklab,canvastext,transparent 90%);border-radius:10px;background:color-mix(in oklab,canvastext,transparent 96%)}article[data-astro-cid-ztig7rse] code[data-astro-cid-ztig7rse]:not(pre code){background:color-mix(in oklab,canvastext,transparent 94%);padding:2px 6px;border-radius:6px}article[data-astro-cid-ztig7rse] blockquote[data-astro-cid-ztig7rse]{border-left:3px solid #9CA3AF;margin:8px 0;padding:4px 12px;color:#6b7280}.actions[data-astro-cid-ztig7rse]{display:flex;gap:8px;flex-wrap:wrap;margin:12px 0 18px}
</style></head> <body class="container" style="padding:24px;max-width:900px" data-astro-cid-ztig7rse> <div class="topbar" data-astro-cid-ztig7rse> <div class="inner" data-astro-cid-ztig7rse> <a class="btn" href="/pages/" aria-label="홈으로" data-astro-cid-ztig7rse>← 홈</a> <a class="btn" id="source" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>원문 보기</a> </div> </div> <div class="wrap" data-astro-cid-ztig7rse> <h1 style="margin:10px 0 6px" data-astro-cid-ztig7rse></h1> <div class="hero" id="hero" data-astro-cid-ztig7rse><img alt="" id="heroImg" loading="eager" data-astro-cid-ztig7rse></div> <div class="actions" data-astro-cid-ztig7rse> <a class="btn primary" id="ctaSource" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>원문 바로가기</a> </div> <article data-astro-cid-ztig7rse> <h1 id="claude-코드와-함께-gpt-oss를-사용하는-방법">Claude 코드와 함께 gpt-oss를 사용하는 방법</h1>
<p>발견일: 2025/08/11
원문 URL: <a href="https://garysvenson09.medium.com/how-to-use-gpt-oss-with-claude-code-da3cea5c8af8">https://garysvenson09.medium.com/how-to-use-gpt-oss-with-claude-code-da3cea5c8af8</a>
분류: TIP
원문 Source: 🔗garysvenson09.medium
즐겨찾기: No</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1200/0*ha6xWapdDCyD_m5q.png" alt=""></p>
<p><img src="https://miro.medium.com/v2/resize:fill:64:64/1*lZW-IQU2EQXpcFQ3elV9AQ.png" alt=""></p>
<p><a href="https://apidog.com/?source=post_page---byline--da3cea5c8af8---------------------------------------">Gary Svenson</a></p>
<p>Follow</p>
<p>5 min read</p>
<p>2 days ago</p>
<p>7</p>
<p>1</p>
<p><a href="https://medium.com/plans?dimension=post_audio_button&#x26;postId=da3cea5c8af8&#x26;source=upgrade_membership---post_audio_button-----------------------------------------">Listen</a></p>
<p>Share</p>
<p>More</p>
<p>Want to run OpenAI‑compatible, open‑weight models in the familiar <strong>Claude Code</strong> CLI? <strong>GPT-OSS</strong> (20B and 120B) delivers strong coding and reasoning performance, and you can route Claude Code to it through OpenAI‑style endpoints. This guide shows three reliable ways to connect Claude Code to GPT-OSS using Hugging Face Inference Endpoints, OpenRouter, or a LiteLLM proxy. Pick the path that fits your workflow and budget.
친숙한 <strong>Claude Code</strong> CLI에서 OpenAI 호환 오픈 웨이트 모델을 실행하고 싶으신가요? <strong>GPT-OSS</strong>(20B 및 120B)는 강력한 코딩 및 추론 성능을 제공하며 OpenAI 스타일 엔드포인트를 통해 Claude 코드를 라우팅할 수 있습니다. 이 가이드에서는 Hugging Face Inference Endpoints, OpenRouter 또는 LiteLLM 프록시를 사용하여 Claude Code를 GPT-OSS에 연결하는 세 가지 신뢰할 수 있는 방법을 보여줍니다. 작업 흐름과 예산에 맞는 경로를 선택하십시오.</p>
<blockquote>
<p><em>Pro tip: Building and testing APIs while you experiment with GPT-OSS? Use</em> <a href="https://bit.ly/3Teeyxv"><em>Apidog</em></a> <em>— the all-in-one API development platform — to</em> <a href="https://bit.ly/4hKOxQZ">*design</a>,* <a href="https://bit.ly/3AHtaPS">*mock</a>,* <a href="https://bit.ly/4fMLpCJ">*test</a>, and* <a href="https://bit.ly/3Cs56kA"><em>publish docs</em></a> <em>in one place. It pairs nicely with Claude Code so you can iterate on prompts and verify endpoints without switching tools.</em>
<em>전문가 팁: GPT-OSS를 실험하는 동안 API를 구축하고 테스트하시나요?</em> <em>올인원 API 개발 플랫폼인</em> <a href="https://bit.ly/3Teeyxv"><em>Apidog</em></a>을 사용하여 한 곳에서 문서를 <a href="https://bit.ly/4hKOxQZ">*디자인</a>,* <a href="https://bit.ly/3AHtaPS">*모의</a>,* <a href="https://bit.ly/4fMLpCJ"><em>테스트</em></a> <em>및</em> <a href="https://bit.ly/3Cs56kA"><em>게시</em></a>할 <em>수 있습니다. Claude Code와 잘 어울리므로 도구를 전환하지 않고도 프롬프트를 반복하고 엔드포인트를 확인할 수 있습니다.</em></p>
</blockquote>
<h1 id="why-pair-gpt-oss-with-claude-code">Why pair GPT-OSS with Claude Code?</h1>
<p>GPT-OSS를 Claude Code와 페어링하는 이유는 무엇인가요?</p>
<p><strong>GPT-OSS</strong> is Open AI’s open‑weight model family with a 128K context window and an Apache 2.0 license — ideal for private deployments and customization. <strong>Claude Code</strong> (v0.5.3+) provides a fast, conversational CLI for coding. By pointing Claude Code at an OpenAI‑compatible endpoint for GPT-OSS, you keep the UX you like while controlling costs and deployment.
<strong>GPT-OSS</strong>는 128K 컨텍스트 창과 Apache 2.0 라이선스를 갖춘 Open AI의 오픈 웨이트 모델 제품군으로, 프라이빗 배포 및 사용자 지정에 이상적입니다. <strong>Claude Code</strong> (v0.5.3+)는 코딩을 위한 빠른 대화형 CLI를 제공합니다. GPT-OSS용 OpenAI 호환 엔드포인트에서 Claude Code를 가리키면 비용과 배포를 제어하면서 원하는 UX를 유지할 수 있습니다.</p>
<h1 id="prerequisites-필수-구성-요소">Prerequisites 필수 구성 요소</h1>
<p>Make sure you have: 다음이 있는지 확인합니다.</p>
<ul>
<li><strong>Claude Code ≥ 0.5.3</strong>: Check <code>claude --version</code>. Install with <code>pip install claude-code</code> or update with <code>pip install --upgrade claude-code</code>.
<strong>Claude 코드 ≥ 0.5.3</strong>: <code>claude --version</code>을 확인하십시오. <code>pip install claude-code</code>로 설치하거나 <code>pip install --upgrade claude-code</code> .</li>
<li><strong>Hugging Face account</strong>: Create a read/write token at <a href="https://huggingface.co/">huggingface.co</a>.
<strong>Hugging Face 계정</strong>: <a href="https://huggingface.co/">huggingface.co</a> 에서 읽기/쓰기 토큰을 만듭니다.</li>
<li><strong>OpenRouter API key</strong> (for the OpenRouter path): Get one at <a href="https://openrouter.ai/">openrouter.ai</a>.
<strong>OpenRouter API 키</strong>(OpenRouter 경로용): <a href="https://openrouter.ai/">openrouter.ai</a> 에서 가져옵니다.</li>
<li><strong>Python 3.10+ and Docker</strong>: Needed for local runs or LiteLLM.
<strong>Python 3.10+ 및 Docker</strong>: 로컬 실행 또는 LiteLLM에 필요합니다.</li>
<li><strong>Command‑line familiarity</strong>: You’ll set environment variables and run basic commands.
<strong>명령줄 친숙도</strong>: 환경 변수를 설정하고 기본 명령을 실행합니다.</li>
</ul>
<p>Press enter or click to view image in full size</p>
<p><img src="https://miro.medium.com/v2/resize:fit:700/0*ha6xWapdDCyD_m5q.png" alt=""></p>
<h1 id="option-a--selfhost-gptoss-on-hugging-face">Option A — Self‑host GPT‑OSS on Hugging Face</h1>
<p>옵션 A — Hugging Face에서 GPT-OSS 자체 호스팅</p>
<p>Use Hugging Face Inference Endpoints to deploy a private, scalable TGI server with OpenAI compatibility.
Hugging Face Inference Endpoints를 사용하여 OpenAI와 호환되는 확장 가능한 프라이빗 TGI 서버를 배포하세요.</p>
<ol>
<li>Choose a model 1) 모델 선택</li>
</ol>
<hr>
<ol>
<li>Open the GPT‑OSS repo on Hugging Face: <a href="https://huggingface.co/openai/gpt-oss-20b">openai/gpt-oss-20b</a> or <a href="https://huggingface.co/openai/gpt-oss-120b">openai/gpt-oss-120b</a>.
Hugging Face에서 GPT-OSS 리포지토리(<a href="https://huggingface.co/openai/gpt-oss-20b">openai/gpt-oss-20b</a> 또는 <a href="https://huggingface.co/openai/gpt-oss-120b">openai/gpt-oss-120b</a>)를 엽니다.</li>
<li>Accept the Apache 2.0 license.
Apache 2.0 라이선스에 동의합니다.</li>
<li>Optional alternative: <strong>Qwen3-Coder-480B-A35B-Instruct</strong> (<a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct">Qwen/Qwen3-Coder-480B-A35B-Instruct</a>); choose a GGUF build for lighter hardware.
선택적 대안: <strong>Qwen3-Coder-480B-A35B-Instruct</strong>(<a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct">Qwen/Qwen3-Coder-480B-A35B-Instruct</a>); 더 가벼운 하드웨어를 위해 GGUF 빌드를 선택하십시오.</li>
</ol>
<p>Press enter or click to view image in full size</p>
<p><img src="https://miro.medium.com/v2/resize:fit:700/0*hxRmLY-NEeS3L24l.png" alt=""></p>
<ol>
<li>Deploy a Text Generation Inference endpoint</li>
<li>텍스트 생성 추론 엔드포인트 배포</li>
</ol>
<hr>
<ol>
<li>On the model page, select <strong>Deploy</strong> > <strong>Inference Endpoint</strong>.
모델 페이지에서 > <strong>유추 엔드포인트</strong> <strong>배포를</strong> 선택합니다.</li>
<li>Pick the <strong>Text Generation Inference (TGI)</strong> template (≥ v1.4.0).
<strong>TGI(텍스트 생성 유추)</strong> 템플릿(≥ v1.4.0)을 선택합니다.</li>
<li>Enable OpenAI compatibility (check <strong>Enable OpenAI compatibility</strong> or add <code>--enable-openai</code>).
OpenAI 호환성을 활성화합니다(<strong>OpenAI 호환성 활성화</strong> 또는 <code>--enable-openai</code> 추가).</li>
<li>Choose hardware: A10G/CPU for 20B; A100 for 120B. Create the endpoint.
하드웨어 선택: 10B용 A20G/CPU; 100B의 경우 A120. 엔드포인트를 만듭니다.</li>
<li>Gather credentials 3) 자격 증명 수집</li>
</ol>
<hr>
<ol>
<li>When the endpoint is <strong>Running</strong>, note:
엔드포인트가 <strong>실행 중</strong>인 경우 다음을 참고하십시오.</li>
</ol>
<ul>
<li>
<p><strong>ENDPOINT_URL</strong> like <code>https://&#x3C;your-endpoint>.us-east-1.aws.endpoints.huggingface.cloud</code>.</p>
</li>
<li>
<p><strong>ENDPOINT_URL</strong> 좋아요 <code>https://&#x3C;your-endpoint>.us-east-1.aws.endpoints.huggingface.cloud</code> .</p>
</li>
<li>
<p><strong>HF_API_TOKEN</strong> from your Hugging Face account.</p>
</li>
<li>
<p>Hugging Face 계정에서 <strong>HF_API_TOKEN</strong>합니다.</p>
</li>
</ul>
<ol start="2">
<li>
<p>Capture the model ID (e.g., <code>gpt-oss-20b</code> or <code>gpt-oss-120b</code>).</p>
</li>
<li>
<p>모델 ID(예: <code>gpt-oss-20b</code> 또는 <code>gpt-oss-120b</code>)를 캡처합니다.</p>
</li>
<li>
<p>Point Claude Code at your endpoint</p>
</li>
<li>
<p>엔드포인트에서 Claude 코드를 가리킵니다.</p>
</li>
</ol>
<hr>
<ol>
<li>Export environment variables:</li>
<li>환경 변수 내보내기:</li>
</ol>
<p>export ANTHROPIC_BASE_URL=“<a href="https://.us-east-1.aws.endpoints.huggingface.cloud">https://.us-east-1.aws.endpoints.huggingface.cloud</a>”</p>
<p>export ANTHROPIC_AUTH_TOKEN=“hf_xxxxxxxxxxxxxxxxx”
export ANTHROPIC_MODEL=“gpt-oss-20b”  # or gpt-oss-120b</p>
<ol start="2">
<li>Verify the connection:</li>
<li>연결을 확인합니다.</li>
</ol>
<p>claude —model gpt-oss-20b</p>
<p>Claude Code will route to your TGI endpoint via <code>/v1/chat/completions</code> using the OpenAI‑compatible schema.
Claude Code는 OpenAI 호환 스키마를 사용하여 <code>/v1/chat/completions</code>를 통해 TGI 엔드포인트로 라우팅합니다.</p>
<ol>
<li>Cost and scaling 5) 비용 및 확장</li>
</ol>
<hr>
<ul>
<li>Hugging Face Inference Endpoints auto‑scale; monitor usage. A10G is about ~0.60/hour; A100 is about ~3/hour.
Hugging Face 추론 엔드포인트 자동 크기 조정; 사용량을 모니터링합니다. A10G는 약 ~0.60/시간입니다. A100은 약 ~3/시간입니다.</li>
<li>Prefer local? Run TGI via Docker and point Claude Code at it:
현지를 선호하시나요? Docker를 통해 TGI를 실행하고 Claude Code를 가리킵니다.</li>
</ul>
<p>docker run —name tgi -p 8080:80 -e HF_TOKEN=hf_xxxxxxxxxxxxxxxxx ghcr.io/huggingface/text-generation-inference:latest —model-id openai/gpt-oss-20b —enable-openai</p>
<p>Then set <code>ANTHROPIC_BASE_URL="http://localhost:8080"</code>. 그런 다음 <code>ANTHROPIC_BASE_URL="http://localhost:8080"</code> .</p>
<h1 id="option-b--use-openrouter-as-a-managed-proxy">Option B — Use OpenRouter as a managed proxy</h1>
<p>옵션 B — OpenRouter를 관리형 프록시로 사용</p>
<p>Skip infrastructure and access GPT‑OSS through OpenRouter’s unified API.
인프라를 건너뛰고 OpenRouter의 통합 API를 통해 GPT-OSS에 액세스하세요.</p>
<ol>
<li>Create an account and pick a model</li>
<li>계정 생성 및 모델 선택</li>
</ol>
<hr>
<ol>
<li>Sign up at <a href="https://openrouter.ai/">openrouter.ai</a> and copy your API key from <strong>Keys</strong>.
<a href="https://openrouter.ai/">openrouter.ai</a> 에서 가입하고 Keys에서 API 키를 복사<strong>합니다.</strong></li>
<li>Choose a model slug: 모델 슬러그 선택:</li>
</ol>
<ul>
<li>
<p><code>openai/gpt-oss-20b</code></p>
</li>
<li>
<p><code>오픈AI/GPT-OSS-20B</code></p>
</li>
<li>
<p><code>openai/gpt-oss-120b</code></p>
</li>
<li>
<p><code>오픈AI/GPT-OSS-120B</code></p>
</li>
<li>
<p><code>qwen/qwen3-coder-480b</code> (Qwen’s coder model)</p>
</li>
<li>
<p><code>qwen/qwen3-coder-480b</code> (Qwen의 코더 모델)</p>
</li>
</ul>
<p>Press enter or click to view image in full size</p>
<p><img src="https://miro.medium.com/v2/resize:fit:700/0*0N_Qv1JcXiIVudCS.png" alt=""></p>
<ol>
<li>Configure Claude Code 2) Claude 코드 구성</li>
</ol>
<hr>
<ol>
<li>Export variables: 1. 변수 내보내기:</li>
</ol>
<p>export ANTHROPIC_BASE_URL=“<a href="https://openrouter.ai/api/v1">https://openrouter.ai/api/v1</a>”</p>
<p>export ANTHROPIC_AUTH_TOKEN=“or_xxxxxxxxx”
export ANTHROPIC_MODEL=“openai/gpt-oss-20b”</p>
<ol start="2">
<li>Test the connection: 2. 연결을 테스트합니다.</li>
</ol>
<p>claude —model openai/gpt-oss-20b</p>
<p>Claude Code will stream responses via OpenRouter, with built‑in fallback support.
Claude Code는 내장된 대체 지원을 통해 OpenRouter를 통해 응답을 스트리밍합니다.</p>
<ol>
<li>Pricing notes 3) 가격 참고 사항</li>
</ol>
<hr>
<ul>
<li>Approx. costs: ~0.50/M input tokens and ~2.00/M output tokens for GPT‑OSS‑120B; far below many proprietary options.
대략적인 비용: GPT-OSS-120B의 경우 ~0.50/M 입력 토큰 및 ~2.00/M 출력 토큰; 많은 독점 옵션보다 훨씬 낮습니다.</li>
<li>OpenRouter bills per usage; no infrastructure to manage.
OpenRouter는 사용량당 청구됩니다. 관리할 인프라가 없습니다.</li>
</ul>
<h1 id="option-c--route-via-litellm-for-multimodel-workflows">Option C — Route via LiteLLM for multi‑model workflows</h1>
<p>옵션 C — 다중 모델 워크플로를 위해 LiteLLM을 통해 라우팅</p>
<p>Use LiteLLM as a local proxy if you want to hot‑swap between GPT‑OSS, Qwen, or Anthropic models.
GPT-OSS, Qwen 또는 Anthropic 모델 간에 핫스왑하려는 경우 LiteLLM을 로컬 프록시로 사용합니다.</p>
<ol>
<li>Install and configure LiteLLM</li>
<li>LiteLLM 설치 및 구성</li>
</ol>
<hr>
<ol>
<li>Install <a href="https://docs.litellm.ai/">LiteLLM</a>:
<a href="https://docs.litellm.ai/">LiteLLM</a>을 설치합니다.</li>
</ol>
<p>pip install litellm</p>
<ol start="2">
<li>Create <code>litellm.yaml</code>:</li>
<li><code>litellm.yaml</code>을 만듭니다.</li>
</ol>
<p>model_list:</p>
<ul>
<li>
<p>model_name: gpt-oss-20b
litellm_params:
model: openai/gpt-oss-20b
api_key: or_xxxxxxxxx  # OpenRouter key
api_base: <a href="https://openrouter.ai/api/v1">https://openrouter.ai/api/v1</a></p>
</li>
<li>
<p>model_name: qwen3-coder
litellm_params:
model: openrouter/qwen/qwen3-coder
api_key: or_xxxxxxxxx
api_base: <a href="https://openrouter.ai/api/v1">https://openrouter.ai/api/v1</a></p>
</li>
</ul>
<ol start="3">
<li>Start the proxy: 3. 프록시를 시작합니다.</li>
</ol>
<p>litellm —config litellm.yaml</p>
<ol>
<li>Point Claude Code to LiteLLM</li>
<li>Claude 코드를 LiteLLM에 가리키기</li>
</ol>
<hr>
<ol>
<li>Export variables: 1. 변수 내보내기:</li>
</ol>
<p>export ANTHROPIC_BASE_URL=“<a href="http://localhost:4000">http://localhost:4000</a>”</p>
<p>export ANTHROPIC_AUTH_TOKEN=“litellm_master”
export ANTHROPIC_MODEL=“gpt-oss-20b”</p>
<ol start="2">
<li>Confirm it works: 2. 작동하는지 확인합니다.</li>
</ol>
<p>claude —model gpt-oss-20b</p>
<p>LiteLLM forwards to GPT‑OSS via OpenRouter and supports cost logging. Prefer simple‑shuffle routing to avoid incompatibilities with Anthropic providers.
LiteLLM은 OpenRouter를 통해 GPT-OSS로 전달하고 비용 로깅을 지원합니다. Anthropic 공급자와의 비호환성을 방지하기 위해 단순 셔플 라우팅을 선호합니다.</p>
<ol>
<li>Notes 3) 메모</li>
</ol>
<hr>
<ul>
<li>Use simple‑shuffle over latency‑based routing for stability with Anthropic models.
Anthropic 모델의 안정성을 위해 지연 시간 기반 라우팅에 대한 단순 섞기를 사용합니다.</li>
<li>Review LiteLLM logs to track usage and costs.
LiteLLM 로그를 검토하여 사용량 및 비용을 추적합니다.</li>
</ul>
<h1 id="validate-your-setup-설정-검증">Validate your setup 설정 검증</h1>
<p>Open Claude Code and try these checks:
Claude Code를 열고 다음 확인을 시도해 보세요:</p>
<ul>
<li><strong>Code generation 코드 생성</strong></li>
</ul>
<p>claude —model gpt-oss-20b “Write a Python REST API with Flask”</p>
<p>Expected response (example):
예상 응답(예):</p>
<p>from flask import Flask, jsonify</p>
<p>app = Flask(<strong>name</strong>)
@app.route(‘/api’, methods=[‘GET’])
def get_data():
return jsonify({“message”: “Hello from GPT-OSS!”})
if <strong>name</strong> == ‘<strong>main</strong>’:
app.run(debug=True)</p>
<ul>
<li><strong>Codebase analysis 코드베이스 분석</strong></li>
</ul>
<p>claude —model gpt-oss-20b “Summarize src/server.js”</p>
<ul>
<li><strong>Debugging 디버깅</strong></li>
</ul>
<p>claude —model gpt-oss-20b “Debug this buggy Python code: [paste code]“</p>
<h1 id="troubleshooting-문제-해결">Troubleshooting 문제 해결</h1>
<ul>
<li><strong>404 on</strong> <code>**/v1/chat/completions**</code>: Ensure <code>--enable-openai</code> is enabled in TGI (Option A) or verify model availability on OpenRouter (Option B).
<code>**/v1/chat/completions**</code><strong>의 404</strong>: TGI(옵션 A)에서 <code>--enable-openai</code>가 활성화되어 있는지 확인하거나 OpenRouter(옵션 B)에서 모델 가용성을 확인합니다.</li>
<li><strong>Empty responses</strong>: Confirm <code>ANTHROPIC_MODEL</code> exactly matches the model slug (e.g., <code>gpt-oss-20b</code>).
<strong>빈 응답</strong>: 모델 슬러그(예: <code>gpt-oss-20b</code>)와 정확히 일치<code>ANTHROPIC_MODEL</code> 확인합니다.</li>
<li><strong>400 after swapping models</strong>: Set LiteLLM to simple‑shuffle routing (Option C).
<strong>모델 교체 후 400</strong>: LiteLLM을 단순 셔플 라우팅(옵션 C)으로 설정합니다.</li>
<li><strong>Slow first token</strong>: Warm up Hugging Face endpoints with a small prompt after scale‑to‑zero.
<strong>느린 첫 번째 토큰</strong>: 0으로 확장한 후 작은 프롬프트를 사용하여 Hugging Face 엔드포인트를 워밍업합니다.</li>
<li><strong>Claude Code crashes</strong>: Upgrade to ≥ 0.5.3 and recheck environment variables.
<strong>Claude Code 충돌</strong>: ≥ 0.5.3으로 업그레이드하고 환경 변수를 다시 확인하세요.</li>
</ul>
<h1 id="benefits-of-this-setup-이-설정의-이점">Benefits of this setup 이 설정의 이점</h1>
<ul>
<li><strong>Lower cost</strong>: OpenRouter’s pricing is competitive; local TGI can be very cost‑effective once hardware is in place.
<strong>저렴한 비용</strong>: OpenRouter의 가격은 경쟁력이 있습니다. 로컬 TGI는 하드웨어가 설치되면 매우 비용 효율적일 수 있습니다.</li>
<li><strong>Open and flexible</strong>: GPT‑OSS’s Apache 2.0 license supports private customization and deployment.
<strong>개방성과 유연성:</strong> GPT-OSS의 Apache 2.0 라이선스는 개인 사용자 지정 및 배포를 지원합니다.</li>
<li><strong>Great ergonomics</strong>: Keep Claude Code’s productive CLI while tapping GPT‑OSS’s capability.
<strong>뛰어난 인체공학:</strong> GPT-OSS의 기능을 활용하면서 Claude Code의 생산적인 CLI를 유지하세요.</li>
<li><strong>Model agility</strong>: Switch between GPT‑OSS, Qwen, and Anthropic providers with LiteLLM or OpenRouter.
<strong>모델 민첩성</strong>: LiteLLM 또는 OpenRouter를 사용하여 GPT-OSS, Qwen 및 Anthropic 공급자 간에 전환합니다.</li>
</ul>
<h1 id="wrapup-마무리">Wrap‑up 마무리</h1>
<p>You now have three proven ways to use GPT‑OSS with Claude Code — self‑host on Hugging Face, go fully managed with OpenRouter, or proxy locally with LiteLLM. Use the validation steps to confirm everything is wired correctly, then iterate on prompts, analyze codebases, and debug faster with an open‑weight model behind the scenes.
이제 Claude Code와 함께 GPT-OSS를 사용하는 세 가지 입증된 방법, 즉 Hugging Face에서 자체 호스팅, OpenRouter로 완전 관리, LiteLLM을 사용하여 로컬에서 프록시하는 방법이 있습니다. 유효성 검사 단계를 사용하여 모든 것이 올바르게 연결되었는지 확인한 다음, 프롬프트를 반복하고, 코드베이스를 분석하고, 백그라운드에서 오픈 가중치 모델을 사용하여 더 빠르게 디버깅합니다.</p> </article> </div> <script type="module">
      // 목적: index.json에서 현재 글 메타/썸네일을 찾아 상세 화면에 반영한다.
      async function hydrateMeta() {
        try {
          const BASE = import.meta.env.BASE_URL;
          const slug = decodeURIComponent(location.pathname.replace(/.*\/post\//,'').replace(/\/?$/,''));
          const res = await fetch(`${BASE}index.json`);
          const data = await res.json();
          const items = (data && data.items) || [];
          const item = items.find((i) => i.slug === slug);
          if (!item) return;

          const hero = document.getElementById('hero');
          const heroImg = document.getElementById('heroImg');
          const source = document.getElementById('source');
          const cta = document.getElementById('ctaSource');
          if (item.thumbnail && hero && heroImg) {
            heroImg.setAttribute('src', item.thumbnail);
            hero.style.display = 'block';
          }
          if (item.source_url && source && cta) {
            source.setAttribute('href', item.source_url);
            cta.setAttribute('href', item.source_url);
            source.style.display='inline-block';
            cta.style.display='inline-block';
          }
        } catch {}
      }
      hydrateMeta();

      // 복사 버튼 제거됨 — 상단에 원문 보기 버튼만 유지
    </script> </body> </html>