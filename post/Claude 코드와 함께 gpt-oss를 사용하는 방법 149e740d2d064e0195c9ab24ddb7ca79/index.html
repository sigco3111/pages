<!DOCTYPE html><html lang="ko" data-astro-cid-ztig7rse> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>ìƒì„¸</title><link rel="icon" href="/pages/favicon.svg" type="image/svg+xml"><link rel="icon" href="/pages/favicon-32x32.png" sizes="32x32"><link rel="apple-touch-icon" href="/pages/apple-touch-icon.png" sizes="180x180"><style>:root{color-scheme:light dark}body{margin:0;font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif}.wrap[data-astro-cid-ztig7rse]{max-width:860px;margin:0 auto;padding:20px}.topbar[data-astro-cid-ztig7rse]{position:sticky;top:0;backdrop-filter:blur(6px);background:color-mix(in oklab,canvas,transparent 35%);border-bottom:1px solid color-mix(in oklab,canvastext,transparent 90%);z-index:10}.topbar[data-astro-cid-ztig7rse] .inner[data-astro-cid-ztig7rse]{display:flex;align-items:center;gap:8px;padding:10px 20px;max-width:860px;margin:0 auto}.btn[data-astro-cid-ztig7rse]{appearance:none;border:1px solid color-mix(in oklab,canvastext,transparent 85%);background:transparent;color:inherit;border-radius:10px;padding:8px 12px;cursor:pointer;font-size:14px}.btn[data-astro-cid-ztig7rse].primary{background:#111827;color:#fff;border-color:#111827}@media (prefers-color-scheme: dark){.btn[data-astro-cid-ztig7rse].primary{background:#e5e7eb;color:#111827;border-color:#e5e7eb}}.hero[data-astro-cid-ztig7rse]{margin:14px 0 8px;display:none}.hero[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{width:100%;height:auto;border-radius:12px;display:block;background:#f3f4f6}article[data-astro-cid-ztig7rse]{line-height:1.72;font-size:16px}article[data-astro-cid-ztig7rse] :is(h1,h2,h3)[data-astro-cid-ztig7rse]{line-height:1.25;margin:24px 0 10px}article[data-astro-cid-ztig7rse] h1[data-astro-cid-ztig7rse]{font-size:28px}article[data-astro-cid-ztig7rse] h2[data-astro-cid-ztig7rse]{font-size:22px}article[data-astro-cid-ztig7rse] h3[data-astro-cid-ztig7rse]{font-size:18px}article[data-astro-cid-ztig7rse] p[data-astro-cid-ztig7rse]{margin:10px 0}article[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{max-width:100%;height:auto;border-radius:8px;background:#f3f4f6}article[data-astro-cid-ztig7rse] pre[data-astro-cid-ztig7rse]{overflow:auto;padding:14px;border:1px solid color-mix(in oklab,canvastext,transparent 90%);border-radius:10px;background:color-mix(in oklab,canvastext,transparent 96%)}article[data-astro-cid-ztig7rse] code[data-astro-cid-ztig7rse]:not(pre code){background:color-mix(in oklab,canvastext,transparent 94%);padding:2px 6px;border-radius:6px}article[data-astro-cid-ztig7rse] blockquote[data-astro-cid-ztig7rse]{border-left:3px solid #9CA3AF;margin:8px 0;padding:4px 12px;color:#6b7280}.actions[data-astro-cid-ztig7rse]{display:flex;gap:8px;flex-wrap:wrap;margin:12px 0 18px}
</style></head> <body class="container" style="padding:24px;max-width:900px" data-astro-cid-ztig7rse> <div class="topbar" data-astro-cid-ztig7rse> <div class="inner" data-astro-cid-ztig7rse> <a class="btn" href="/pages/" aria-label="í™ˆìœ¼ë¡œ" data-astro-cid-ztig7rse>â† í™ˆ</a> <a class="btn" id="source" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>ì›ë¬¸ ë³´ê¸°</a> </div> </div> <div class="wrap" data-astro-cid-ztig7rse> <h1 style="margin:10px 0 6px" data-astro-cid-ztig7rse></h1> <div class="hero" id="hero" data-astro-cid-ztig7rse><img alt="" id="heroImg" loading="eager" data-astro-cid-ztig7rse></div> <div class="actions" data-astro-cid-ztig7rse> <a class="btn primary" id="ctaSource" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>ì›ë¬¸ ë°”ë¡œê°€ê¸°</a> </div> <article data-astro-cid-ztig7rse> <h1 id="claude-ì½”ë“œì™€-í•¨ê»˜-gpt-ossë¥¼-ì‚¬ìš©í•˜ëŠ”-ë°©ë²•">Claude ì½”ë“œì™€ í•¨ê»˜ gpt-ossë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•</h1>
<p>ë°œê²¬ì¼: 2025/08/11
ì›ë¬¸ URL: <a href="https://garysvenson09.medium.com/how-to-use-gpt-oss-with-claude-code-da3cea5c8af8">https://garysvenson09.medium.com/how-to-use-gpt-oss-with-claude-code-da3cea5c8af8</a>
ë¶„ë¥˜: TIP
ì›ë¬¸ Source: ğŸ”—garysvenson09.medium
ì¦ê²¨ì°¾ê¸°: No</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1200/0*ha6xWapdDCyD_m5q.png" alt=""></p>
<p><img src="https://miro.medium.com/v2/resize:fill:64:64/1*lZW-IQU2EQXpcFQ3elV9AQ.png" alt=""></p>
<p><a href="https://apidog.com/?source=post_page---byline--da3cea5c8af8---------------------------------------">Gary Svenson</a></p>
<p>Follow</p>
<p>5 min read</p>
<p>2 days ago</p>
<p>7</p>
<p>1</p>
<p><a href="https://medium.com/plans?dimension=post_audio_button&#x26;postId=da3cea5c8af8&#x26;source=upgrade_membership---post_audio_button-----------------------------------------">Listen</a></p>
<p>Share</p>
<p>More</p>
<p>Want to run OpenAIâ€‘compatible, openâ€‘weight models in the familiar <strong>Claude Code</strong> CLI? <strong>GPT-OSS</strong> (20B and 120B) delivers strong coding and reasoning performance, and you can route Claude Code to it through OpenAIâ€‘style endpoints. This guide shows three reliable ways to connect Claude Code to GPT-OSS using Hugging Face Inference Endpoints, OpenRouter, or a LiteLLM proxy. Pick the path that fits your workflow and budget.
ì¹œìˆ™í•œ <strong>Claude Code</strong> CLIì—ì„œ OpenAI í˜¸í™˜ ì˜¤í”ˆ ì›¨ì´íŠ¸ ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? <strong>GPT-OSS</strong>(20B ë° 120B)ëŠ” ê°•ë ¥í•œ ì½”ë”© ë° ì¶”ë¡  ì„±ëŠ¥ì„ ì œê³µí•˜ë©° OpenAI ìŠ¤íƒ€ì¼ ì—”ë“œí¬ì¸íŠ¸ë¥¼ í†µí•´ Claude ì½”ë“œë¥¼ ë¼ìš°íŒ…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” Hugging Face Inference Endpoints, OpenRouter ë˜ëŠ” LiteLLM í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ Claude Codeë¥¼ GPT-OSSì— ì—°ê²°í•˜ëŠ” ì„¸ ê°€ì§€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‘ì—… íë¦„ê³¼ ì˜ˆì‚°ì— ë§ëŠ” ê²½ë¡œë¥¼ ì„ íƒí•˜ì‹­ì‹œì˜¤.</p>
<blockquote>
<p><em>Pro tip: Building and testing APIs while you experiment with GPT-OSS? Use</em> <a href="https://bit.ly/3Teeyxv"><em>Apidog</em></a> <em>â€” the all-in-one API development platform â€” to</em> <a href="https://bit.ly/4hKOxQZ">*design</a>,* <a href="https://bit.ly/3AHtaPS">*mock</a>,* <a href="https://bit.ly/4fMLpCJ">*test</a>, and* <a href="https://bit.ly/3Cs56kA"><em>publish docs</em></a> <em>in one place. It pairs nicely with Claude Code so you can iterate on prompts and verify endpoints without switching tools.</em>
<em>ì „ë¬¸ê°€ íŒ: GPT-OSSë¥¼ ì‹¤í—˜í•˜ëŠ” ë™ì•ˆ APIë¥¼ êµ¬ì¶•í•˜ê³  í…ŒìŠ¤íŠ¸í•˜ì‹œë‚˜ìš”?</em> <em>ì˜¬ì¸ì› API ê°œë°œ í”Œë«í¼ì¸</em> <a href="https://bit.ly/3Teeyxv"><em>Apidog</em></a>ì„ ì‚¬ìš©í•˜ì—¬ í•œ ê³³ì—ì„œ ë¬¸ì„œë¥¼ <a href="https://bit.ly/4hKOxQZ">*ë””ìì¸</a>,* <a href="https://bit.ly/3AHtaPS">*ëª¨ì˜</a>,* <a href="https://bit.ly/4fMLpCJ"><em>í…ŒìŠ¤íŠ¸</em></a> <em>ë°</em> <a href="https://bit.ly/3Cs56kA"><em>ê²Œì‹œ</em></a>í•  <em>ìˆ˜ ìˆìŠµë‹ˆë‹¤. Claude Codeì™€ ì˜ ì–´ìš¸ë¦¬ë¯€ë¡œ ë„êµ¬ë¥¼ ì „í™˜í•˜ì§€ ì•Šê³ ë„ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜ë³µí•˜ê³  ì—”ë“œí¬ì¸íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</em></p>
</blockquote>
<h1 id="why-pair-gpt-oss-with-claude-code">Why pair GPT-OSS with Claude Code?</h1>
<p>GPT-OSSë¥¼ Claude Codeì™€ í˜ì–´ë§í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?</p>
<p><strong>GPT-OSS</strong> is Open AIâ€™s openâ€‘weight model family with a 128K context window and an Apache 2.0 license â€” ideal for private deployments and customization. <strong>Claude Code</strong> (v0.5.3+) provides a fast, conversational CLI for coding. By pointing Claude Code at an OpenAIâ€‘compatible endpoint for GPT-OSS, you keep the UX you like while controlling costs and deployment.
<strong>GPT-OSS</strong>ëŠ” 128K ì»¨í…ìŠ¤íŠ¸ ì°½ê³¼ Apache 2.0 ë¼ì´ì„ ìŠ¤ë¥¼ ê°–ì¶˜ Open AIì˜ ì˜¤í”ˆ ì›¨ì´íŠ¸ ëª¨ë¸ ì œí’ˆêµ°ìœ¼ë¡œ, í”„ë¼ì´ë¹— ë°°í¬ ë° ì‚¬ìš©ì ì§€ì •ì— ì´ìƒì ì…ë‹ˆë‹¤. <strong>Claude Code</strong> (v0.5.3+)ëŠ” ì½”ë”©ì„ ìœ„í•œ ë¹ ë¥¸ ëŒ€í™”í˜• CLIë¥¼ ì œê³µí•©ë‹ˆë‹¤. GPT-OSSìš© OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ Claude Codeë¥¼ ê°€ë¦¬í‚¤ë©´ ë¹„ìš©ê³¼ ë°°í¬ë¥¼ ì œì–´í•˜ë©´ì„œ ì›í•˜ëŠ” UXë¥¼ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<h1 id="prerequisites-í•„ìˆ˜-êµ¬ì„±-ìš”ì†Œ">Prerequisites í•„ìˆ˜ êµ¬ì„± ìš”ì†Œ</h1>
<p>Make sure you have: ë‹¤ìŒì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.</p>
<ul>
<li><strong>Claude Code â‰¥ 0.5.3</strong>: Check <code>claude --version</code>. Install with <code>pip install claude-code</code> or update with <code>pip install --upgrade claude-code</code>.
<strong>Claude ì½”ë“œ â‰¥ 0.5.3</strong>: <code>claude --version</code>ì„ í™•ì¸í•˜ì‹­ì‹œì˜¤. <code>pip install claude-code</code>ë¡œ ì„¤ì¹˜í•˜ê±°ë‚˜ <code>pip install --upgrade claude-code</code> .</li>
<li><strong>Hugging Face account</strong>: Create a read/write token at <a href="https://huggingface.co/">huggingface.co</a>.
<strong>Hugging Face ê³„ì •</strong>: <a href="https://huggingface.co/">huggingface.co</a> ì—ì„œ ì½ê¸°/ì“°ê¸° í† í°ì„ ë§Œë“­ë‹ˆë‹¤.</li>
<li><strong>OpenRouter API key</strong> (for the OpenRouter path): Get one at <a href="https://openrouter.ai/">openrouter.ai</a>.
<strong>OpenRouter API í‚¤</strong>(OpenRouter ê²½ë¡œìš©): <a href="https://openrouter.ai/">openrouter.ai</a> ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.</li>
<li><strong>Python 3.10+ and Docker</strong>: Needed for local runs or LiteLLM.
<strong>Python 3.10+ ë° Docker</strong>: ë¡œì»¬ ì‹¤í–‰ ë˜ëŠ” LiteLLMì— í•„ìš”í•©ë‹ˆë‹¤.</li>
<li><strong>Commandâ€‘line familiarity</strong>: Youâ€™ll set environment variables and run basic commands.
<strong>ëª…ë ¹ì¤„ ì¹œìˆ™ë„</strong>: í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ê¸°ë³¸ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.</li>
</ul>
<p>Press enter or click to view image in full size</p>
<p><img src="https://miro.medium.com/v2/resize:fit:700/0*ha6xWapdDCyD_m5q.png" alt=""></p>
<h1 id="option-a--selfhost-gptoss-on-hugging-face">Option A â€” Selfâ€‘host GPTâ€‘OSS on Hugging Face</h1>
<p>ì˜µì…˜ A â€” Hugging Faceì—ì„œ GPT-OSS ìì²´ í˜¸ìŠ¤íŒ…</p>
<p>Use Hugging Face Inference Endpoints to deploy a private, scalable TGI server with OpenAI compatibility.
Hugging Face Inference Endpointsë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAIì™€ í˜¸í™˜ë˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ í”„ë¼ì´ë¹— TGI ì„œë²„ë¥¼ ë°°í¬í•˜ì„¸ìš”.</p>
<ol>
<li>Choose a model 1) ëª¨ë¸ ì„ íƒ</li>
</ol>
<hr>
<ol>
<li>Open the GPTâ€‘OSS repo on Hugging Face: <a href="https://huggingface.co/openai/gpt-oss-20b">openai/gpt-oss-20b</a> or <a href="https://huggingface.co/openai/gpt-oss-120b">openai/gpt-oss-120b</a>.
Hugging Faceì—ì„œ GPT-OSS ë¦¬í¬ì§€í† ë¦¬(<a href="https://huggingface.co/openai/gpt-oss-20b">openai/gpt-oss-20b</a> ë˜ëŠ” <a href="https://huggingface.co/openai/gpt-oss-120b">openai/gpt-oss-120b</a>)ë¥¼ ì—½ë‹ˆë‹¤.</li>
<li>Accept the Apache 2.0 license.
Apache 2.0 ë¼ì´ì„ ìŠ¤ì— ë™ì˜í•©ë‹ˆë‹¤.</li>
<li>Optional alternative: <strong>Qwen3-Coder-480B-A35B-Instruct</strong> (<a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct">Qwen/Qwen3-Coder-480B-A35B-Instruct</a>); choose a GGUF build for lighter hardware.
ì„ íƒì  ëŒ€ì•ˆ: <strong>Qwen3-Coder-480B-A35B-Instruct</strong>(<a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct">Qwen/Qwen3-Coder-480B-A35B-Instruct</a>); ë” ê°€ë²¼ìš´ í•˜ë“œì›¨ì–´ë¥¼ ìœ„í•´ GGUF ë¹Œë“œë¥¼ ì„ íƒí•˜ì‹­ì‹œì˜¤.</li>
</ol>
<p>Press enter or click to view image in full size</p>
<p><img src="https://miro.medium.com/v2/resize:fit:700/0*hxRmLY-NEeS3L24l.png" alt=""></p>
<ol>
<li>Deploy a Text Generation Inference endpoint</li>
<li>í…ìŠ¤íŠ¸ ìƒì„± ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ ë°°í¬</li>
</ol>
<hr>
<ol>
<li>On the model page, select <strong>Deploy</strong> > <strong>Inference Endpoint</strong>.
ëª¨ë¸ í˜ì´ì§€ì—ì„œ > <strong>ìœ ì¶” ì—”ë“œí¬ì¸íŠ¸</strong> <strong>ë°°í¬ë¥¼</strong> ì„ íƒí•©ë‹ˆë‹¤.</li>
<li>Pick the <strong>Text Generation Inference (TGI)</strong> template (â‰¥ v1.4.0).
<strong>TGI(í…ìŠ¤íŠ¸ ìƒì„± ìœ ì¶”)</strong> í…œí”Œë¦¿(â‰¥ v1.4.0)ì„ ì„ íƒí•©ë‹ˆë‹¤.</li>
<li>Enable OpenAI compatibility (check <strong>Enable OpenAI compatibility</strong> or add <code>--enable-openai</code>).
OpenAI í˜¸í™˜ì„±ì„ í™œì„±í™”í•©ë‹ˆë‹¤(<strong>OpenAI í˜¸í™˜ì„± í™œì„±í™”</strong> ë˜ëŠ” <code>--enable-openai</code> ì¶”ê°€).</li>
<li>Choose hardware: A10G/CPU for 20B; A100 for 120B. Create the endpoint.
í•˜ë“œì›¨ì–´ ì„ íƒ: 10Bìš© A20G/CPU; 100Bì˜ ê²½ìš° A120. ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.</li>
<li>Gather credentials 3) ìê²© ì¦ëª… ìˆ˜ì§‘</li>
</ol>
<hr>
<ol>
<li>When the endpoint is <strong>Running</strong>, note:
ì—”ë“œí¬ì¸íŠ¸ê°€ <strong>ì‹¤í–‰ ì¤‘</strong>ì¸ ê²½ìš° ë‹¤ìŒì„ ì°¸ê³ í•˜ì‹­ì‹œì˜¤.</li>
</ol>
<ul>
<li>
<p><strong>ENDPOINT_URL</strong> like <code>https://&#x3C;your-endpoint>.us-east-1.aws.endpoints.huggingface.cloud</code>.</p>
</li>
<li>
<p><strong>ENDPOINT_URL</strong> ì¢‹ì•„ìš” <code>https://&#x3C;your-endpoint>.us-east-1.aws.endpoints.huggingface.cloud</code> .</p>
</li>
<li>
<p><strong>HF_API_TOKEN</strong> from your Hugging Face account.</p>
</li>
<li>
<p>Hugging Face ê³„ì •ì—ì„œ <strong>HF_API_TOKEN</strong>í•©ë‹ˆë‹¤.</p>
</li>
</ul>
<ol start="2">
<li>
<p>Capture the model ID (e.g., <code>gpt-oss-20b</code> or <code>gpt-oss-120b</code>).</p>
</li>
<li>
<p>ëª¨ë¸ ID(ì˜ˆ: <code>gpt-oss-20b</code> ë˜ëŠ” <code>gpt-oss-120b</code>)ë¥¼ ìº¡ì²˜í•©ë‹ˆë‹¤.</p>
</li>
<li>
<p>Point Claude Code at your endpoint</p>
</li>
<li>
<p>ì—”ë“œí¬ì¸íŠ¸ì—ì„œ Claude ì½”ë“œë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.</p>
</li>
</ol>
<hr>
<ol>
<li>Export environment variables:</li>
<li>í™˜ê²½ ë³€ìˆ˜ ë‚´ë³´ë‚´ê¸°:</li>
</ol>
<p>export ANTHROPIC_BASE_URL=â€œ<a href="https://.us-east-1.aws.endpoints.huggingface.cloud">https://.us-east-1.aws.endpoints.huggingface.cloud</a>â€</p>
<p>export ANTHROPIC_AUTH_TOKEN=â€œhf_xxxxxxxxxxxxxxxxxâ€
export ANTHROPIC_MODEL=â€œgpt-oss-20bâ€  # or gpt-oss-120b</p>
<ol start="2">
<li>Verify the connection:</li>
<li>ì—°ê²°ì„ í™•ì¸í•©ë‹ˆë‹¤.</li>
</ol>
<p>claude â€”model gpt-oss-20b</p>
<p>Claude Code will route to your TGI endpoint via <code>/v1/chat/completions</code> using the OpenAIâ€‘compatible schema.
Claude CodeëŠ” OpenAI í˜¸í™˜ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ <code>/v1/chat/completions</code>ë¥¼ í†µí•´ TGI ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.</p>
<ol>
<li>Cost and scaling 5) ë¹„ìš© ë° í™•ì¥</li>
</ol>
<hr>
<ul>
<li>Hugging Face Inference Endpoints autoâ€‘scale; monitor usage. A10G is about ~0.60/hour; A100 is about ~3/hour.
Hugging Face ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ ìë™ í¬ê¸° ì¡°ì •; ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤. A10GëŠ” ì•½ ~0.60/ì‹œê°„ì…ë‹ˆë‹¤. A100ì€ ì•½ ~3/ì‹œê°„ì…ë‹ˆë‹¤.</li>
<li>Prefer local? Run TGI via Docker and point Claude Code at it:
í˜„ì§€ë¥¼ ì„ í˜¸í•˜ì‹œë‚˜ìš”? Dockerë¥¼ í†µí•´ TGIë¥¼ ì‹¤í–‰í•˜ê³  Claude Codeë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.</li>
</ul>
<p>docker run â€”name tgi -p 8080:80 -e HF_TOKEN=hf_xxxxxxxxxxxxxxxxx ghcr.io/huggingface/text-generation-inference:latest â€”model-id openai/gpt-oss-20b â€”enable-openai</p>
<p>Then set <code>ANTHROPIC_BASE_URL="http://localhost:8080"</code>. ê·¸ëŸ° ë‹¤ìŒ <code>ANTHROPIC_BASE_URL="http://localhost:8080"</code> .</p>
<h1 id="option-b--use-openrouter-as-a-managed-proxy">Option B â€” Use OpenRouter as a managed proxy</h1>
<p>ì˜µì…˜ B â€” OpenRouterë¥¼ ê´€ë¦¬í˜• í”„ë¡ì‹œë¡œ ì‚¬ìš©</p>
<p>Skip infrastructure and access GPTâ€‘OSS through OpenRouterâ€™s unified API.
ì¸í”„ë¼ë¥¼ ê±´ë„ˆë›°ê³  OpenRouterì˜ í†µí•© APIë¥¼ í†µí•´ GPT-OSSì— ì•¡ì„¸ìŠ¤í•˜ì„¸ìš”.</p>
<ol>
<li>Create an account and pick a model</li>
<li>ê³„ì • ìƒì„± ë° ëª¨ë¸ ì„ íƒ</li>
</ol>
<hr>
<ol>
<li>Sign up at <a href="https://openrouter.ai/">openrouter.ai</a> and copy your API key from <strong>Keys</strong>.
<a href="https://openrouter.ai/">openrouter.ai</a> ì—ì„œ ê°€ì…í•˜ê³  Keysì—ì„œ API í‚¤ë¥¼ ë³µì‚¬<strong>í•©ë‹ˆë‹¤.</strong></li>
<li>Choose a model slug: ëª¨ë¸ ìŠ¬ëŸ¬ê·¸ ì„ íƒ:</li>
</ol>
<ul>
<li>
<p><code>openai/gpt-oss-20b</code></p>
</li>
<li>
<p><code>ì˜¤í”ˆAI/GPT-OSS-20B</code></p>
</li>
<li>
<p><code>openai/gpt-oss-120b</code></p>
</li>
<li>
<p><code>ì˜¤í”ˆAI/GPT-OSS-120B</code></p>
</li>
<li>
<p><code>qwen/qwen3-coder-480b</code> (Qwenâ€™s coder model)</p>
</li>
<li>
<p><code>qwen/qwen3-coder-480b</code> (Qwenì˜ ì½”ë” ëª¨ë¸)</p>
</li>
</ul>
<p>Press enter or click to view image in full size</p>
<p><img src="https://miro.medium.com/v2/resize:fit:700/0*0N_Qv1JcXiIVudCS.png" alt=""></p>
<ol>
<li>Configure Claude Code 2) Claude ì½”ë“œ êµ¬ì„±</li>
</ol>
<hr>
<ol>
<li>Export variables: 1. ë³€ìˆ˜ ë‚´ë³´ë‚´ê¸°:</li>
</ol>
<p>export ANTHROPIC_BASE_URL=â€œ<a href="https://openrouter.ai/api/v1">https://openrouter.ai/api/v1</a>â€</p>
<p>export ANTHROPIC_AUTH_TOKEN=â€œor_xxxxxxxxxâ€
export ANTHROPIC_MODEL=â€œopenai/gpt-oss-20bâ€</p>
<ol start="2">
<li>Test the connection: 2. ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.</li>
</ol>
<p>claude â€”model openai/gpt-oss-20b</p>
<p>Claude Code will stream responses via OpenRouter, with builtâ€‘in fallback support.
Claude CodeëŠ” ë‚´ì¥ëœ ëŒ€ì²´ ì§€ì›ì„ í†µí•´ OpenRouterë¥¼ í†µí•´ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.</p>
<ol>
<li>Pricing notes 3) ê°€ê²© ì°¸ê³  ì‚¬í•­</li>
</ol>
<hr>
<ul>
<li>Approx. costs: ~0.50/M input tokens and ~2.00/M output tokens for GPTâ€‘OSSâ€‘120B; far below many proprietary options.
ëŒ€ëµì ì¸ ë¹„ìš©: GPT-OSS-120Bì˜ ê²½ìš° ~0.50/M ì…ë ¥ í† í° ë° ~2.00/M ì¶œë ¥ í† í°; ë§ì€ ë…ì  ì˜µì…˜ë³´ë‹¤ í›¨ì”¬ ë‚®ìŠµë‹ˆë‹¤.</li>
<li>OpenRouter bills per usage; no infrastructure to manage.
OpenRouterëŠ” ì‚¬ìš©ëŸ‰ë‹¹ ì²­êµ¬ë©ë‹ˆë‹¤. ê´€ë¦¬í•  ì¸í”„ë¼ê°€ ì—†ìŠµë‹ˆë‹¤.</li>
</ul>
<h1 id="option-c--route-via-litellm-for-multimodel-workflows">Option C â€” Route via LiteLLM for multiâ€‘model workflows</h1>
<p>ì˜µì…˜ C â€” ë‹¤ì¤‘ ëª¨ë¸ ì›Œí¬í”Œë¡œë¥¼ ìœ„í•´ LiteLLMì„ í†µí•´ ë¼ìš°íŒ…</p>
<p>Use LiteLLM as a local proxy if you want to hotâ€‘swap between GPTâ€‘OSS, Qwen, or Anthropic models.
GPT-OSS, Qwen ë˜ëŠ” Anthropic ëª¨ë¸ ê°„ì— í•«ìŠ¤ì™‘í•˜ë ¤ëŠ” ê²½ìš° LiteLLMì„ ë¡œì»¬ í”„ë¡ì‹œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>
<ol>
<li>Install and configure LiteLLM</li>
<li>LiteLLM ì„¤ì¹˜ ë° êµ¬ì„±</li>
</ol>
<hr>
<ol>
<li>Install <a href="https://docs.litellm.ai/">LiteLLM</a>:
<a href="https://docs.litellm.ai/">LiteLLM</a>ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.</li>
</ol>
<p>pip install litellm</p>
<ol start="2">
<li>Create <code>litellm.yaml</code>:</li>
<li><code>litellm.yaml</code>ì„ ë§Œë“­ë‹ˆë‹¤.</li>
</ol>
<p>model_list:</p>
<ul>
<li>
<p>model_name: gpt-oss-20b
litellm_params:
model: openai/gpt-oss-20b
api_key: or_xxxxxxxxx  # OpenRouter key
api_base: <a href="https://openrouter.ai/api/v1">https://openrouter.ai/api/v1</a></p>
</li>
<li>
<p>model_name: qwen3-coder
litellm_params:
model: openrouter/qwen/qwen3-coder
api_key: or_xxxxxxxxx
api_base: <a href="https://openrouter.ai/api/v1">https://openrouter.ai/api/v1</a></p>
</li>
</ul>
<ol start="3">
<li>Start the proxy: 3. í”„ë¡ì‹œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.</li>
</ol>
<p>litellm â€”config litellm.yaml</p>
<ol>
<li>Point Claude Code to LiteLLM</li>
<li>Claude ì½”ë“œë¥¼ LiteLLMì— ê°€ë¦¬í‚¤ê¸°</li>
</ol>
<hr>
<ol>
<li>Export variables: 1. ë³€ìˆ˜ ë‚´ë³´ë‚´ê¸°:</li>
</ol>
<p>export ANTHROPIC_BASE_URL=â€œ<a href="http://localhost:4000">http://localhost:4000</a>â€</p>
<p>export ANTHROPIC_AUTH_TOKEN=â€œlitellm_masterâ€
export ANTHROPIC_MODEL=â€œgpt-oss-20bâ€</p>
<ol start="2">
<li>Confirm it works: 2. ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.</li>
</ol>
<p>claude â€”model gpt-oss-20b</p>
<p>LiteLLM forwards to GPTâ€‘OSS via OpenRouter and supports cost logging. Prefer simpleâ€‘shuffle routing to avoid incompatibilities with Anthropic providers.
LiteLLMì€ OpenRouterë¥¼ í†µí•´ GPT-OSSë¡œ ì „ë‹¬í•˜ê³  ë¹„ìš© ë¡œê¹…ì„ ì§€ì›í•©ë‹ˆë‹¤. Anthropic ê³µê¸‰ìì™€ì˜ ë¹„í˜¸í™˜ì„±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë‹¨ìˆœ ì…”í”Œ ë¼ìš°íŒ…ì„ ì„ í˜¸í•©ë‹ˆë‹¤.</p>
<ol>
<li>Notes 3) ë©”ëª¨</li>
</ol>
<hr>
<ul>
<li>Use simpleâ€‘shuffle over latencyâ€‘based routing for stability with Anthropic models.
Anthropic ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ ìœ„í•´ ì§€ì—° ì‹œê°„ ê¸°ë°˜ ë¼ìš°íŒ…ì— ëŒ€í•œ ë‹¨ìˆœ ì„ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</li>
<li>Review LiteLLM logs to track usage and costs.
LiteLLM ë¡œê·¸ë¥¼ ê²€í† í•˜ì—¬ ì‚¬ìš©ëŸ‰ ë° ë¹„ìš©ì„ ì¶”ì í•©ë‹ˆë‹¤.</li>
</ul>
<h1 id="validate-your-setup-ì„¤ì •-ê²€ì¦">Validate your setup ì„¤ì • ê²€ì¦</h1>
<p>Open Claude Code and try these checks:
Claude Codeë¥¼ ì—´ê³  ë‹¤ìŒ í™•ì¸ì„ ì‹œë„í•´ ë³´ì„¸ìš”:</p>
<ul>
<li><strong>Code generation ì½”ë“œ ìƒì„±</strong></li>
</ul>
<p>claude â€”model gpt-oss-20b â€œWrite a Python REST API with Flaskâ€</p>
<p>Expected response (example):
ì˜ˆìƒ ì‘ë‹µ(ì˜ˆ):</p>
<p>from flask import Flask, jsonify</p>
<p>app = Flask(<strong>name</strong>)
@app.route(â€˜/apiâ€™, methods=[â€˜GETâ€™])
def get_data():
return jsonify({â€œmessageâ€: â€œHello from GPT-OSS!â€})
if <strong>name</strong> == â€˜<strong>main</strong>â€™:
app.run(debug=True)</p>
<ul>
<li><strong>Codebase analysis ì½”ë“œë² ì´ìŠ¤ ë¶„ì„</strong></li>
</ul>
<p>claude â€”model gpt-oss-20b â€œSummarize src/server.jsâ€</p>
<ul>
<li><strong>Debugging ë””ë²„ê¹…</strong></li>
</ul>
<p>claude â€”model gpt-oss-20b â€œDebug this buggy Python code: [paste code]â€œ</p>
<h1 id="troubleshooting-ë¬¸ì œ-í•´ê²°">Troubleshooting ë¬¸ì œ í•´ê²°</h1>
<ul>
<li><strong>404 on</strong> <code>**/v1/chat/completions**</code>: Ensure <code>--enable-openai</code> is enabled in TGI (Option A) or verify model availability on OpenRouter (Option B).
<code>**/v1/chat/completions**</code><strong>ì˜ 404</strong>: TGI(ì˜µì…˜ A)ì—ì„œ <code>--enable-openai</code>ê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê±°ë‚˜ OpenRouter(ì˜µì…˜ B)ì—ì„œ ëª¨ë¸ ê°€ìš©ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.</li>
<li><strong>Empty responses</strong>: Confirm <code>ANTHROPIC_MODEL</code> exactly matches the model slug (e.g., <code>gpt-oss-20b</code>).
<strong>ë¹ˆ ì‘ë‹µ</strong>: ëª¨ë¸ ìŠ¬ëŸ¬ê·¸(ì˜ˆ: <code>gpt-oss-20b</code>)ì™€ ì •í™•íˆ ì¼ì¹˜<code>ANTHROPIC_MODEL</code> í™•ì¸í•©ë‹ˆë‹¤.</li>
<li><strong>400 after swapping models</strong>: Set LiteLLM to simpleâ€‘shuffle routing (Option C).
<strong>ëª¨ë¸ êµì²´ í›„ 400</strong>: LiteLLMì„ ë‹¨ìˆœ ì…”í”Œ ë¼ìš°íŒ…(ì˜µì…˜ C)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.</li>
<li><strong>Slow first token</strong>: Warm up Hugging Face endpoints with a small prompt after scaleâ€‘toâ€‘zero.
<strong>ëŠë¦° ì²« ë²ˆì§¸ í† í°</strong>: 0ìœ¼ë¡œ í™•ì¥í•œ í›„ ì‘ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ Hugging Face ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì›Œë°ì—…í•©ë‹ˆë‹¤.</li>
<li><strong>Claude Code crashes</strong>: Upgrade to â‰¥ 0.5.3 and recheck environment variables.
<strong>Claude Code ì¶©ëŒ</strong>: â‰¥ 0.5.3ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ê³  í™˜ê²½ ë³€ìˆ˜ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.</li>
</ul>
<h1 id="benefits-of-this-setup-ì´-ì„¤ì •ì˜-ì´ì ">Benefits of this setup ì´ ì„¤ì •ì˜ ì´ì </h1>
<ul>
<li><strong>Lower cost</strong>: OpenRouterâ€™s pricing is competitive; local TGI can be very costâ€‘effective once hardware is in place.
<strong>ì €ë ´í•œ ë¹„ìš©</strong>: OpenRouterì˜ ê°€ê²©ì€ ê²½ìŸë ¥ì´ ìˆìŠµë‹ˆë‹¤. ë¡œì»¬ TGIëŠ” í•˜ë“œì›¨ì–´ê°€ ì„¤ì¹˜ë˜ë©´ ë§¤ìš° ë¹„ìš© íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>Open and flexible</strong>: GPTâ€‘OSSâ€™s Apache 2.0 license supports private customization and deployment.
<strong>ê°œë°©ì„±ê³¼ ìœ ì—°ì„±:</strong> GPT-OSSì˜ Apache 2.0 ë¼ì´ì„ ìŠ¤ëŠ” ê°œì¸ ì‚¬ìš©ì ì§€ì • ë° ë°°í¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.</li>
<li><strong>Great ergonomics</strong>: Keep Claude Codeâ€™s productive CLI while tapping GPTâ€‘OSSâ€™s capability.
<strong>ë›°ì–´ë‚œ ì¸ì²´ê³µí•™:</strong> GPT-OSSì˜ ê¸°ëŠ¥ì„ í™œìš©í•˜ë©´ì„œ Claude Codeì˜ ìƒì‚°ì ì¸ CLIë¥¼ ìœ ì§€í•˜ì„¸ìš”.</li>
<li><strong>Model agility</strong>: Switch between GPTâ€‘OSS, Qwen, and Anthropic providers with LiteLLM or OpenRouter.
<strong>ëª¨ë¸ ë¯¼ì²©ì„±</strong>: LiteLLM ë˜ëŠ” OpenRouterë¥¼ ì‚¬ìš©í•˜ì—¬ GPT-OSS, Qwen ë° Anthropic ê³µê¸‰ì ê°„ì— ì „í™˜í•©ë‹ˆë‹¤.</li>
</ul>
<h1 id="wrapup-ë§ˆë¬´ë¦¬">Wrapâ€‘up ë§ˆë¬´ë¦¬</h1>
<p>You now have three proven ways to use GPTâ€‘OSS with Claude Code â€” selfâ€‘host on Hugging Face, go fully managed with OpenRouter, or proxy locally with LiteLLM. Use the validation steps to confirm everything is wired correctly, then iterate on prompts, analyze codebases, and debug faster with an openâ€‘weight model behind the scenes.
ì´ì œ Claude Codeì™€ í•¨ê»˜ GPT-OSSë¥¼ ì‚¬ìš©í•˜ëŠ” ì„¸ ê°€ì§€ ì…ì¦ëœ ë°©ë²•, ì¦‰ Hugging Faceì—ì„œ ìì²´ í˜¸ìŠ¤íŒ…, OpenRouterë¡œ ì™„ì „ ê´€ë¦¬, LiteLLMì„ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ í”„ë¡ì‹œí•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ìœ íš¨ì„± ê²€ì‚¬ ë‹¨ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ê²ƒì´ ì˜¬ë°”ë¥´ê²Œ ì—°ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸í•œ ë‹¤ìŒ, í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜ë³µí•˜ê³ , ì½”ë“œë² ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³ , ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤í”ˆ ê°€ì¤‘ì¹˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë” ë¹ ë¥´ê²Œ ë””ë²„ê¹…í•©ë‹ˆë‹¤.</p> </article> </div> <script type="module">
      // ëª©ì : index.jsonì—ì„œ í˜„ì¬ ê¸€ ë©”íƒ€/ì¸ë„¤ì¼ì„ ì°¾ì•„ ìƒì„¸ í™”ë©´ì— ë°˜ì˜í•œë‹¤.
      async function hydrateMeta() {
        try {
          const BASE = import.meta.env.BASE_URL;
          const slug = decodeURIComponent(location.pathname.replace(/.*\/post\//,'').replace(/\/?$/,''));
          const res = await fetch(`${BASE}index.json`);
          const data = await res.json();
          const items = (data && data.items) || [];
          const item = items.find((i) => i.slug === slug);
          if (!item) return;

          const hero = document.getElementById('hero');
          const heroImg = document.getElementById('heroImg');
          const source = document.getElementById('source');
          const cta = document.getElementById('ctaSource');
          if (item.thumbnail && hero && heroImg) {
            heroImg.setAttribute('src', item.thumbnail);
            hero.style.display = 'block';
          }
          if (item.source_url && source && cta) {
            source.setAttribute('href', item.source_url);
            cta.setAttribute('href', item.source_url);
            source.style.display='inline-block';
            cta.style.display='inline-block';
          }
        } catch {}
      }
      hydrateMeta();

      // ë³µì‚¬ ë²„íŠ¼ ì œê±°ë¨ â€” ìƒë‹¨ì— ì›ë¬¸ ë³´ê¸° ë²„íŠ¼ë§Œ ìœ ì§€
    </script> </body> </html>