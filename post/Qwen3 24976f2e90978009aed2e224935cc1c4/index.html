<!DOCTYPE html><html lang="ko" data-astro-cid-ztig7rse> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>상세</title><link rel="icon" href="/pages/favicon.svg" type="image/svg+xml"><link rel="icon" href="/pages/favicon-32x32.png" sizes="32x32"><link rel="apple-touch-icon" href="/pages/apple-touch-icon.png" sizes="180x180"><style>:root{color-scheme:light dark}body{margin:0;font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif}.wrap[data-astro-cid-ztig7rse]{max-width:860px;margin:0 auto;padding:20px}.topbar[data-astro-cid-ztig7rse]{position:sticky;top:0;backdrop-filter:blur(6px);background:color-mix(in oklab,canvas,transparent 35%);border-bottom:1px solid color-mix(in oklab,canvastext,transparent 90%);z-index:10}.topbar[data-astro-cid-ztig7rse] .inner[data-astro-cid-ztig7rse]{display:flex;align-items:center;gap:8px;padding:10px 20px;max-width:860px;margin:0 auto}.btn[data-astro-cid-ztig7rse]{appearance:none;border:1px solid color-mix(in oklab,canvastext,transparent 85%);background:transparent;color:inherit;border-radius:10px;padding:8px 12px;cursor:pointer;font-size:14px}.btn[data-astro-cid-ztig7rse].primary{background:#111827;color:#fff;border-color:#111827}@media (prefers-color-scheme: dark){.btn[data-astro-cid-ztig7rse].primary{background:#e5e7eb;color:#111827;border-color:#e5e7eb}}.hero[data-astro-cid-ztig7rse]{margin:14px 0 8px;display:none}.hero[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{width:100%;height:auto;border-radius:12px;display:block;background:#f3f4f6}article[data-astro-cid-ztig7rse]{line-height:1.72;font-size:16px}article[data-astro-cid-ztig7rse] :is(h1,h2,h3)[data-astro-cid-ztig7rse]{line-height:1.25;margin:24px 0 10px}article[data-astro-cid-ztig7rse] h1[data-astro-cid-ztig7rse]{font-size:28px}article[data-astro-cid-ztig7rse] h2[data-astro-cid-ztig7rse]{font-size:22px}article[data-astro-cid-ztig7rse] h3[data-astro-cid-ztig7rse]{font-size:18px}article[data-astro-cid-ztig7rse] p[data-astro-cid-ztig7rse]{margin:10px 0}article[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{max-width:100%;height:auto;border-radius:8px;background:#f3f4f6}article[data-astro-cid-ztig7rse] pre[data-astro-cid-ztig7rse]{overflow:auto;padding:14px;border:1px solid color-mix(in oklab,canvastext,transparent 90%);border-radius:10px;background:color-mix(in oklab,canvastext,transparent 96%)}article[data-astro-cid-ztig7rse] code[data-astro-cid-ztig7rse]:not(pre code){background:color-mix(in oklab,canvastext,transparent 94%);padding:2px 6px;border-radius:6px}article[data-astro-cid-ztig7rse] blockquote[data-astro-cid-ztig7rse]{border-left:3px solid #9CA3AF;margin:8px 0;padding:4px 12px;color:#6b7280}.actions[data-astro-cid-ztig7rse]{display:flex;gap:8px;flex-wrap:wrap;margin:12px 0 18px}
</style></head> <body class="container" style="padding:24px;max-width:900px" data-astro-cid-ztig7rse> <div class="topbar" data-astro-cid-ztig7rse> <div class="inner" data-astro-cid-ztig7rse> <a class="btn" href="/pages/" aria-label="홈으로" data-astro-cid-ztig7rse>← 홈</a> <a class="btn" id="source" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>원문 보기</a> </div> </div> <div class="wrap" data-astro-cid-ztig7rse> <h1 style="margin:10px 0 6px" data-astro-cid-ztig7rse></h1> <div class="hero" id="hero" data-astro-cid-ztig7rse><img alt="" id="heroImg" loading="eager" data-astro-cid-ztig7rse></div> <div class="actions" data-astro-cid-ztig7rse> <a class="btn primary" id="ctaSource" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>원문 바로가기</a> </div> <article data-astro-cid-ztig7rse> <h1 id="qwen3">Qwen3</h1>
<p>발견일: 2025/07/23
분류: 오픈소스
즐겨찾기: No</p>
<h2 id="커뮤니티-및-지원">커뮤니티 및 지원</h2>
<ul>
<li><strong>GitHub</strong>: <a href="https://github.com/QwenLM/Qwen3">https://github.com/QwenLM/Qwen3</a></li>
<li><strong>Hugging Face</strong>: <a href="https://huggingface.co/Qwen">https://huggingface.co/Qwen</a></li>
<li><strong>ModelScope</strong>: 모델 가중치 및 문서 제공.</li>
<li><strong>디스코드</strong>: 커뮤니티 지원 및 개발자 교류.</li>
<li><strong>문서</strong>: 영어/중국어 문서(<a href="https://qwenlm.github.io/">https://qwenlm.github.io</a>).</li>
</ul>
<h1 id="qwen3-개요">Qwen3 개요</h1>
<p>Qwen3는 알리바바 클라우드의 Qwen 팀이 2025년 4월 29일에 공개한 최신 대형 언어 모델(LLM) 시리즈입니다. Qwen2.5와 QwQ 시리즈를 기반으로 성능이 크게 향상되었으며, 다양한 크기의 밀집 모델(Dense Model)과 전문가 혼합 모델(Mixture-of-Experts, MoE)을 포함합니다. Apache 2.0 라이선스 하에 오픈 웨이트(Open-Weight)로 제공되어 상업적 이용이 가능하며, 연구 및 개발에 최적화된 도구입니다.</p>
<h2 id="주요-특징">주요 특징</h2>
<h3 id="1-모델-구성">1. 모델 구성</h3>
<p>Qwen3는 다양한 크기의 모델로 제공되며, 사용자의 하드웨어 및 작업 요구사항에 맞게 선택 가능합니다:</p>
<ul>
<li><strong>MoE 모델</strong>:
<ul>
<li><strong>Qwen3-235B-A22B</strong>: 총 2350억 개 파라미터, 활성 파라미터 220억 개. DeepSeek-R1, Grok-3, Gemini-2.5-Pro 등과 경쟁.</li>
<li><strong>Qwen3-30B-A3B</strong>: 총 300억 개 파라미터, 활성 파라미터 30억 개. QwQ-32B를 능가하는 효율성.</li>
</ul>
</li>
<li><strong>밀집 모델(Dense Models)</strong>:
<ul>
<li>Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B.</li>
<li>Qwen3-4B는 Qwen2.5-72B-Instruct와 유사한 성능을 18배 적은 파라미터로 제공.</li>
</ul>
</li>
<li>모든 모델은 Apache 2.0 라이선스로 상업적 및 연구 용도에 제한 없이 사용 가능.</li>
</ul>
<h3 id="2-하이브리드-사고-모드-hybrid-thinking-modes">2. 하이브리드 사고 모드 (Hybrid Thinking Modes)</h3>
<ul>
<li><strong>Thinking Mode</strong>: 복잡한 논리적 추론, 수학, 코딩 등에 적합. <code>...</code> 블록으로 단계별 추론 제공.</li>
<li><strong>Non-Thinking Mode</strong>: 간단한 질문에 빠른 응답 제공.</li>
<li><code>/think</code> 또는 <code>/no_think</code> 토큰으로 모드 제어, 사고 예산(Thinking Budget) 조절 가능.</li>
</ul>
<h3 id="3-향상된-성능">3. 향상된 성능</h3>
<ul>
<li><strong>추론 능력</strong>: 수학, 코딩, 상식적 논리 추론에서 QwQ 및 Qwen2.5를 능가.</li>
<li><strong>인간 선호도 정렬</strong>: 창의적 글쓰기, 역할극, 다중 턴 대화에서 자연스러운 경험 제공.</li>
<li><strong>멀티모달 지원</strong>: Qwen3-VL(비전-언어 모델)은 이미지 및 비디오 처리 지원(2024년 10월 기준 개발 중).</li>
<li><strong>다국어 지원</strong>: 119개 언어 및 방언 지원, 번역 및 다국어 지시사항 처리 강화.</li>
</ul>
<h3 id="4-대규모-학습-데이터">4. 대규모 학습 데이터</h3>
<ul>
<li><strong>프리트레이닝</strong>: 약 36조 토큰(영어, 중국어 중심, 119개 언어 포함)으로 학습. Qwen2.5(18조 토큰) 대비 2배 증가.</li>
<li><strong>데이터 소스</strong>: 웹 데이터 및 PDF 문서 추출 텍스트(Qwen2.5-VL 및 Qwen2.5 활용).</li>
<li><strong>포스트트레이닝</strong>: 지시사항 준수, 추론 능력 강화를 위한 4단계 파이프라인.</li>
</ul>
<h3 id="5-도구-호출-및-에이전트-기능">5. 도구 호출 및 에이전트 기능</h3>
<ul>
<li><strong>Model Context Protocol (MCP)</strong>: 외부 도구, API, 데이터베이스와 표준화된 상호작용 지원(계산기, 날씨 API, 코드 인터프리터 등).</li>
<li><strong>Qwen-Agent</strong>: 도구 호출 템플릿과 파서 제공으로 개발 복잡성 감소.</li>
<li><strong>활용 사례</strong>: 수학 문제 해결, 데이터 시각화, 파일 처리, 웹 스크래핑.</li>
</ul>
<h3 id="6-효율적인-moe-아키텍처">6. 효율적인 MoE 아키텍처</h3>
<ul>
<li>MoE 모델은 전체 파라미터 중 일부만 활성화(예: 235B 중 22B, 30B 중 3B)하여 추론 속도와 메모리 효율성 향상.</li>
<li>Qwen3-30B-A3B는 단일 고성능 GPU(예: RTX 4090)에서 실행 가능.</li>
</ul>
<h2 id="배포-및-사용-방법">배포 및 사용 방법</h2>
<h3 id="1-지원-플랫폼">1. 지원 플랫폼</h3>
<ul>
<li><strong>클라우드</strong>: Hugging Face, ModelScope, Kaggle에서 모델 가중치 제공.</li>
<li><strong>로컬 실행</strong>:
<ul>
<li><strong>프레임워크</strong>: SGLang, vLLM (고속 추론).</li>
<li><strong>도구</strong>: Ollama (<code>ollama run qwen3:30b-a3b</code>), LMStudio, MLX, llama.cpp, KTransformers.</li>
</ul>
</li>
<li><strong>Qwen Chat</strong>: 웹(<code>chat.qwen.ai</code>) 및 모바일 앱에서 체험 가능.</li>
</ul>
<h3 id="2-요구사항">2. 요구사항</h3>
<ul>
<li><strong>Hugging Face Transformers</strong>: 최신 버전(4.51.0 이상) 권장.</li>
<li><strong>GPU 메모리</strong>: 모델 크기에 따라 다름(Qwen3-30B-A3B는 RTX 4090에서 실행 가능).</li>
</ul>
<h3 id="3-코드-예시">3. 코드 예시</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> transformers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">model_name </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> "Qwen/Qwen3-235B-A22B-Instruct-2507"</span></span>
<span class="line"><span style="color:#E1E4E8">tokenizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span class="line"><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoModelForCausalLM.from_pretrained(model_name, </span><span style="color:#FFAB70">device_map</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">prompt </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> "Write a short story about a futuristic city."</span></span>
<span class="line"><span style="color:#E1E4E8">inputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer(prompt, </span><span style="color:#FFAB70">return_tensors</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"pt"</span><span style="color:#E1E4E8">).to(</span><span style="color:#9ECBFF">"cuda"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">outputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generate(</span><span style="color:#F97583">**</span><span style="color:#E1E4E8">inputs, </span><span style="color:#FFAB70">max_length</span><span style="color:#F97583">=</span><span style="color:#79B8FF">200</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(tokenizer.decode(outputs[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], </span><span style="color:#FFAB70">skip_special_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">))</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
<h2 id="성능-벤치마크">성능 벤치마크</h2>
<ul>
<li><strong>Qwen3-235B-A22B</strong>: 코딩, 수학, 일반 추론에서 DeepSeek-R1, o1, o3-mini, Grok-3, Gemini-2.5-Pro와 경쟁.</li>
<li><strong>Qwen3-30B-A3B</strong>: QwQ-32B 대비 10배 적은 활성 파라미터로 유사한 성능.</li>
<li><strong>Qwen3-4B</strong>: Qwen2.5-72B-Instruct와 비슷한 성능, 효율성 우수.</li>
<li><strong>장점</strong>: 128K 토큰(일부 32K) 컨텍스트 처리, 다중 턴 대화, 창의적 글쓰기.</li>
</ul>
<h2 id="qwen3-vl-비전-언어-모델">Qwen3-VL (비전-언어 모델)</h2>
<ul>
<li><strong>설명</strong>: 이미지 및 비디오 이해, 질의 응답, 콘텐츠 생성 지원. 20분 이상 비디오 처리 가능.</li>
<li><strong>특징</strong>:
<ul>
<li>다양한 해상도와 비율의 이미지 이해.</li>
<li>다국어 텍스트 인식(영어, 중국어, 유럽 언어, 일본어, 한국어 등).</li>
<li>디바이스 통합을 위한 에이전트 기능.</li>
</ul>
</li>
<li><strong>상태</strong>: 2024년 10월 기준 개발 중.</li>
</ul>
<h2 id="qwen3-embedding-시리즈">Qwen3 Embedding 시리즈</h2>
<ul>
<li><strong>설명</strong>: 텍스트 임베딩, 검색, 재랭킹 작업에 특화.</li>
<li><strong>특징</strong>:
<ul>
<li>0.6B~8B 크기 제공.</li>
<li>100개 이상 언어 및 프로그래밍 언어 지원.</li>
<li>LoRA 파인튜닝으로 텍스트 이해 능력 강화.</li>
</ul>
</li>
<li><strong>활용 사례</strong>: 문서 검색, 의미적 유사성 분석, 다국어 텍스트 처리.</li>
</ul>
<h2 id="장점-및-한계">장점 및 한계</h2>
<h3 id="장점">장점</h3>
<ul>
<li><strong>효율성</strong>: MoE 아키텍처로 추론 비용 감소, 소규모 하드웨어에서도 고성능.</li>
<li><strong>유연성</strong>: 사고 모드 전환, MCP 통합, 다국어 지원.</li>
<li><strong>오픈소스</strong>: Apache 2.0 라이선스로 상업적 및 연구 활용 가능.</li>
<li><strong>커뮤니티 지원</strong>: GitHub, Hugging Face, ModelScope에서 활발한 업데이트.</li>
</ul>
<h3 id="한계">한계</h3>
<ul>
<li><strong>멀티모달 제한</strong>: Qwen3는 텍스트 기반, Qwen3-VL은 개발 중.</li>
<li><strong>학습 데이터 투명성 부족</strong>: 36조 토큰 데이터셋의 세부 구성 미공개.</li>
<li><strong>고급 하드웨어 요구</strong>: Qwen3-235B-A22B는 고성능 GPU 필요.</li>
</ul>
<h2 id="활용-사례">활용 사례</h2>
<ul>
<li><strong>코딩</strong>: Python, Java 등 코드 생성 및 디버깅.</li>
<li><strong>에이전트 개발</strong>: MCP로 자동화 워크플로우 구축.</li>
<li><strong>다국어 애플리케이션</strong>: 번역, 다국어 챗봇, 문서 분석.</li>
<li><strong>연구</strong>: 언어 모델 추론 및 파인튜닝 연구.</li>
</ul>
<h2 id="결론">결론</h2>
<p>Qwen3는 성능, 효율성, 유연성을 결합한 차세대 오픈소스 언어 모델로, 연구자와 개발자에게 강력한 도구를 제공합니다. MoE 아키텍처와 하이브리드 사고 모드로 복잡한 작업과 간단한 대화를 효율적으로 처리하며, 다국어 및 도구 호출 기능으로 글로벌 애플리케이션에 적합합니다. 지속적인 커뮤니티 피드백과 업데이트로 AI 연구 및 개발의 선두주자로 자리 잡고 있습니다.</p> </article> </div> <script type="module">
      // 목적: index.json에서 현재 글 메타/썸네일을 찾아 상세 화면에 반영한다.
      async function hydrateMeta() {
        try {
          const BASE = import.meta.env.BASE_URL;
          const slug = decodeURIComponent(location.pathname.replace(/.*\/post\//,'').replace(/\/?$/,''));
          const res = await fetch(`${BASE}index.json`);
          const data = await res.json();
          const items = (data && data.items) || [];
          const item = items.find((i) => i.slug === slug);
          if (!item) return;

          const hero = document.getElementById('hero');
          const heroImg = document.getElementById('heroImg');
          const source = document.getElementById('source');
          const cta = document.getElementById('ctaSource');
          if (item.thumbnail && hero && heroImg) {
            heroImg.setAttribute('src', item.thumbnail);
            hero.style.display = 'block';
          }
          if (item.source_url && source && cta) {
            source.setAttribute('href', item.source_url);
            cta.setAttribute('href', item.source_url);
            source.style.display='inline-block';
            cta.style.display='inline-block';
          }
        } catch {}
      }
      hydrateMeta();

      // 복사 버튼 제거됨 — 상단에 원문 보기 버튼만 유지
    </script> </body> </html>