<!DOCTYPE html><html lang="ko"> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>상세</title><link rel="icon" href="/pages/favicon.svg" type="image/svg+xml"><link rel="icon" href="/pages/favicon-32x32.png" sizes="32x32"><link rel="apple-touch-icon" href="/pages/apple-touch-icon.png" sizes="180x180"></head> <body class="container" style="padding:24px;max-width:900px"> <a href="/pages/" aria-label="홈으로">← 홈</a> <h1 style="margin:8px 0 4px"></h1>  <article style="margin-top:16px"> <h1 id="glm-45-지능형-에이전트를-위한-오픈소스-대형-언어-모델">GLM-4.5: 지능형 에이전트를 위한 오픈소스 대형 언어 모델</h1>
<p>발견일: 2025/07/31
원문 URL: <a href="https://github.com/zai-org/GLM-4.5">https://github.com/zai-org/GLM-4.5</a>
분류: 오픈소스
원문 Source: 🔗github
즐겨찾기: No</p>
<p><a href="https://opengraph.githubassets.com/77dc499cf16105edd2c1b7c8c556e54c553ea4840d9a3da5cccb21e2e46e4d90/zai-org/GLM-4.5"></a></p>
<h1 id="glm-45-지능형-에이전트를-위한-오픈소스-대형-언어-모델-1">GLM-4.5: 지능형 에이전트를 위한 오픈소스 대형 언어 모델</h1>
<h2 id="개요">개요</h2>
<p>GLM-4.5는 <a href="http://z.ai/">Z.ai</a>(구 Zhipu AI)에서 개발한 최신 오픈소스 대형 언어 모델(LLM) 시리즈로, 지능형 에이전트 애플리케이션을 위해 설계되었습니다. 이 모델은 추론, 코딩, 에이전트 기능을 단일 모델에 통합하여 복잡한 작업을 처리할 수 있도록 최적화되었습니다. GLM-4.5와 경량화된 GLM-4.5-Air 두 가지 버전이 제공됩니다.</p>
<ul>
<li><strong>출시일</strong>: 2025년 7월 28일</li>
<li><strong>라이선스</strong>: MIT (오픈소스로 상업적 사용 및 2차 개발 가능)</li>
<li><strong>접근성</strong>: <a href="http://z.ai/">Z.ai</a> 플랫폼, <a href="http://z.ai/">Z.ai</a> API, HuggingFace, ModelScope에서 사용 가능</li>
<li><strong>커뮤니티</strong>: WeChat 및 Discord 커뮤니티 참여 가능</li>
</ul>
<h2 id="모델-구성">모델 구성</h2>
<p>GLM-4.5 시리즈는 Mixture-of-Experts (MoE) 아키텍처를 기반으로 하며, 두 가지 모델로 구성됩니다:</p>
<ul>
<li><strong>GLM-4.5</strong>: 총 3550억 파라미터, 활성 파라미터 320억</li>
<li><strong>GLM-4.5-Air</strong>: 총 1060억 파라미터, 활성 파라미터 120억 (경량화로 소비자 GPU에서도 실행 가능, 32~64GB VRAM 지원)</li>
</ul>
<h3 id="주요-특징">주요 특징</h3>
<ul>
<li><strong>하이브리드 추론 모드</strong>:
<ul>
<li><strong>Thinking Mode</strong>: 복잡한 추론 및 도구 사용에 최적화.</li>
<li><strong>Non-Thinking Mode</strong>: 즉각적인 응답에 적합.</li>
<li>사용자는 <code>reasoning enabled</code> 불리언으로 모드 제어 가능</li>
</ul>
</li>
<li><strong>컨텍스트 길이</strong>: 최대 128,000 토큰 지원</li>
<li><strong>도구 호출</strong>: 웹 브라우징, 소프트웨어 개발, 프론트엔드 개발 등에 최적화된 도구 호출 기능</li>
<li><strong>성능</strong>: 12개 벤치마크(MMLU Pro, GSM8K, HumanEval 등)에서 평균 점수 63.2로 글로벌 3위, 오픈소스 모델 중 1위. GLM-4.5-Air는 1000억 파라미터급 모델 중 선두(평균 점수 59.8)</li>
</ul>
<h2 id="기술적-세부사항">기술적 세부사항</h2>
<h3 id="학습-파이프라인">학습 파이프라인</h3>
<ul>
<li><strong>사전 학습</strong>: 15조 토큰의 일반 도메인 데이터로 학습.</li>
<li><strong>미세 조정</strong>: 코드, 추론, 에이전트 작업 관련 데이터셋으로 타겟팅된 미세 조정.</li>
<li><strong>강화 학습(RL)</strong>:
<ul>
<li>단일 단계 RL로 64K 컨텍스트에서 난이도 기반 커리큘럼 적용.</li>
<li>동적 샘플링 온도와 적응형 클리핑으로 안정성 확보.</li>
<li>에이전트 작업(정보 검색 QA, 소프트웨어 엔지니어링)에 특화된 RL 적용</li>
</ul>
</li>
<li><strong>전문가 증류</strong>: 추론, 코딩, 에이전트 능력을 통합하여 전반적인 성능 강화</li>
</ul>
<h3 id="성능-최적화">성능 최적화</h3>
<ul>
<li><strong>다중 토큰 예측(MTP)</strong> 및 추측적 디코딩: 최대 8배 빠른 추론 속도(고속 API에서 100~200 토큰/초)</li>
<li><strong>파라미터 효율성</strong>: GLM-4.5는 DeepSeek-R1(1/2 파라미터) 및 Kimi-K2(1/3 파라미터)보다 높은 성능 달성</li>
<li><strong>소프트웨어 통합</strong>: vLLM, SGLang, transformers와 통합, FP8 버전 제공</li>
</ul>
<h3 id="벤치마크-성과">벤치마크 성과</h3>
<ul>
<li><strong>추론</strong>: MMLU Pro(0.835), AIME24, MATH 500 등에서 Claude 3.5 Sonnet 및 Kimi K2와 경쟁</li>
<li><strong>코딩</strong>: Claude Code를 사용한 52개 코딩 작업(프론트엔드, 도구 개발, 데이터 분석 등)에서 Kimi K2 대비 53.9%, Qwen3-Coder 대비 80.8% 성공률</li>
<li><strong>에이전트 기능</strong>: 도구 호출 성공률 90.6%, Claude 3.5 Sonnet 및 Kimi K2 상회</li>
<li><strong>중국어 작업</strong>: 중국어 기반 작업에서 일관된 최상위 성과</li>
</ul>
<h2 id="활용-사례">활용 사례</h2>
<ul>
<li><strong>풀스택 개발</strong>: 프론트엔드, 백엔드, 데이터베이스 관리, 웹 애플리케이션 생성</li>
<li><strong>시각 자료 생성</strong>: 슬라이드, 포스터 등 고품질 프레젠테이션 자료 제작</li>
<li><strong>게임 개발</strong>: Flappy Bird 클론과 같은 플레이 가능한 게임 생성</li>
<li><strong>웹 스크래핑</strong>: 이미지 및 데이터 검색을 위한 안정적인 에이전트 기능</li>
</ul>
<h2 id="가격-및-접근성">가격 및 접근성</h2>
<ul>
<li><strong>API 가격</strong>: 입력 토큰 100만 개당 $0.11(약 ¥0.8), 출력 토큰 100만 개당 $0.28(약 ¥2)으로 업계 최저 수준</li>
<li><strong>무료 사용</strong>: <a href="https://chat.z.ai/">chat.z.ai</a>에서 계정 없이 무료 체험 가능</li>
<li><strong>로컬 실행</strong>: GLM-4.5-Air는 48GB RAM(M4 Mac) 또는 32~64GB VRAM GPU에서 실행 가능(3비트/4비트 양자화 지원)</li>
</ul>
<h2 id="한계">한계</h2>
<ul>
<li>일부 벤치마크(MMLU)에서 Claude 및 GPT-4.1에 비해 약간 뒤짐</li>
<li>복잡한 장문 작업에서 GPT-4.1의 깊이에 미치지 못할 수 있음</li>
<li>8x H100 GPU 사용 시 메모리 부족 문제 발생 가능(vLLM에서 <code>-cpu-offload-gb 16</code> 설정 필요)</li>
</ul>
<h2 id="리소스">리소스</h2>
<ul>
<li><strong>공식 웹사이트</strong>: <a href="https://www.z.ai/">https://www.z.ai/</a></li>
<li><strong>GitHub 저장소</strong>: <a href="https://github.com/zai-org/GLM-4.5">https://github.com/zai-org/GLM-4.5</a></li>
<li><strong>HuggingFace</strong>: <a href="https://huggingface.co/zai-org/GLM-4.5">https://huggingface.co/zai-org/GLM-4.5</a></li>
<li><strong>기술 블로그</strong>: <a href="https://docs.z.ai/">https://docs.z.ai/</a></li>
<li><strong>API 문서</strong>: <a href="https://www.z.ai/api">https://www.z.ai/api</a></li>
</ul>
<h2 id="인용">인용</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>@misc{glm4.5_2025,</span></span>
<span class="line"><span>    title={GLM-4.5: Reasoning, Coding, and Agentic Abilities},</span></span>
<span class="line"><span>    author={Z.ai},</span></span>
<span class="line"><span>    journal={Z.ai Technical Blog},</span></span>
<span class="line"><span>    year={2025},</span></span>
<span class="line"><span>    url={&#x3C;https://docs.z.ai/>}</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span></code></pre>
<p>추가 정보 : <a href="https://medium.com/data-science-in-your-pocket/glm-4-5-the-best-open-source-ai-model-beats-kimi-k2-qwen3-b56a5df2ec34">https://medium.com/data-science-in-your-pocket/glm-4-5-the-best-open-source-ai-model-beats-kimi-k2-qwen3-b56a5df2ec34</a></p> </article> </body></html>