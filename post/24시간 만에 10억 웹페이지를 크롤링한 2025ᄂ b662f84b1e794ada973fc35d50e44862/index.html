<!DOCTYPE html><html lang="ko" data-astro-cid-ztig7rse> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>ìƒì„¸</title><link rel="icon" href="/pages/favicon.svg" type="image/svg+xml"><link rel="icon" href="/pages/favicon-32x32.png" sizes="32x32"><link rel="apple-touch-icon" href="/pages/apple-touch-icon.png" sizes="180x180"><style>:root{color-scheme:light dark}body{margin:0;font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif}.wrap[data-astro-cid-ztig7rse]{max-width:860px;margin:0 auto;padding:20px}.topbar[data-astro-cid-ztig7rse]{position:sticky;top:0;backdrop-filter:blur(6px);background:color-mix(in oklab,canvas,transparent 35%);border-bottom:1px solid color-mix(in oklab,canvastext,transparent 90%);z-index:10}.topbar[data-astro-cid-ztig7rse] .inner[data-astro-cid-ztig7rse]{display:flex;align-items:center;gap:8px;padding:10px 20px;max-width:860px;margin:0 auto}.btn[data-astro-cid-ztig7rse]{appearance:none;border:1px solid color-mix(in oklab,canvastext,transparent 85%);background:transparent;color:inherit;border-radius:10px;padding:8px 12px;cursor:pointer;font-size:14px}.btn[data-astro-cid-ztig7rse].primary{background:#111827;color:#fff;border-color:#111827}@media (prefers-color-scheme: dark){.btn[data-astro-cid-ztig7rse].primary{background:#e5e7eb;color:#111827;border-color:#e5e7eb}}.hero[data-astro-cid-ztig7rse]{margin:14px 0 8px;display:none}.hero[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{width:100%;height:auto;border-radius:12px;display:block;background:#f3f4f6}article[data-astro-cid-ztig7rse]{line-height:1.72;font-size:16px}article[data-astro-cid-ztig7rse] :is(h1,h2,h3)[data-astro-cid-ztig7rse]{line-height:1.25;margin:24px 0 10px}article[data-astro-cid-ztig7rse] h1[data-astro-cid-ztig7rse]{font-size:28px}article[data-astro-cid-ztig7rse] h2[data-astro-cid-ztig7rse]{font-size:22px}article[data-astro-cid-ztig7rse] h3[data-astro-cid-ztig7rse]{font-size:18px}article[data-astro-cid-ztig7rse] p[data-astro-cid-ztig7rse]{margin:10px 0}article[data-astro-cid-ztig7rse] img[data-astro-cid-ztig7rse]{max-width:100%;height:auto;border-radius:8px;background:#f3f4f6}article[data-astro-cid-ztig7rse] pre[data-astro-cid-ztig7rse]{overflow:auto;padding:14px;border:1px solid color-mix(in oklab,canvastext,transparent 90%);border-radius:10px;background:color-mix(in oklab,canvastext,transparent 96%)}article[data-astro-cid-ztig7rse] code[data-astro-cid-ztig7rse]:not(pre code){background:color-mix(in oklab,canvastext,transparent 94%);padding:2px 6px;border-radius:6px}article[data-astro-cid-ztig7rse] blockquote[data-astro-cid-ztig7rse]{border-left:3px solid #9CA3AF;margin:8px 0;padding:4px 12px;color:#6b7280}.actions[data-astro-cid-ztig7rse]{display:flex;gap:8px;flex-wrap:wrap;margin:12px 0 18px}
</style></head> <body class="container" style="padding:24px;max-width:900px" data-astro-cid-ztig7rse> <div class="topbar" data-astro-cid-ztig7rse> <div class="inner" data-astro-cid-ztig7rse> <a class="btn" href="/pages/" aria-label="í™ˆìœ¼ë¡œ" data-astro-cid-ztig7rse>â† í™ˆ</a> <a class="btn" id="source" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>ì›ë¬¸ ë³´ê¸°</a> </div> </div> <div class="wrap" data-astro-cid-ztig7rse> <h1 style="margin:10px 0 6px" data-astro-cid-ztig7rse></h1> <div class="hero" id="hero" data-astro-cid-ztig7rse><img alt="" id="heroImg" loading="eager" data-astro-cid-ztig7rse></div> <div class="actions" data-astro-cid-ztig7rse> <a class="btn primary" id="ctaSource" href="#" target="_blank" rel="noopener" style="display:none" data-astro-cid-ztig7rse>ì›ë¬¸ ë°”ë¡œê°€ê¸°</a> </div> <article data-astro-cid-ztig7rse> <h1 id="24ì‹œê°„-ë§Œì—-10ì–µ-ì›¹í˜ì´ì§€ë¥¼-í¬ë¡¤ë§í•œ-2025ë…„í˜•-ëŒ€ê·œëª¨-í¬ë¡¤ëŸ¬">24ì‹œê°„ ë§Œì— 10ì–µ ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•œ 2025ë…„í˜• ëŒ€ê·œëª¨ í¬ë¡¤ëŸ¬</h1>
<p>ë°œê²¬ì¼: 2025/07/24
ì›ë¬¸ URL: <a href="https://andrewkchan.dev/posts/crawler.html">https://andrewkchan.dev/posts/crawler.html</a>
ë¶„ë¥˜: ì¸ì‚¬ì´íŠ¸
ì›ë¬¸ Source: ğŸ”—andrewkchan
ì¦ê²¨ì°¾ê¸°: No</p>
<h3 id="contents-ëª©ì°¨">Contents ëª©ì°¨</h3>
<ul>
<li><a href="https://andrewkchan.dev/posts/crawler.html#">(Top) (ìœ„)</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-1">Problem statement ë¬¸ì œ ì„¤ëª…</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-2">High-level design ë†’ì€ ìˆ˜ì¤€ì˜ ë””ìì¸</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-2.1">Alternatives investigated</a> <a href="https://andrewkchan.dev/posts/crawler.html#section-2.1">ì¡°ì‚¬ëœ ëŒ€ì•ˆ</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-3">Learnings í•™ìŠµ</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-3.1">Parsing is a big bottleneck</a> <a href="https://andrewkchan.dev/posts/crawler.html#section-3.1">êµ¬ë¬¸ ë¶„ì„ì€ í° ë³‘ëª© í˜„ìƒì…ë‹ˆë‹¤.</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-3.2">Fetching: both easier and harder</a> <a href="https://andrewkchan.dev/posts/crawler.html#section-3.2">ê°€ì ¸ì˜¤ê¸°: ë” ì‰½ê³  ë” ì–´ë µìŠµë‹ˆë‹¤.</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-3.3">The big crawl</a> <a href="https://andrewkchan.dev/posts/crawler.html#section-3.3">í° í¬ë¡¤ë§</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-4">Discussion í† ë¡ </a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-4.1">Theory vs. Practice</a> <a href="https://andrewkchan.dev/posts/crawler.html#section-4.1">ì´ë¡  ëŒ€ ì‹¤ìŠµ</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#section-4.2">What now?</a> <a href="https://andrewkchan.dev/posts/crawler.html#section-4.2">ì´ë²ˆì—” ë˜ ë­ì˜ˆìš”?</a></li>
<li><a href="https://andrewkchan.dev/posts/crawler.html#bottom">(Comments) (ëŒ“ê¸€)</a></li>
<li><em>Discussion on <a href="https://www.reddit.com/r/programming/comments/1m1hvh3/crawling_a_billion_web_pages_in_just_over_24/">r/programming</a>.</em>
<a href="https://www.reddit.com/r/programming/comments/1m1hvh3/crawling_a_billion_web_pages_in_just_over_24/">*r/í”„ë¡œê·¸ë˜ë°</a>ì— ëŒ€í•œ í† ë¡ .*</li>
</ul>
<p>For some reason, nobodyâ€™s written about what it takes to crawl a big chunk of the web in a while: the last point of reference I saw was Michael Nielsenâ€™s post from 2012[1].
ì–´ë–¤ ì´ìœ ì—ì„œì¸ì§€ í•œë™ì•ˆ ì›¹ì˜ í° ë©ì–´ë¦¬ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ë° í•„ìš”í•œ ê²ƒì— ëŒ€í•´ ì•„ë¬´ë„ ì“°ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: ë‚´ê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ ë³¸ ì°¸ì¡° ì§€ì ì€ 2012ë…„ ë§ˆì´í´ ë‹ìŠ¨ì˜ ê²Œì‹œë¬¼ì´ì—ˆìŠµë‹ˆë‹¤[1].</p>
<p>Obviously lots of things have changed since then. Most bigger, better, faster: CPUs have gotten a lot more cores, spinning disks have been replaced by NVMe solid state drives with near-RAM I/O bandwidth, network pipe widths have exploded, EC2 has gone from a tasting menu of instance types to a whole rolodexâ€™s worth, yada yada. But some harder: much more of the web is dynamic, with heavier content too. How has the state of the art changed? Have the bottlenecks shifted, and would it still cost <a href="https://news.ycombinator.com/item?id=17461384">~$41k to bootstrap your own Google</a>? I wanted to find out, so I built and ran my own web crawler1 under similar constraints.
ë¶„ëª…íˆ ê·¸ ì´í›„ë¡œ ë§ì€ ê²ƒë“¤ì´ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤. ê°€ì¥ í¬ê³ , ë” ì¢‹ê³ , ë” ë¹ ë¦„: CPUëŠ” í›¨ì”¬ ë” ë§ì€ ì½”ì–´ë¥¼ ì–»ì—ˆê³ , íšŒì „í•˜ëŠ” ë””ìŠ¤í¬ëŠ” RAMì— ê°€ê¹Œìš´ I/O ëŒ€ì—­í­ì„ ê°€ì§„ NVMe ì†”ë¦¬ë“œ ìŠ¤í…Œì´íŠ¸ ë“œë¼ì´ë¸Œë¡œ ëŒ€ì²´ë˜ì—ˆìœ¼ë©°, ë„¤íŠ¸ì›Œí¬ íŒŒì´í”„ ë„ˆë¹„ëŠ” í­ë°œì ìœ¼ë¡œ ì¦ê°€í–ˆìœ¼ë©°, EC2ëŠ” ì¸ìŠ¤í„´ìŠ¤ ìœ í˜•ì˜ ì‹œì‹ ë©”ë‰´ì—ì„œ ì „ì²´ ë¡¤ë¡œë±ìŠ¤ì˜ ê°€ì¹˜ë¡œ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë” ì–´ë ¤ìš´ ì ì€ í›¨ì”¬ ë” ë§ì€ ì›¹ì´ ì—­ë™ì ì´ë©° ì½˜í…ì¸ ë„ ë” ë¬´ê±°ì›Œì§„ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ìµœì²¨ë‹¨ ê¸°ìˆ ì€ ì–´ë–»ê²Œ ë³€í–ˆìŠµë‹ˆê¹Œ? ë³‘ëª© í˜„ìƒì´ ë°”ë€Œì—ˆê³  <a href="https://news.ycombinator.com/item?id=17461384">ìì‹ ì˜ Googleì„ ë¶€íŠ¸ìŠ¤íŠ¸ë©í•˜ëŠ” ë° ì—¬ì „íˆ ~$41k</a>ì˜ ë¹„ìš©ì´ ë“¤ê¹Œìš”? ì•Œê³  ì‹¶ì–´ì„œ ë¹„ìŠ·í•œ ì œì•½ ì¡°ê±´ì—ì„œ ë‚˜ë§Œì˜ ì›¹ í¬ë¡¤ëŸ¬1ì„ êµ¬ì¶•í•˜ê³  ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.</p>
<h1 id="problem-statement-ë¬¸ì œ-ì„¤ëª…">Problem statement ë¬¸ì œ ì„¤ëª…</h1>
<p><strong>Time limit of 24 hours</strong>. Because I thought a billion pages crawled in a day was achievable based on preliminary experiments and 40 hours doesnâ€™t sound as cool. In my final crawl, the average active time of each machine was 25.5 hours with a tiny bit of variance. This doesnâ€™t include a few hours for some machines that had to be restarted.
ì‹œê°„ <strong>ì œí•œì€ 24ì‹œê°„</strong>ì…ë‹ˆë‹¤. ì˜ˆë¹„ ì‹¤í—˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë£¨ì— 10ì–µ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ê²ƒì´ ë‹¬ì„± ê°€ëŠ¥í•˜ë‹¤ê³  ìƒê°í–ˆê¸° ë•Œë¬¸ì— 40ì‹œê°„ì€ ê·¸ë‹¤ì§€ ë©‹ì§€ì§€ ì•Šê²Œ ë“¤ë¦½ë‹ˆë‹¤. ë§ˆì§€ë§‰ í¬ë¡¤ë§ì—ì„œ ê° ê¸°ê³„ì˜ í‰ê·  í™œì„± ì‹œê°„ì€ 25.5ì‹œê°„ì´ì—ˆê³  ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ë‹¤ì‹œ ì‹œì‘í•´ì•¼ í•˜ëŠ” ì¼ë¶€ ì‹œìŠ¤í…œì˜ ê²½ìš° ëª‡ ì‹œê°„ì´ í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>
<p><strong>Budget of a few hundred dollars</strong>. Nielsenâ€™s crawl cost a bit under $580. Iâ€™m lucky enough to have some disposable income saved up, and aimed for my final crawl to fit in the same. The final run including only the 25.5 active hours cost about $462. I also ran a bunch of small-scale experiments while optimizing the single-node system (which cost much less) and a second large-scale experiment to see how far I could take vertical scaling (which I cut off early, but was in the same ballpark).
ìˆ˜<strong>ë°± ë‹¬ëŸ¬ì˜ ì˜ˆì‚°</strong>. ë‹ìŠ¨ì˜ í¬ë¡¤ë§ ë¹„ìš©ì€ 580ë‹¬ëŸ¬ ë¯¸ë§Œì´ì—ˆìŠµë‹ˆë‹¤. ë‚˜ëŠ” ìš´ì´ ì¢‹ê²Œë„ ê°€ì²˜ë¶„ ì†Œë“ì„ ì €ì¶•í•  ìˆ˜ ìˆì—ˆê³  ë§ˆì§€ë§‰ í¬ë¡¤ë§ì´ ê°™ì€ ê³³ì— ë§ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆìŠµë‹ˆë‹¤. ì´ 25.5ì‹œê°„ì˜ í™œì„± ì‹œê°„ë§Œ í¬í•¨í•œ ìµœì¢… ì‹¤í–‰ ë¹„ìš©ì€ ì•½ $462ì…ë‹ˆë‹¤. ë˜í•œ ë‹¨ì¼ ë…¸ë“œ ì‹œìŠ¤í…œ(ë¹„ìš©ì´ í›¨ì”¬ ì ìŒ)ê³¼ ë‘ ë²ˆì§¸ ì‹œìŠ¤í…œì„ ìµœì í™”í•˜ë©´ì„œ ì†Œê·œëª¨ ì‹¤í—˜ì„ ë§ì´ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤ ìˆ˜ì§ ìŠ¤ì¼€ì¼ë§ì„ ì–¼ë§ˆë‚˜ ë©€ë¦¬ í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³´ê¸° ìœ„í•œ ëŒ€ê·œëª¨ ì‹¤í—˜(ì¼ì° ì°¨ë‹¨í–ˆì§€ë§Œ ê°™ì€ ì•¼êµ¬ì¥ì— ìˆì—ˆìŠµë‹ˆë‹¤).</p>
<p><strong>HTML only</strong>. The elephant in the room. <a href="https://web.archive.org/web/20170607105910/https://sonniesedge.co.uk/blog/a-day-without-javascript">Even by 2017</a> much of the web had come to require JavaScript. But I wanted an apples-to-apples comparison with older web crawls, and in any case, I was doing this as a side project and didnâ€™t have time to add and optimize a bunch of playwright workers. So I did things the old fashioned way: request all links, but donâ€™t run any JS - just parse the HTML as-is and add all links from <code>&#x3C;a></code> tags to the frontier. I was also curious how much of the web can still be crawled this way; as it turns out a lot!
<strong>HTMLë§Œ í•´ë‹¹</strong>ë©ë‹ˆë‹¤. ë°© ì•ˆì˜ ì½”ë¼ë¦¬. <a href="https://web.archive.org/web/20170607105910/https://sonniesedge.co.uk/blog/a-day-without-javascript">2017ë…„ì—ë„</a> ë§ì€ ì›¹ì´ JavaScriptë¥¼ ìš”êµ¬í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì €ëŠ” ì´ì „ ì›¹ í¬ë¡¤ë§ê³¼ ì‚¬ê³¼ ëŒ€ ì‚¬ê³¼ë¥¼ ë¹„êµí•˜ê³  ì‹¶ì—ˆê³ , ì–´ì¨Œë“  ì´ ì‘ì—…ì„ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¡œ ìˆ˜í–‰í•˜ê³  ìˆì—ˆê³  ë§ì€ ê·¹ì‘ê°€ ì‘ì—…ìë¥¼ ì¶”ê°€í•˜ê³  ìµœì í™”í•  ì‹œê°„ì´ ì—†ì—ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ë‚˜ëŠ” êµ¬ì‹ ë°©ì‹ìœ¼ë¡œ ì¼ì„ í–ˆìŠµë‹ˆë‹¤ : ëª¨ë“  ë§í¬ë¥¼ ìš”ì²­í•˜ë˜ JSë¥¼ ì‹¤í–‰í•˜ì§€ ë§ˆì‹­ì‹œì˜¤ - HTMLì„ ìˆëŠ” ê·¸ëŒ€ë¡œ êµ¬ë¬¸ ë¶„ì„í•˜ê³  <code>&#x3C;a></code> íƒœê·¸ì˜ ëª¨ë“  ë§í¬ë¥¼ í”„ë¡ í‹°ì–´ì— ì¶”ê°€í•©ë‹ˆë‹¤. ë‚˜ëŠ” ë˜í•œ ì–¼ë§ˆë‚˜ ë§ì€ ì›¹ì´ ì—¬ì „íˆ ì´ëŸ° ì‹ìœ¼ë¡œ í¬ë¡¤ë§ë  ìˆ˜ ìˆëŠ”ì§€ ê¶ê¸ˆí–ˆìŠµë‹ˆë‹¤. ë§ì€ ê²ƒìœ¼ë¡œ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤!</p>
<p><strong>Politeness</strong>. This is super important! Iâ€™ve read a couple stories (<a href="https://drewdevault.com/2025/03/17/2025-03-17-Stop-externalizing-your-costs-on-me.html">example</a>) about how much pain is caused to admins by massive web crawls that donâ€™t respect robots.txt, spoof other agents to evade blocks, and relentlessly hammer endpoints. I followed prior art: I adhered to robots.txt, added an informative user agent containing my contact information, maintained a list of excluded domains which I would add to on request, stuck to my seed list of the top 1 million domains to avoid hitting mom-and-pop sites, and enforced a 70 second minimum delay between hitting the same domain.
<strong>ê³µì†í•¨</strong>. ì´ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤! ë‚˜ëŠ” ì–¼ë§ˆë‚˜ ë§ì€ ê²ƒì— ëŒ€í•œ ëª‡ ê°€ì§€ ì´ì•¼ê¸°(<a href="https://drewdevault.com/2025/03/17/2025-03-17-Stop-externalizing-your-costs-on-me.html">ì˜ˆ</a>)ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤. robots.txt ì¡´ì¤‘í•˜ì§€ ì•ŠëŠ” ëŒ€ê·œëª¨ ì›¹ í¬ë¡¤ë§, ì°¨ë‹¨ì„ íšŒí”¼í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë¥¼ ìŠ¤í‘¸í•‘í•˜ê³  ì—”ë“œí¬ì¸íŠ¸ë¥¼ ê°€ì°¨ì—†ì´ ë§ì¹˜ì§ˆí•˜ëŠ” ëŒ€ê·œëª¨ ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ì¸í•´ ê´€ë¦¬ìì—ê²Œ ê³ í†µì´ ë°œìƒí•©ë‹ˆë‹¤. ì¢…ë˜ ê¸°ìˆ ì„ ë”°ëë‹¤: ê³ ìˆ˜í–ˆë‹¤ robots.txt ìœ„í•´ ë‚´ ì—°ë½ì²˜ ì •ë³´ê°€ í¬í•¨ëœ ìœ ìµí•œ ì‚¬ìš©ì ì—ì´ì „íŠ¸ë¥¼ ì¶”ê°€í•˜ê³ , ìš”ì²­ ì‹œ ì¶”ê°€í•  ì œì™¸ëœ ë„ë©”ì¸ ëª©ë¡ì„ ìœ ì§€í•˜ê³ , ìƒìœ„ 100ë§Œ ê°œì˜ ì‹œë“œ ëª©ë¡ì— ë¶™ì˜€ìŠµë‹ˆë‹¤. ë„ë©”ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì†Œê·œëª¨ ì‚¬ì´íŠ¸ì— ë„ë‹¬í•˜ì§€ ì•Šë„ë¡ í•˜ê³  ë™ì¼í•œ ë„ë©”ì¸ì— ë„ë‹¬í•˜ëŠ” ì‚¬ì´ì— ìµœì†Œ 70ì´ˆì˜ ì§€ì—°ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.</p>
<p><strong>Fault-tolerance</strong>. This was important in case I needed to stop and resume the crawl for whatever reason (which I did). It also helped a lot for experiments because in my one-time crawl procedure, the performance characteristics were state-dependent: the beginning of the crawl looked pretty different than steady-state. I didnâ€™t aim for perfect fault tolerance; losing some visited sites in the recovery after a crash or failure was fine, because my crawl was fundamentally a sample of the web.
<strong>ë‚´ê²°í•¨ì„±</strong>. ì´ê²ƒì€ ì–´ë–¤ ì´ìœ ë¡œë“  í¬ë¡¤ë§ì„ ì¤‘ì§€í•˜ê³  ì¬ê°œí•´ì•¼ í•˜ëŠ” ê²½ìš°ì— ì¤‘ìš”í–ˆìŠµë‹ˆë‹¤. ë˜í•œ ì¼íšŒì„± í¬ë¡¤ë§ì—ì„œ ì‹¤í—˜ì— ë§ì€ ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ì ˆì°¨ì—ì„œ ì„±ëŠ¥ íŠ¹ì„±ì€ ìƒíƒœì— ë”°ë¼ ë‹¬ë¼ì¡ŒìŠµë‹ˆë‹¤: í¬ë¡¤ë§ì˜ ì‹œì‘ì€ ì •ìƒ ìƒíƒœì™€ ìƒë‹¹íˆ ë‹¤ë¥´ê²Œ ë³´ì˜€ìŠµë‹ˆë‹¤. ë‚˜ëŠ” ì™„ë²½í•œ ë‚´ê²°í•¨ì„±ì„ ëª©í‘œë¡œ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë¶€ ì†ì‹¤ ì¶©ëŒ ë˜ëŠ” ì‹¤íŒ¨ í›„ ë³µêµ¬ ì‹œ ë°©ë¬¸í•œ ì‚¬ì´íŠ¸ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì›¹ì˜ ìƒ˜í”Œì´ì—ˆê¸° ë•Œë¬¸ì— ê´œì°®ì•˜ìŠµë‹ˆë‹¤.</p>
<h1 id="high-level-design-ë†’ì€-ìˆ˜ì¤€ì˜-ë””ìì¸">High-level design ë†’ì€ ìˆ˜ì¤€ì˜ ë””ìì¸</h1>
<p>The design I ended up with looked pretty different than the typical crawler solution for systems design interviews, which generally disaggregates the functions (parsing, fetching, datastore, crawl state) into totally separate machine pools. What I went with instead was a cluster of a dozen highly-optimized independent nodes, each of which contained all the crawler functionality and handled a shard of domains. I did this because:
ë‚´ê°€ ëë‚¸ ë””ìì¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ê¸°ëŠ¥(êµ¬ë¬¸ ë¶„ì„, ê°€ì ¸ì˜¤ê¸°, ë°ì´í„° ì €ì¥ì†Œ, í¬ë¡¤ë§ ìƒíƒœ)ì„ ì™„ì „íˆ ë³„ë„ì˜ ë¨¸ì‹  í’€ë¡œ ë¶„í•´í•˜ëŠ” ì‹œìŠ¤í…œ ì„¤ê³„ ì¸í„°ë·°ë¥¼ ìœ„í•œ ì¼ë°˜ì ì¸ í¬ë¡¤ëŸ¬ ì†”ë£¨ì…˜ê³¼ ìƒë‹¹íˆ ë‹¬ë¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ëŒ€ì‹  ì œê°€ ì„ íƒí•œ ê²ƒì€ ê³ ë„ë¡œ ìµœì í™”ëœ 12ê°œì˜ ë…ë¦½ ë…¸ë“œë¡œ êµ¬ì„±ëœ í´ëŸ¬ìŠ¤í„°ì˜€ìœ¼ë©°, ê° ë…¸ë“œì—ëŠ” ëª¨ë“  í¬ë¡¤ëŸ¬ ê¸°ëŠ¥ì´ í¬í•¨ë˜ì–´ ìˆê³  ë„ë©”ì¸ ìƒ¤ë“œë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ë‚˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ ì´ê²ƒì„ í–ˆìŠµë‹ˆë‹¤.</p>
<ul>
<li>I was operating under a limited budget for both experiments and the final run, so it made sense for me to start small, pack as much as possible onto a single machine, and then scale that up.
ì‹¤í—˜ê³¼ ìµœì¢… ì‹¤í–‰ ëª¨ë‘ì— ëŒ€í•´ ì œí•œëœ ì˜ˆì‚°ìœ¼ë¡œ ìš´ì˜í•˜ê³  ìˆì—ˆê¸° ë•Œë¬¸ì— ì‘ê²Œ ì‹œì‘í•˜ì—¬ ê°€ëŠ¥í•œ í•œ ë§ì€ ê²ƒì„ ë‹¨ì¼ ê¸°ê³„ì— ë‹´ì€ ë‹¤ìŒ í™•ì¥í•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì´ì—ˆìŠµë‹ˆë‹¤.</li>
<li>Iâ€™d actually started with the goal of maximizing performance of a single machine rather than the goal above of a billion pages in 24 hours (which I added halfway through). Even after adding that goal, I was still really optimistic about vertical scaling, and only gave up and moved to a cluster design when I started to approach my self-imposed deadline.
ë‚˜ëŠ” ì‹¤ì œë¡œ 24ì‹œê°„ ì•ˆì— 10ì–µ í˜ì´ì§€ë¼ëŠ” ëª©í‘œë³´ë‹¤ëŠ” ë‹¨ì¼ ì»´í“¨í„°ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ëª©í‘œë¡œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤(ì¤‘ê°„ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤). ê·¸ ëª©í‘œë¥¼ ì¶”ê°€í•œ í›„ì—ë„ ì €ëŠ” ì—¬ì „íˆ ìˆ˜ì§ í™•ì¥ì— ëŒ€í•´ ì •ë§ ë‚™ê´€ì ì´ì—ˆê³ , ìŠ¤ìŠ¤ë¡œ ì •í•œ ë§ˆê°ì¼ì´ ë‹¤ê°€ì˜¤ê¸° ì‹œì‘í–ˆì„ ë•Œì—ì•¼ í¬ê¸°í•˜ê³  í´ëŸ¬ìŠ¤í„° ë””ìì¸ìœ¼ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.</li>
</ul>
<p><img src="https://andrewkchan.dev/posts/crawler-assets/crawler-design.png" alt=""></p>
<p>In detail, each node consisted of the following:
ì„¸ë¶€ì ìœ¼ë¡œ ê° ë…¸ë“œëŠ” ë‹¤ìŒìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.</p>
<ul>
<li><strong>A single redis instance</strong> storing data structures representing the crawl state:
í¬ë¡¤ë§ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° êµ¬ì¡°ë¥¼ ì €ì¥í•˜ëŠ” <strong>ë‹¨ì¼ redis ì¸ìŠ¤í„´ìŠ¤</strong>:
<ul>
<li><strong>Per-domain frontiers</strong>, or lists of URLs to crawl
<strong>ë„ë©”ì¸ë³„ í”„ë¡ í‹°ì–´</strong> ë˜ëŠ” í¬ë¡¤ë§í•  URL ëª©ë¡</li>
<li><strong>Queue of domains</strong> ordered by the next timestamp at which they could be fetched based on their crawl delay2
í¬ë¡¤ë§ ì§€ì—°ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ë‹¤ìŒ íƒ€ì„ìŠ¤íƒ¬í”„ë³„ë¡œ ì •ë ¬ëœ <strong>ë„ë©”ì¸ í</strong> 2</li>
<li><strong>Entries for all visited URLs</strong>, with each URL associated with some metadata and path to the saved content3 on disk
<strong>ë°©ë¬¸í•œ ëª¨ë“  URLì— ëŒ€í•œ í•­ëª©</strong>, ê° URLì€ ì¼ë¶€ ë©”íƒ€ë°ì´í„°ì™€ ì—°ê²°ëœ í•­ëª© ë° ë””ìŠ¤í¬ì— ì €ì¥ëœ ì½˜í…ì¸ 3ì˜ ê²½ë¡œ</li>
<li><strong>Seen URLs bloom filter</strong> so that we could quickly determine whether a URL had been added the frontier already. This was separate from the visited entries because we didnâ€™t want to add a URL to a frontier if it was already there, but not yet fetched. The small probability of the bloom filter giving false positives4 was fine because again, Iâ€™d decided my crawl was a sample of the internet, and I was optimizing for speed.
<strong>ë³¸ URL ë¸”ë£¸ í•„í„°</strong>ë¥¼ ì‚¬ìš©í•˜ë©´ URLì´ ì´ë¯¸ í”„ë¡ í‹°ì–´ì— ì¶”ê°€ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ì´ë¯¸ ì¡´ì¬í•˜ì§€ë§Œ ì•„ì§ ê°€ì ¸ì˜¤ì§€ ì•Šì€ ê²½ìš° í”„ë¡ í‹°ì–´ì— URLì„ ì¶”ê°€í•˜ê³  ì‹¶ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ë°©ë¬¸í•œ í•­ëª©ê³¼ ë³„ê°œì˜€ìŠµë‹ˆë‹¤. ë¸”ë£¸ í•„í„°ê°€ ì˜¤íƒ4ë¥¼ ì¤„ í™•ë¥ ì€ ì ì—ˆëŠ”ë°, ë‹¤ì‹œ ë§í•˜ì§€ë§Œ, ë‚´ í¬ë¡¤ë§ì´ ì¸í„°ë„·ì˜ ìƒ˜í”Œì´ë¼ê³  ê²°ì •í•˜ê³  ì†ë„ë¥¼ ìµœì í™”í•˜ê³  ìˆì—ˆê¸° ë•Œë¬¸ì— ê´œì°®ì•˜ìŠµë‹ˆë‹¤.</li>
<li><strong>Domain metadata</strong>, including whether a domain was manually excluded, part of the original seed list, and the full content of its robots.txt (+ robots expiration timestamp).
ë„ë©”ì¸ì´ ìˆ˜ë™ìœ¼ë¡œ ì œì™¸ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€, ì›ë˜ ì‹œë“œ ëª©ë¡ì˜ ì¼ë¶€, robots.txtì˜ ì „ì²´ ì½˜í…ì¸ (+ ë¡œë´‡ ë§Œë£Œ íƒ€ì„ìŠ¤íƒ¬í”„)ë¥¼ í¬í•¨í•œ <strong>ë„ë©”ì¸ ë©”íƒ€ë°ì´í„°</strong>ì…ë‹ˆë‹¤.</li>
<li><strong>Parse queue</strong> containing the fetched HTML pages for the parsers to process.
êµ¬ë¬¸ ë¶„ì„ê¸°ê°€ ì²˜ë¦¬í•  ê°€ì ¸ì˜¤ê¸° HTML í˜ì´ì§€ê°€ í¬í•¨ëœ <strong>ëŒ€ê¸°ì—´</strong>ì…ë‹ˆë‹¤.</li>
</ul>
</li>
<li><strong>Pool of fetcher processes:</strong>
<strong>í˜ì²˜ í”„ë¡œì„¸ìŠ¤ í’€:</strong>
<ul>
<li>Fetchers operated in a simple loop: pop the next ready domain from redis, then pop the next URL from its frontier and fetch it (+ replace the domain in the ready queue), then push the result onto the parse queue.
ê°„ë‹¨í•œ ë£¨í”„ì—ì„œ ì‘ë™í•˜ëŠ” í˜ì²˜ëŠ” redisì—ì„œ ë‹¤ìŒ ì¤€ë¹„ ë„ë©”ì¸ì„ íŒí•œ ë‹¤ìŒ í”„ë¡ í‹°ì–´ì—ì„œ ë‹¤ìŒ URLì„ íŒí•˜ì—¬ ê°€ì ¸ì˜¨ ë‹¤ìŒ (+ ì¤€ë¹„ ëŒ€ê¸°ì—´ì˜ ë„ë©”ì¸ êµì²´) ê²°ê³¼ë¥¼ êµ¬ë¬¸ ë¶„ì„ ëŒ€ê¸°ì—´ì— í‘¸ì‹œí•©ë‹ˆë‹¤.</li>
<li>Each process packed high concurrency onto a single core via asyncio; I empirically found fetchers could support 6000-7000 â€œworkersâ€ (independent asynchronous fetch loop). Note this didnâ€™t come close to saturating network bandwidth: the bottleneck was the CPU, which Iâ€™ll go into later. The async design is a form of user-space multitasking and has been popular for a while for high concurrency systems (<a href="https://en.wikipedia.org/wiki/Tornado_(web_server)">Python Tornado</a> came out in 2009!) because it avoids context switching entirely.
ê° í”„ë¡œì„¸ìŠ¤ëŠ” asyncioë¥¼ í†µí•´ ë‹¨ì¼ ì½”ì–´ì— ë†’ì€ ë™ì‹œì„±ì„ í¬ì¥í–ˆìŠµë‹ˆë‹¤. ê²½í—˜ì ìœ¼ë¡œ í˜ì²˜ê°€ 6000-7000 ê°œì˜ â€œì‘ì—…ìâ€(ë…ë¦½ì  ì¸ ë¹„ë™ê¸° í˜ì¹˜ ë£¨í”„)ë¥¼ ì§€ì›í•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ì„ í¬í™”ì‹œí‚¤ëŠ” ë° ê·¼ì ‘í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: ë³‘ëª© í˜„ìƒì€ ë‚˜ì¤‘ì— ì„¤ëª…í•  CPUì˜€ìŠµë‹ˆë‹¤. ë¹„ë™ê¸° ë””ìì¸ì€ ì‚¬ìš©ì ê³µê°„ ë©€í‹°íƒœìŠ¤í‚¹ì˜ í•œ í˜•íƒœì´ë©° ì»¨í…ìŠ¤íŠ¸ ì „í™˜ì„ ì™„ì „íˆ í”¼í•˜ê¸° ë•Œë¬¸ì— ë™ì‹œì„±ì´ ë†’ì€ ì‹œìŠ¤í…œ(<a href="https://en.wikipedia.org/wiki/Tornado_(web_server)">Python Tornado</a>ëŠ” 2009ë…„ì— ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤!)ì—ì„œ í•œë™ì•ˆ ì¸ê¸°ë¥¼ ëŒì—ˆìŠµë‹ˆë‹¤.</li>
<li>Both fetchers and parsers also maintained LRU caches of important domain data such as robots.txt content so as to minimize load on redis.
ë˜í•œ í˜ì²˜ì™€ íŒŒì„œ ëª¨ë‘ redisì˜ ë¶€í•˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ robots.txt ì½˜í…ì¸ ì™€ ê°™ì€ ì¤‘ìš”í•œ ë„ë©”ì¸ ë°ì´í„°ì˜ LRU ìºì‹œë¥¼ ìœ ì§€ ê´€ë¦¬í–ˆìŠµë‹ˆë‹¤.</li>
</ul>
</li>
<li><strong>Pool of parser processes:</strong>
<strong>íŒŒì„œ í”„ë¡œì„¸ìŠ¤ í’€:</strong>
<ul>
<li>Parsers operated similarly to fetchers; each consisted of 80 async workers pulling the next item from the parse queue, parsing the HTML content, extracting links to write back to the appropriate domain frontiers in redis, and writing the saved content to persistent storage. The reason the concurrency was much lower is because parsing is CPU-bound rather than IO-bound (although parsers still needed to talk to redis and occasionally fetch robots.txt), and 80 workers was enough to saturate the CPU.
íŒŒì„œëŠ” í˜ì²˜ì™€ ìœ ì‚¬í•˜ê²Œ ì‘ë™í–ˆìŠµë‹ˆë‹¤. ê°ê°ì€ êµ¬ë¬¸ ë¶„ì„ ëŒ€ê¸°ì—´ì—ì„œ ë‹¤ìŒ í•­ëª©ì„ ê°€ì ¸ì˜¤ê³ , HTML ì½˜í…ì¸ ë¥¼ êµ¬ë¬¸ ë¶„ì„í•˜ê³ , ë§í¬ë¥¼ ì¶”ì¶œí•˜ì—¬ Redisì˜ ì ì ˆí•œ ë„ë©”ì¸ í”„ë¡ í‹°ì–´ì— ë‹¤ì‹œ ì“°ê³ , ì €ì¥ëœ ì½˜í…ì¸ ë¥¼ ì˜êµ¬ ìŠ¤í† ë¦¬ì§€ì— ì“°ëŠ” 80ê°œì˜ ë¹„ë™ê¸° ì‘ì—…ìë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë™ì‹œì„±ì´ í›¨ì”¬ ë‚®ì€ ì´ìœ ëŠ” êµ¬ë¬¸ ë¶„ì„ì´ IO ë°”ìš´ë“œê°€ ì•„ë‹Œ CPU ë°”ìš´ë“œì´ê¸° ë•Œë¬¸ì´ë©°(íŒŒì„œëŠ” ì—¬ì „íˆ redisì™€ í†µì‹ í•˜ê³  ë•Œë•Œë¡œ robots.txt ê°€ì ¸ì™€ì•¼ í•¨) 80ëª…ì˜ ì‘ì—…ìê°€ CPUë¥¼ í¬í™”ì‹œí‚¤ê¸°ì— ì¶©ë¶„í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</li>
</ul>
</li>
<li><strong>Other: ë‹¤ë¥¸:</strong>
<ul>
<li>For persistent storage, I followed prior art and used instance storage. The textbook interview solution will tell you to use S3; I considered this, but S3 charges per-request as well as pro-rated GB-months, and holding 1 billion pages assuming 250KB per page (250TB total) for just a single day wouldâ€™ve cost <code>0.022*1000*250*(1/30)+0.005*1e6</code> = $5183.33 with the standard tier or <code>0.11*1000*250*(1/30)+0.00113*1e6</code> = $2046.67 with express - an order of magnitude over what I ended up spending! Even ignoring all PUT costs, it wouldâ€™ve been $183.33 at standard or $916.67 at express to hold my data for a day, meaning even if Iâ€™d batched pages together it wouldnâ€™t have been competitive.
ì˜êµ¬ ìŠ¤í† ë¦¬ì§€ì˜ ê²½ìš° ê¸°ì¡´ ê¸°ìˆ ì„ ë”°ë¥´ê³  ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ë¦¬ì§€ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. êµê³¼ì„œ ì¸í„°ë·° ì†”ë£¨ì…˜ì€ S3ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤. ë‚˜ëŠ” ì´ê²ƒì„ ê³ ë ¤í–ˆì§€ë§Œ S3ëŠ” ìš”ì²­ë‹¹ ìš”ê¸ˆê³¼ ë¹„ë¡€ ë°°ë¶„ëœ GB-ì›”ì„ ì²­êµ¬í•˜ë©°, ë‹¨ í•˜ë£¨ ë™ì•ˆ í˜ì´ì§€ë‹¹ 250KB(ì´ 250TB)ë¥¼ ê°€ì •í•  ë•Œ 10ì–µ í˜ì´ì§€ë¥¼ ë³´ìœ í•˜ë©´ ë¹„ìš© <code>0.022*1000*250*(1/30)+0.005*1e6</code> = í‘œì¤€ ê³„ì¸µì˜ ê²½ìš° $5183.33 ë˜ëŠ” <code>0.11*1000*250*(1/30)+0.00113*1e6</code> ìµìŠ¤í”„ë ˆìŠ¤ì˜ ê²½ìš° = $2046.67 - ë‚´ê°€ ì§€ì¶œí•œ ê¸ˆì•¡ë³´ë‹¤ í›¨ì”¬ ë” ë¹„ì •ìƒì ì…ë‹ˆë‹¤! ëª¨ë“  PUT ë¹„ìš©ì„ ë¬´ì‹œí•˜ë”ë¼ë„ í‘œì¤€ìœ¼ë¡œ $183.33 ë˜ëŠ” $916.67ê°€ ë˜ì—ˆì„ ê²ƒì…ë‹ˆë‹¤ Expressì—ì„œ í•˜ë£¨ ë™ì•ˆ ë‚´ ë°ì´í„°ë¥¼ ë³´ê´€í–ˆê¸° ë•Œë¬¸ì— í˜ì´ì§€ë¥¼ í•¨ê»˜ ì¼ê´„ ì²˜ë¦¬í•˜ë”ë¼ë„ ê²½ìŸë ¥ì´ ì—†ì—ˆì„ ê²ƒì…ë‹ˆë‹¤.</li>
<li>I ended up going with the <code>i7i</code> series of storage-optimized instances, and truncated my saved pages to ensure they fit. Obviously truncating wouldnâ€™t be a good idea for a real crawler; I thought about using a fast compression method in the parser like snappy or a slower, background compressor, but didnâ€™t have time to try.
ê²°êµ­ <code>i7i</code> ì‹œë¦¬ì¦ˆ ìŠ¤í† ë¦¬ì§€ì— ìµœì í™”ëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í–ˆê³  ì €ì¥ëœ í˜ì´ì§€ë¥¼ ì˜ë¼ ì í•©í•œì§€ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë¶„ëª…íˆ ìë¥´ëŠ” ê²ƒì€ ì‹¤ì œ ì‚¬ëŒì—ê²Œ ì¢‹ì€ ìƒê°ì´ ì•„ë‹™ë‹ˆë‹¤. í¬ë¡¤ëŸ¬; íŒŒì„œì—ì„œ snappy ë˜ëŠ” ë” ëŠë¦° ë°±ê·¸ë¼ìš´ë“œ ì••ì¶•ê¸°ì™€ ê°™ì€ ë¹ ë¥¸ ì••ì¶• ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì— ëŒ€í•´ ìƒê°í–ˆì§€ë§Œ ì‹œë„ í•  ì‹œê°„ì´ ì—†ì—ˆìŠµë‹ˆë‹¤.</li>
<li>The first fetcher process in the pool was also designated the â€œleaderâ€ and would periodically write metrics to a local prometheus DB. In a real system it wouldâ€™ve been better to have a single metrics DB for all nodes.
í’€ì˜ ì²« ë²ˆì§¸ ê°€ì ¸ì˜¤ê¸° í”„ë¡œì„¸ìŠ¤ë„ â€œë¦¬ë”â€ë¡œ ì§€ì •ë˜ì—ˆìœ¼ë©° ì£¼ê¸°ì ìœ¼ë¡œ ë¡œì»¬ í”„ë¡œë©”í…Œìš°ìŠ¤ DBì— ë©”íŠ¸ë¦­ì„ ì”ë‹ˆë‹¤. ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œëŠ” ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ ë‹¨ì¼ ë©”íŠ¸ë¦­ DBë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì•˜ì„ ê²ƒì…ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
<p>The final cluster consisted of:
ìµœì¢… í´ëŸ¬ìŠ¤í„°ëŠ” ë‹¤ìŒìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<ul>
<li>12 nodes 12 ë…¸ë“œ</li>
<li>Each on an <code>i7i.4xlarge</code> machine with 16 vCPUs, 128GB RAM, 10Gbps network bandwidth, and 3750GB instance storage
ê°ê° 16ê°œì˜ vCPU, 128GB RAM, 10Gbps ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ë° 3750GB ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ë¦¬ì§€ê°€ ìˆëŠ” <code>i7i.4xlarge</code> ë¨¸ì‹ ì— ìˆìŠµë‹ˆë‹¤.</li>
<li>Each centered around 1 redis process + 9 fetcher processes + 6 parser processes
ê°ê°ì€ 1ê°œì˜ redis í”„ë¡œì„¸ìŠ¤ + 9ê°œì˜ í˜ì²˜ í”„ë¡œì„¸ìŠ¤ + 6ê°œì˜ íŒŒì„œ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í–ˆìŠµë‹ˆë‹¤</li>
</ul>
<p>The domain seed list was sharded across the nodes in the cluster with no cross-node communication. Since I also only crawled seeded domains, that meant nodes crawled their own non-overlapping regions of the internet. This was mainly because I ran out of time trying to get my alternate design (with cross-communication) working.
ë„ë©”ì¸ ì‹œë“œ ëª©ë¡ì€ ë…¸ë“œ ê°„ í†µì‹  ì—†ì´ í´ëŸ¬ìŠ¤í„°ì˜ ë…¸ë“œ ê°„ì— ìƒ¤ë”©ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ ì‹œë“œëœ ë„ë©”ì¸ë§Œ í¬ë¡¤ë§í–ˆê¸° ë•Œë¬¸ì— ë…¸ë“œê°€ ì¸í„°ë„·ì˜ ê²¹ì¹˜ì§€ ì•ŠëŠ” ìì²´ ì˜ì—­ì„ í¬ë¡¤ë§í•œë‹¤ëŠ” ì˜ë¯¸ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì£¼ë¡œ ëŒ€ì²´ ë””ìì¸(êµì°¨ í†µì‹  í¬í•¨)ì„ ì‘ë™ì‹œí‚¤ëŠ” ë° ì‹œê°„ì´ ë¶€ì¡±í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</p>
<p>Why just 12 nodes? I found in one experiment that sharding the seed domains too thin led to a serious hot shard problem where some nodes with very popular domains had lots of work to do while others finished quickly. I also stopped the vertical scaling of the fetcher and parser pools at 15 processes total per redis process because redis began to hit 120 ops/sec and Iâ€™d read that any more would cause issues (given more time, I wouldâ€™ve ran experiments to find the exact saturation point).
ì™œ 12ê°œì˜ ë…¸ë“œë§Œ ìˆìŠµë‹ˆê¹Œ? í•œ ì‹¤í—˜ì—ì„œ ì‹œë“œ ë„ë©”ì¸ì„ ë„ˆë¬´ ì–‡ê²Œ ìƒ¤ë”©í•˜ë©´ ë§¤ìš° ì¸ê¸° ìˆëŠ” ë„ë©”ì¸ì„ ê°€ì§„ ì¼ë¶€ ë…¸ë“œëŠ” í•´ì•¼ í•  ì‘ì—…ì´ ë§ê³  ë‹¤ë¥¸ ë…¸ë“œëŠ” ë¹¨ë¦¬ ëë‚˜ëŠ” ì‹¬ê°í•œ í•« ìƒ¤ë“œ ë¬¸ì œê°€ ë°œìƒí•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ redisê°€ ì´ˆë‹¹ 120 opsì— ë„ë‹¬í•˜ê¸° ì‹œì‘í–ˆê³  ë” ì´ìƒ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì½ì—ˆê¸° ë•Œë¬¸ì— redis í”„ë¡œì„¸ìŠ¤ë‹¹ ì´ 15 ê°œì˜ í”„ë¡œì„¸ìŠ¤ì—ì„œ fetcher ë° parser í’€ì˜ ìˆ˜ì§ í™•ì¥ì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤ (ë” ë§ì€ ì‹œê°„ì´ ì£¼ì–´ì§€ë©´ ì •í™•í•œ í¬í™” ì§€ì ì„ ì°¾ê¸° ìœ„í•´ ì‹¤í—˜ì„ ì‹¤í–‰í–ˆì„ ê²ƒì…ë‹ˆë‹¤).</p>
<h3 id="alternatives-investigated">Alternatives investigated</h3>
<p>ì¡°ì‚¬ëœ ëŒ€ì•ˆ</p>
<p>I went through a few different designs before ending up with the one above. It seems like most recent crawlers[1][2][3] use a fast in-memory datastore like Redis, and for good reason. I made small-scale prototypes with SQLite and PostgreSQL backends, but making frontier queries fast was overly complex despite the conceptual simplicity of the data structure. AI coding tools helped with this exploration a lot; Iâ€™ve written about this <a href="https://andrewkchan.dev/posts/systems.html">here</a>.
ìœ„ì˜ ë””ìì¸ìœ¼ë¡œ ëë‚˜ê¸° ì „ì— ëª‡ ê°€ì§€ ë‹¤ë¥¸ ë””ìì¸ì„ ê±°ì³¤ìŠµë‹ˆë‹¤. ê°€ì¥ ìµœê·¼ì˜ í¬ë¡¤ëŸ¬[1][2][3]ëŠ” Redisì™€ ê°™ì€ ë¹ ë¥¸ ë©”ëª¨ë¦¬ ë‚´ ë°ì´í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ ê°™ìœ¼ë©° ê·¸ëŸ´ ë§Œí•œ ì´ìœ ê°€ ìˆìŠµë‹ˆë‹¤. SQLite ë° PostgreSQL ë°±ì—”ë“œë¡œ ì†Œê·œëª¨ í”„ë¡œí† íƒ€ì…ì„ ë§Œë“¤ì—ˆì§€ë§Œ ë°ì´í„° êµ¬ì¡°ì˜ ê°œë…ì  ë‹¨ìˆœì„±ì—ë„ ë¶ˆêµ¬í•˜ê³  í”„ë¡ í‹°ì–´ ì¿¼ë¦¬ë¥¼ ë¹ ë¥´ê²Œ ë§Œë“œëŠ” ê²ƒì€ ì§€ë‚˜ì¹˜ê²Œ ë³µì¡í–ˆìŠµë‹ˆë‹¤. AI ì½”ë”© ë„êµ¬ëŠ” ì´ëŸ¬í•œ íƒìƒ‰ì— ë§ì€ ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚˜ëŠ” <a href="https://andrewkchan.dev/posts/systems.html">ì—¬ê¸°ì—</a> ì´ê²ƒì— ëŒ€í•´ ì¼ìŠµë‹ˆë‹¤.</p>
<p>I also tried pretty hard to make vertically scaling a single node work; I was optimistic about this because so many of the hardware bottlenecks that had restricted past big crawls[1][4] to distributed systems seemed to have disappeared. For instance, AWS offers a <code>i7i.48xlarge</code> instance which is essentially just 12 <code>i7i.4xlarge</code> machines stuck together. It has quite a bit less network bandwidth (100Gbps instead of 12x25Gbps), but at the throughput needed to hit 1 billion pages in 24 hours, even if every page was 1MB (which wasnâ€™t the case), Iâ€™d only be using <code>8*1e6*(1e9/86400)=92Gbps</code>, with room leftover for outbound (which certainly wasnâ€™t 1MB per request!).
ë‚˜ëŠ” ë˜í•œ ë‹¨ì¼ ë…¸ë“œë¥¼ ìˆ˜ì§ìœ¼ë¡œ í™•ì¥í•˜ê¸° ìœ„í•´ ê½¤ ì—´ì‹¬íˆ ë…¸ë ¥í–ˆìŠµë‹ˆë‹¤. ë‚˜ëŠ” ì´ê²ƒì— ëŒ€í•´ ë‚™ê´€ì ì´ì—ˆëŠ”ë°, ê³¼ê±°ì˜ ëŒ€ê·œëª¨ í¬ë¡¤ë§[1][4]ì„ ë¶„ì‚° ì‹œìŠ¤í…œì— ì œí•œí–ˆë˜ ë§ì€ í•˜ë“œì›¨ì–´ ë³‘ëª© í˜„ìƒì´ ì‚¬ë¼ì§„ ê²ƒì²˜ëŸ¼ ë³´ì˜€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, AWSëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 12ê°œì— ë¶ˆê³¼í•œ <code>i7i.48xlarge</code> ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. <code>i7i.4xlarge</code> ë¨¸ì‹ ì´ ì„œë¡œ ë¶™ì–´ ìˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ì€ ìƒë‹¹íˆ ì ì§€ë§Œ(12x25Gbps ëŒ€ì‹  100Gbps) 24ì‹œê°„ ë™ì•ˆ 10ì–µ í˜ì´ì§€ì— ë„ë‹¬í•˜ëŠ” ë° í•„ìš”í•œ ì²˜ë¦¬ëŸ‰ì—ì„œëŠ” ëª¨ë“  í˜ì´ì§€ê°€ 1MBì´ë”ë¼ë„(ê·¸ë ‡ì§€ ì•ŠìŒ) <code>8*1e6*(1e9/86400)=92Gbps</code>ë§Œ ì‚¬ìš©í•˜ê³  ì•„ì›ƒë°”ìš´ë“œë¥¼ ìœ„í•œ ê³µê°„ì´ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤(ìš”ì²­ë‹¹ 1MBëŠ” ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤!).</p>
<p>The first large-scale design I tried packed everything onto a single <code>i7i.48xlarge</code>, organizing processes into â€œpodsâ€ which looked a lot like the nodes in my final cluster (groups of 16 processes with a single redis instance), but with cross-communication allowed. A second design removed the cross-communication and just ran independent pods; a large run with this yielded disappointing results (the entire system managed only 1k pages/sec, which was only a bit over the throughput of a single node in the final cluster). I ran out of my timebox, so gave up and moved to horizontal scaling. I suspect the limiting factor may be more software (operating system resources) rather than hardware.
ë‚´ê°€ ì‹œë„í•œ ì²« ë²ˆì§¸ ëŒ€ê·œëª¨ ì„¤ê³„ëŠ” ëª¨ë“  ê²ƒì„ ë‹¨ì¼ <code>i7i.48xlarge</code>ì— ì••ì¶•í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ìµœì¢… í´ëŸ¬ìŠ¤í„°ì˜ ë…¸ë“œì™€ ë§¤ìš° ìœ ì‚¬í•œ â€œí¬ë“œâ€ë¡œ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤(16ê°œì˜ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹). ë‹¨ì¼ redis ì¸ìŠ¤í„´ìŠ¤)ì´ì§€ë§Œ êµì°¨ í†µì‹ ì´ í—ˆìš©ë©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ì„¤ê³„ëŠ” êµì°¨ í†µì‹ ì„ ì œê±°í•˜ê³  ë…ë¦½ì ì¸ í¬ë“œë¥¼ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ëŒ€ê·œëª¨ë¡œ ì‹¤í–‰í•˜ë©´ ì‹¤ë§ìŠ¤ëŸ¬ìš´ ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤ (ì „ì²´ ì‹œìŠ¤í…œì€ ì´ˆë‹¹ 1k í˜ì´ì§€ë§Œ ê´€ë¦¬í–ˆëŠ”ë°, ì´ëŠ” ìµœì¢… í´ëŸ¬ìŠ¤í„°ì—ì„œ ë‹¨ì¼ ë…¸ë“œì˜ ì²˜ë¦¬ëŸ‰ì„ ì•½ê°„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.) íƒ€ì„ë°•ìŠ¤ê°€ ë‹¤ ë–¨ì–´ì ¸ì„œ í¬ê¸°í•˜ê³  ìˆ˜í‰ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤. ì œí•œ ìš”ì¸ì´ í•˜ë“œì›¨ì–´ë³´ë‹¤ëŠ” ì†Œí”„íŠ¸ì›¨ì–´(ìš´ì˜ ì²´ì œ ë¦¬ì†ŒìŠ¤)ê°€ ë” ë§ì„ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.</p>
<h2 id="learnings-í•™ìŠµ">Learnings í•™ìŠµ</h2>
<h3 id="parsing-is-a-big-bottleneck">Parsing is a big bottleneck</h3>
<p>êµ¬ë¬¸ ë¶„ì„ì€ í° ë³‘ëª© í˜„ìƒì…ë‹ˆë‹¤.</p>
<p>I was really surprised by how much of a bottleneck parsing was. In the final system, I only had to allocate processes in a 2:3 parsing-to-fetching ratio, but it didnâ€™t start that way, and it took many iterations to get there. In fact, in the first system I built with dedicated parsing/fetching processes, 2 parsers were needed to keep up with 1 (partially idle) fetcher with 1000 workers running at 55 pages/sec. It really looked like parsing was going to keep me from hitting a billion on budget!
ë‚˜ëŠ” êµ¬ë¬¸ ë¶„ì„ì´ ì–¼ë§ˆë‚˜ ë§ì€ ë³‘ëª© í˜„ìƒì„ ì¼ìœ¼í‚¤ëŠ”ì§€ ì •ë§ ë†€ëìŠµë‹ˆë‹¤. ìµœì¢… ì‹œìŠ¤í…œì—ì„œëŠ” êµ¬ë¬¸ ë¶„ì„ ëŒ€ ê°€ì ¸ì˜¤ê¸° ë¹„ìœ¨ì´ 2:3ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ë¥¼ í• ë‹¹í•˜ê¸°ë§Œ í•˜ë©´ ë˜ì—ˆì§€ë§Œ ê·¸ë ‡ê²Œ ì‹œì‘ë˜ì§€ ì•Šì•˜ê³  ê±°ê¸°ì— ë„ë‹¬í•˜ëŠ” ë° ë§ì€ ë°˜ë³µì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤. ì‚¬ì‹¤, ì „ìš© êµ¬ë¬¸ ë¶„ì„ / í˜ì¹˜ í”„ë¡œì„¸ìŠ¤ë¡œ êµ¬ì¶• í•œ ì²« ë²ˆì§¸ ì‹œìŠ¤í…œì—ì„œëŠ” 55 í˜ì´ì§€ / ì´ˆë¡œ ì‹¤í–‰ë˜ëŠ” 1000 ëª…ì˜ ì‘ì—…ìì™€ í•¨ê»˜ 1 ê°œì˜ (ë¶€ë¶„ì ìœ¼ë¡œ ìœ íœ´ ìƒíƒœ) í˜ì²˜ë¥¼ ë”°ë¼ì¡ê¸° ìœ„í•´ 2 ê°œì˜ íŒŒì„œê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤. êµ¬ë¬¸ ë¶„ì„ì´ ì˜ˆì‚°ì—ì„œ 10ì–µ ë‹¬ëŸ¬ë¥¼ ë‹¬ì„±í•˜ì§€ ëª»í•˜ê²Œ í•  ê²ƒ ê°™ì•˜ìŠµë‹ˆë‹¤!</p>
<p>This was really surprising to me because it meant my quad-core node wasnâ€™t achieving the same throughput that a weaker quad-core box could in 2012. Profiles showed that parsing was clearly the bottleneck, but I was using the same <code>lxml</code> parsing library that was popular in 2012 (as suggested by Gemini). I eventually figured out that it was because the average web page has gotten a lot bigger: metrics from a test run indicated the P50 uncompressed page size is now 138KB5, while the mean is even larger at 242KB - many times larger than Nielsenâ€™s estimated average of 51KB in 2012!
ì´ê²ƒì€ ë‚´ ì¿¼ë“œ ì½”ì–´ ë…¸ë“œê°€ 2012ë…„ì— ë” ì•½í•œ ì¿¼ë“œ ì½”ì–´ ë°•ìŠ¤ê°€ í•  ìˆ˜ ìˆëŠ” ê²ƒê³¼ ë™ì¼í•œ ì²˜ë¦¬ëŸ‰ì„ ë‹¬ì„±í•˜ì§€ ëª»í–ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í–ˆê¸° ë•Œë¬¸ì— ì •ë§ ë†€ëìŠµë‹ˆë‹¤. í”„ë¡œí•„ì€ êµ¬ë¬¸ ë¶„ì„ì´ ë¶„ëª…íˆ ë³‘ëª© í˜„ìƒì„ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ 2012ë…„ì— ì¸ê¸°ë¥¼ ëŒì—ˆë˜ ê²ƒê³¼ ë™ì¼í•œ <code>lxml</code> êµ¬ë¬¸ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤(Geminiê°€ ì œì•ˆí•œ ëŒ€ë¡œ). í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì˜ ì§€í‘œì— ë”°ë¥´ë©´ P50 ë¹„ì••ì¶• í˜ì´ì§€ í¬ê¸°ëŠ” í˜„ì¬ 138KBì…ë‹ˆë‹¤5, í‰ê· ì€ 242KBë¡œ ë‹ìŠ¨ì˜ ì¶”ì • í‰ê· ë³´ë‹¤ ëª‡ ë°° ë” í° ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. 2012ë…„ì—ëŠ” 51KB!</p>
<p>Two things ended up helping the most:
ë‘ ê°€ì§€ê°€ ê°€ì¥ í° ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<ul>
<li>I switched from <code>lxml</code> to <code>selectolax</code>, a much newer library wrapping Lexbor, a modern parser in C++ designed specifically for HTML5. The page claimed it can be 30 times faster than lxml. It wasnâ€™t 30x overall, but it was a huge boost.
ë‚˜ëŠ” <code>lxml</code>ì—ì„œ HTML5 ìš©ìœ¼ë¡œ íŠ¹ë³„íˆ ì„¤ê³„ëœ C ++ì˜ ìµœì‹  íŒŒì„œ ì¸ Lexborë¥¼ ë˜í•‘í•˜ëŠ” í›¨ì”¬ ìƒˆë¡œìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¸ <code>selectolax</code>ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ëŠ” lxmlë³´ë‹¤ 30ë°° ë¹ ë¦…ë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ 30ë°°ëŠ” ì•„ë‹ˆì—ˆì§€ë§Œ ì—„ì²­ë‚œ í–¥ìƒì´ì—ˆìŠµë‹ˆë‹¤.</li>
<li>I also truncated page content to 250KB before passing it to the parser. Since the truncation threshold is above the mean and nearly double the median, I think the rationale from Nielsen[1] still holds: weâ€™re capturing most web pages in their entirety, which should be enough for most applications.
ë˜í•œ íŒŒì„œì— ì „ë‹¬í•˜ê¸° ì „ì— í˜ì´ì§€ ë‚´ìš©ì„ 250KBë¡œ ì˜ëìŠµë‹ˆë‹¤. ì˜ë¦¼ ì„ê³„ê°’ì´ í‰ê· ë³´ë‹¤ ë†’ê³  ì¤‘ì•™ê°’ì˜ ê±°ì˜ ë‘ ë°°ì— ë‹¬í•˜ê¸° ë•Œë¬¸ì— Nielsen[1]ì˜ ê·¼ê±°ëŠ” ì—¬ì „íˆ ìœ íš¨í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤: ìš°ë¦¬ëŠ” ëŒ€ë¶€ë¶„ì˜ ì›¹ í˜ì´ì§€ ì „ì²´ë¥¼ ìº¡ì²˜í•˜ê³  ìˆìœ¼ë©° ì´ëŠ” ëŒ€ë¶€ë¶„ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì¶©ë¶„í•  ê²ƒì…ë‹ˆë‹¤.</li>
</ul>
<p>With this setup, I was able to achieve ~160 pages parsed per second with a single parser process, which allowed my final setup to use 9x fetchers and 6x parsers to crawl ~950 pages/sec.
ì´ ì„¤ì •ì„ í†µí•´ ë‹¨ì¼ íŒŒì„œ í”„ë¡œì„¸ìŠ¤ë¡œ ì´ˆë‹¹ ~160í˜ì´ì§€ë¥¼ êµ¬ë¬¸ ë¶„ì„í•  ìˆ˜ ìˆì—ˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ìµœì¢… ì„¤ì •ì—ì„œ 9x í˜ì²˜ì™€ 6x íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ~950í˜ì´ì§€/ì´ˆë¥¼ í¬ë¡¤ë§í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</p>
<h3 id="fetching-both-easier-and-harder">Fetching: both easier and harder</h3>
<p>ê°€ì ¸ì˜¤ê¸°: ë” ì‰½ê³  ë” ì–´ë µìŠµë‹ˆë‹¤.</p>
<p>Many treatments of crawling take network bandwidth and DNS to be important bottlenecks. For instance, while researching this topic I emailed <a href="https://www.cs.cmu.edu/~callan/">Jamie Callan</a> from CMU about the Sapphire project from 2009[4]; Professor Callan told me that DNS resolution throughput was a bottleneck, and for a later crawl in 2012 which used the CMU campus network, the crawl throughput had to be throttled to avoid using all of the bandwidth. This <a href="https://www.hellointerview.com/learn/system-design/problem-breakdowns/web-crawler">interview analysis from Evan King</a> from about a year ago also suggests optimizations for DNS resolution.
í¬ë¡¤ë§ì— ëŒ€í•œ ë§ì€ ì²˜ë¦¬ëŠ” ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ê³¼ DNSë¥¼ ì¤‘ìš”í•œ ë³‘ëª© í˜„ìƒìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ ì£¼ì œë¥¼ ì—°êµ¬í•˜ëŠ” ë™ì•ˆ CMUì˜ <a href="https://www.cs.cmu.edu/~callan/">Jamie Callanì—ê²Œ</a> 2009ë…„ ì‚¬íŒŒì´ì–´ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ì´ë©”ì¼ì„ ë³´ëƒˆìŠµë‹ˆë‹¤[4]; Callan êµìˆ˜ëŠ” DNS í™•ì¸ ì²˜ë¦¬ëŸ‰ì´ ë³‘ëª© í˜„ìƒì´ë©° CMU ìº í¼ìŠ¤ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ 2012ë…„ ì´í›„ í¬ë¡¤ë§ì˜ ê²½ìš° ëª¨ë“  ëŒ€ì—­í­ì„ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ í¬ë¡¤ë§ ì²˜ë¦¬ëŸ‰ì„ ì œí•œí•´ì•¼ í•œë‹¤ê³  ë§í–ˆìŠµë‹ˆë‹¤. ì—<a href="https://www.hellointerview.com/learn/system-design/problem-breakdowns/web-crawler">ë°˜ í‚¹ì˜ ì¸í„°ë·° ë¶„ì„</a> ì•½ 1ë…„ ì „ë¶€í„° DNS í™•ì¸ì— ëŒ€í•œ ìµœì í™”ë„ ì œì•ˆí•©ë‹ˆë‹¤.</p>
<p>For my crawl, DNS didnâ€™t come up at all. I think this is because I limited crawling to my seed list of the top ~1 million domains. Network bandwidth also wasnâ€™t near saturated for any of the nodes in the cluster; most nodes averaged around 1 GB/s (8 Gbps) at steady state, but the max bandwidth for <code>i7i.4xlarge</code> is 25 Gbps. Datacenter bandwidth is abundant these days, especially for AI: AWS offers a <a href="https://aws.amazon.com/ec2/instance-types/p6/">P6e-GB200</a> instance with 28.8 <em>terabits</em> of network bandwidth!
ë‚´ í¬ë¡¤ë§ì—ì„œëŠ” DNSê°€ ì „í˜€ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ìƒìœ„ ~100ë§Œ ê°œ ë„ë©”ì¸ì˜ ì‹œë“œ ëª©ë¡ìœ¼ë¡œ ì œí•œí–ˆê¸° ë•Œë¬¸ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ë˜í•œ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ì€ í´ëŸ¬ìŠ¤í„°ì˜ ë…¸ë“œì— ëŒ€í•´ ê±°ì˜ í¬í™” ìƒíƒœê°€ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ë…¸ë“œëŠ” ì •ìƒ ìƒíƒœì—ì„œ í‰ê·  ì•½ 1GB/s(8Gbps)ì˜€ì§€ë§Œ <code>i7i.4xlarge</code>ì˜ ìµœëŒ€ ëŒ€ì—­í­ì€ 25Gbpsì…ë‹ˆë‹¤. ìš”ì¦˜ ë°ì´í„° ì„¼í„° ëŒ€ì—­í­ì€ íŠ¹íˆ AIì˜ ê²½ìš° í’ë¶€í•©ë‹ˆë‹¤: AWSëŠ” 28.8_í…Œë¼ë¹„íŠ¸_ì˜ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ì„ ê°–ì¶˜ <a href="https://aws.amazon.com/ec2/instance-types/p6/">P6e-GB200</a> ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤!</p>
<p>That said, one part of fetching got harder: a LOT more websites use SSL now than a decade ago. This was crystal clear in profiles, with SSL handshake computation showing up as the most expensive function call, taking up a whopping 25% of all CPU time on average, which - given that we werenâ€™t near saturating the network pipes, meant fetching became bottlenecked by the CPU before the network!
ì¦‰, ê°€ì ¸ì˜¤ê¸°ì˜ í•œ ë¶€ë¶„ì€ 10 ë…„ ì „ë³´ë‹¤ í›¨ì”¬ ë” ë§ì€ ì›¹ ì‚¬ì´íŠ¸ê°€ SSLì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” SSL í•¸ë“œì…°ì´í¬ ê³„ì‚°ì´ ê°€ì¥ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” í•¨ìˆ˜ í˜¸ì¶œë¡œ í‘œì‹œë˜ì–´ í‰ê· ì ìœ¼ë¡œ ëª¨ë“  CPU ì‹œê°„ì˜ ë¬´ë ¤ 25%ë¥¼ ì°¨ì§€í•˜ëŠ” í”„ë¡œí•„ì—ì„œ ë§¤ìš° ë¶„ëª…í–ˆìœ¼ë©°, ì´ëŠ” ë„¤íŠ¸ì›Œí¬ íŒŒì´í”„ê°€ ê±°ì˜ í¬í™”ë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ì ì„ ê°ì•ˆí•  ë•Œ ê°€ì ¸ì˜¤ê¸°ê°€ ë„¤íŠ¸ì›Œí¬ ì´ì „ì— CPUì— ì˜í•´ ë³‘ëª© í˜„ìƒì´ ë°œìƒí•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í–ˆìŠµë‹ˆë‹¤!</p>
<p><img src="https://andrewkchan.dev/posts/crawler-assets/ssl.png" alt=""></p>
<p><em>From <a href="https://letsencrypt.org/stats/">https://letsencrypt.org/stats/</a> - SSL loads in Firefox have gone from 30% in 2014 to >80% in 2025.</em>
<a href="https://letsencrypt.org/stats/">*https://letsencrypt.org/stats/</a> ë…„ë¶€í„° - Firefoxì˜ SSL ë¡œë“œëŠ” 2014ë…„ 30%ì—ì„œ 2025ë…„ >80%ë¡œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.*</p>
<h3 id="the-big-crawl-í°-í¬ë¡¤ë§">The big crawl í° í¬ë¡¤ë§</h3>
<p><img src="https://andrewkchan.dev/posts/crawler-assets/shard6-metrics1.png" alt=""></p>
<p><img src="https://andrewkchan.dev/posts/crawler-assets/shard6-metrics2.png" alt=""></p>
<p><em>Metrics for one of the nodes early on in the crawl. Some of the units in the grafana dashboard are wrong (e.g. error rate and parse queue size are using â€œbytesâ€ by mistake)</em>
<em>í¬ë¡¤ë§ ì´ˆê¸°ì— ë…¸ë“œ ì¤‘ í•˜ë‚˜ì— ëŒ€í•œ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤. grafana ëŒ€ì‹œë³´ë“œì˜ ì¼ë¶€ ë‹¨ìœ„ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤(ì˜ˆ: ì˜¤ë¥˜ìœ¨ ë° êµ¬ë¬¸ ë¶„ì„ ëŒ€ê¸°ì—´ í¬ê¸°ê°€ ì‹¤ìˆ˜ë¡œ â€œë°”ì´íŠ¸â€ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŒ).</em></p>
<p>Before running the big crawl with 12 <code>i7i.4xlarge</code> nodes, the biggest experiment Iâ€™d done had been a several-hour run on a single <code>i7i.2xlarge</code>, so there were quite a few surprises from the leap in scale that emerged over the course of the run, and I spent an entire Sunday from sunrise to sunset (and beyond) being oncall for my own run, watching metrics and hopping into to fix issues. Some of these were stupid operational oversights like forgetting to set up log rotation and then running out of space on the root volume, but the biggest issue was memory growth due to the frontiers.
12ê°œì˜ <code>i7i.4xlarge</code> ë…¸ë“œë¡œ ë¹… í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ê¸° ì „ì— ì œê°€ í•œ ê°€ì¥ í° ì‹¤í—˜ì€ ë‹¨ì¼ <code>i7i.2xlarge</code>ì—ì„œ ëª‡ ì‹œê°„ ë™ì•ˆ ì‹¤í–‰í•œ ê²ƒì´ì—ˆê¸° ë•Œë¬¸ì— ë‹¬ë¦¬ê¸° ê³¼ì •ì—ì„œ ë‚˜íƒ€ë‚œ ê·œëª¨ì˜ ë„ì•½, ê·¸ë¦¬ê³  ì¼ì¶œë¶€í„° ì¼ëª°ê¹Œì§€(ê·¸ë¦¬ê³  ê·¸ ì´í›„) ì¼ìš”ì¼ ë‚´ë‚´ ë‚´ ë‹¬ë¦¬ê¸°ë¥¼ ìœ„í•´ ëŒ€ê¸°í•˜ê³ , ì§€í‘œë¥¼ ë³´ê³ , ìˆ˜ì •í•˜ê¸° ìœ„í•´ ë›°ì–´ë“¤ì—ˆìŠµë‹ˆë‹¤ ë¬¸ì œ. ì´ë“¤ ì¤‘ ì¼ë¶€ëŠ” ë¡œê·¸ ìˆœí™˜ì„ ì„¤ì •í•˜ëŠ” ê²ƒì„ ìŠì–´ë²„ë¦° ë‹¤ìŒ ë£¨íŠ¸ ë³¼ë¥¨ì˜ ê³µê°„ì´ ë¶€ì¡±í•œ ê²ƒê³¼ ê°™ì€ ì–´ë¦¬ì„ì€ ìš´ì˜ ì‹¤ìˆ˜ì˜€ì§€ë§Œ ê°€ì¥ í° ë¬¸ì œëŠ” ë©”ëª¨ë¦¬ ì¦ê°€ì˜€ìŠµë‹ˆë‹¤. êµ­ê²½ìœ¼ë¡œ.</p>
<p>This was specific to my design, which placed all frontier data in-memory. Iâ€™d had memory issues on the earlier, smaller-scale crawls too, but on other components such as the HTTP client or the visited entries. Iâ€™d calculated out the memory headroom needed for 1 billion visited pages across those components, but failed to anticipate that the frontiers of certain very hot domains would grow to become tens of gigabytes (hundreds of millions or billions of URLs), and halfway through the run, my nodes started dropping like flies. I had to manually intervene by restarting machines which had become unresponsive and truncating frontiers. Luckily the fault tolerance made resumption after fixing easy.
ì´ê²ƒì€ ëª¨ë“  í”„ë¡ í‹°ì–´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë°°ì¹˜í•˜ëŠ” ë‚´ ì„¤ê³„ì—ë§Œ í•´ë‹¹ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì „ì˜ ì†Œê·œëª¨ í¬ë¡¤ë§ì—ì„œë„ ë©”ëª¨ë¦¬ ë¬¸ì œê°€ ìˆì—ˆì§€ë§Œ HTTP í´ë¼ì´ì–¸íŠ¸ ë˜ëŠ” ë°©ë¬¸í•œ í•­ëª©ê³¼ ê°™ì€ ë‹¤ë¥¸ êµ¬ì„± ìš”ì†Œì—ì„œë„ ë©”ëª¨ë¦¬ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ êµ¬ì„± ìš”ì†Œì—ì„œ 10ì–µ ê°œì˜ ë°©ë¬¸ í˜ì´ì§€ì— í•„ìš”í•œ ë©”ëª¨ë¦¬ ì—¬ìœ  ê³µê°„ì„ ê³„ì‚°í–ˆì§€ë§Œ íŠ¹ì • ë§¤ìš° ì¸ê¸° ìˆëŠ” ë„ë©”ì¸ì˜ í”„ë¡ í‹°ì–´ê°€ ìˆ˜ì‹­ ê¸°ê°€ë°”ì´íŠ¸(ìˆ˜ì–µ ë˜ëŠ” ìˆ˜ì‹­ì–µ ê°œì˜ URL)ë¡œ ì„±ì¥í•  ê²ƒì´ë¼ëŠ” ê²ƒì„ ì˜ˆìƒí•˜ì§€ ëª»í–ˆê³  ì‹¤í–‰ ì¤‘ê°„ì— ë‚´ ë…¸ë“œê°€ íŒŒë¦¬ì²˜ëŸ¼ ë–¨ì–´ì§€ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì‘ë‹µí•˜ì§€ ì•ŠëŠ” ê¸°ê³„ë¥¼ ë‹¤ì‹œ ì‹œì‘í•˜ê³  í”„ë¡ í‹°ì–´ë¥¼ ì˜ë¼ ìˆ˜ë™ìœ¼ë¡œ ê°œì…í•´ì•¼ í–ˆìŠµë‹ˆë‹¤. ìš´ ì¢‹ê²Œë„ ë‚´ê²°í•¨ì„± ë•ë¶„ì— ìˆ˜ì • í›„ ì¬ê°œê°€ ì‰¬ì›Œì¡ŒìŠµë‹ˆë‹¤.</p>
<p>Were the problematic domains tarpits? From what I could tell, most were just really popular websites with lots of links. For example, <code>yahoo.com</code> and <code>wikipedia.org</code> were among them. Another was a â€œcosplayfuâ€ website which looked like a strange shopping site on first glance, but after searching on the internet seemed legit. In any case, the most problematic domains were simply added to my manual exclusion list.
ë¬¸ì œê°€ ìˆëŠ” ë„ë©”ì¸ì´ ë°©ì¹¨ì´ì—ˆìŠµë‹ˆê¹Œ? ë‚´ê°€ ì•Œ ìˆ˜ ìˆëŠ” ë°”ë¡œëŠ” ëŒ€ë¶€ë¶„ ë§í¬ê°€ ë§ì€ ì •ë§ ì¸ê¸° ìˆëŠ” ì›¹ì‚¬ì´íŠ¸ì˜€ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ <code>yahoo.com</code> ë° <code>wikipedia.org</code> ê·¸ë“¤ ì¤‘ í•˜ë‚˜ì˜€ìŠµë‹ˆë‹¤. ë˜ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì–¸ëœ» ë³´ê¸°ì—ëŠ” ì´ìƒí•œ ì‡¼í•‘ ì‚¬ì´íŠ¸ì²˜ëŸ¼ ë³´ì˜€ì§€ë§Œ ì¸í„°ë„·ì—ì„œ ê²€ìƒ‰í•´ ë³´ë©´ í•©ë²•ì ì¸ ê²ƒì²˜ëŸ¼ ë³´ì˜€ë˜ â€œì½”ìŠ¤í”„ë ˆí‘¸â€ ì›¹ì‚¬ì´íŠ¸ì˜€ìŠµë‹ˆë‹¤. ì–´ì¨Œë“  ê°€ì¥ ë¬¸ì œê°€ ë˜ëŠ” ë„ë©”ì¸ì€ ë‹¨ìˆœíˆ ìˆ˜ë™ ì œì™¸ ëª©ë¡ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<h2 id="discussion-í† ë¡ ">Discussion í† ë¡ </h2>
<h3 id="theory-vs-practice-ì´ë¡ -ëŒ€-ì‹¤ìŠµ">Theory vs. Practice ì´ë¡  ëŒ€ ì‹¤ìŠµ</h3>
<p>How does my crawler contrast against textbook solutions like the one in Evan Kingâ€™s <a href="https://www.hellointerview.com/learn/system-design/problem-breakdowns/web-crawler">HelloInterview analysis</a>? The metric of interest here is probably Kingâ€™s â€œhand-wavyâ€ estimate that 5 machines can crawl 10 billion pages in 5 days. In this claim, the machines are completely dedicated to fetching, with the parsers and frontier datastore living elsewhere. There are no details on the hardware of each machine besides assuming a 400Gbps bandwidth per-machine from which we achieve 30% utilization.
ë‚´ í¬ë¡¤ëŸ¬ëŠ” Evan Kingì˜ <a href="https://www.hellointerview.com/learn/system-design/problem-breakdowns/web-crawler">HelloInterview ë¶„ì„</a>ê³¼ ê°™ì€ êµê³¼ì„œ ì†”ë£¨ì…˜ê³¼ ì–´ë–»ê²Œ ëŒ€ì¡°ë©ë‹ˆê¹Œ? ì—¬ê¸°ì„œ ê´€ì‹¬ ìˆëŠ” ì§€í‘œëŠ” ì•„ë§ˆë„ 5ëŒ€ì˜ ê¸°ê³„ê°€ 5ì¼ ë™ì•ˆ 100ì–µ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•  ìˆ˜ ìˆë‹¤ëŠ” Kingì˜ â€œì† ëª¨ì–‘â€ ì¶”ì •ì¼ ê²ƒì…ë‹ˆë‹¤. ì´ ì£¼ì¥ì—ì„œ ê¸°ê³„ëŠ” ê°€ì ¸ì˜¤ê¸°ì— ì „ì ìœ¼ë¡œ ì „ë…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê³³ì— ìˆëŠ” íŒŒì„œì™€ í”„ë¡ í‹°ì–´ ë°ì´í„° ì €ì¥ì†Œ. 30%ì˜ í™œìš©ë„ë¥¼ ë‹¬ì„±í•˜ëŠ” ê¸°ê³„ë‹¹ 400Gbps ëŒ€ì—­í­ì„ ê°€ì •í•˜ëŠ” ê²ƒ ì™¸ì—ëŠ” ê° ê¸°ê³„ì˜ í•˜ë“œì›¨ì–´ì— ëŒ€í•œ ì„¸ë¶€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.</p>
<p>The utilization at least is about right; my nodes offered only 25Gbps, but I indeed got about 32% utilization with 8Gbps in + out at steady state. That said, I only dedicated 9/16 cores on each machine to fetching, which using naive scaling suggests I couldâ€™ve achieved 53% network utilization. Similarly, since I used 12 machines to crawl 1 billion pages in ~1 day, I likely could have achieved the same 1 billion per day throughput with 6.75 fetcher-only machines. If we assume straightforward scaling from <code>i7i.4xlarge</code> to <code>i7i.8xlarge</code> as well, this implies 6.75 double-size fetcher-only machines could crawl 10 billion pages in 5 days. So Kingâ€™s number is not too far off, but might require a bit more optimization than I did with my system!
í™œìš©ë„ëŠ” ì ì–´ë„ ì ë‹¹í•©ë‹ˆë‹¤. ë‚´ ë…¸ë“œëŠ” 25Gbpsë§Œ ì œê³µí–ˆì§€ë§Œ ì‹¤ì œë¡œ ì •ìƒ ìƒíƒœì—ì„œ 8Gbps ì…ë ¥ + ì¶œë ¥ìœ¼ë¡œ ì•½ 32%ì˜ í™œìš©ë„ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ì¦‰, ê° ì‹œìŠ¤í…œì—ì„œ 9/16 ì½”ì–´ë§Œ ê°€ì ¸ì˜¤ê¸° ì „ìš©ìœ¼ë¡œ ì‚¬ìš©í–ˆëŠ”ë°, ìˆœì§„í•œ ìŠ¤ì¼€ì¼ë§ì„ ì‚¬ìš©í•˜ë©´ 53%ì˜ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ë¥ ì„ ë‹¬ì„±í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ 12ëŒ€ì˜ ì»´í“¨í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ~1ì¼ ë™ì•ˆ 10ì–µ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í–ˆê¸° ë•Œë¬¸ì— 6.75ê°œì˜ í˜ì²˜ ì „ìš© ì»´í“¨í„°ë¡œ ë™ì¼í•œ í•˜ë£¨ 10ì–µ ì²˜ë¦¬ëŸ‰ì„ ë‹¬ì„±í•  ìˆ˜ ìˆì—ˆì„ ê²ƒì…ë‹ˆë‹¤. <code>i7i.4xlarge</code>ì—ì„œ <code>i7i.8xlarge</code>ë¡œ ê°„ë‹¨í•˜ê²Œ í™•ì¥í•œë‹¤ê³  ê°€ì •í•˜ë©´ ì´ëŠ” 6.75 ë”ë¸” ì‚¬ì´ì¦ˆë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. Fetcher ì „ìš© ë¨¸ì‹ ì€ 5ì¼ ë§Œì— 100ì–µ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ Kingì˜ ìˆ«ìëŠ” ê·¸ë¦¬ ë©€ì§€ ì•Šì§€ë§Œ ë‚´ ì‹œìŠ¤í…œë³´ë‹¤ ì¡°ê¸ˆ ë” ìµœì í™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!</p>
<h3 id="what-now-ì´ë²ˆì—”-ë˜-ë­ì˜ˆìš”">What now? ì´ë²ˆì—” ë˜ ë­ì˜ˆìš”?</h3>
<p>To be honest Iâ€™m surprised so much of the web is still accessible6 without running JS. Itâ€™s great! I found out about some cool websites like <a href="http://ancientfaces.com/">ancientfaces.com</a> through this crawl. But I noticed that even for many crawl-able websites like GitHub, the downloaded pages didnâ€™t really have meaningfully marked-up text content; it was all embedded in gigantic strings which presumably were to be rendered client-side by what we might consider â€œlightweightâ€ JS scripts. I think interesting future work would involve addressing this elephant: how does large-scale crawling look like when we actually need to render pages dynamically? I suspect the same scale will be much more expensive.
ì†”ì§íˆ ë§í•´ì„œ JSë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šê³ ë„ì›¹ì˜ ë§ì€ ë¶€ë¶„ì— ì—¬ì „íˆ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì— ë†€ëìŠµë‹ˆë‹¤. ëŒ€ë‹¨í•´ìš”! ë‚˜ëŠ” <a href="http://ancientfaces.com/">ancientfaces.com</a> ê°™ì€ ë©‹ì§„ ì›¹ì‚¬ì´íŠ¸ì— ëŒ€í•´ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í¬ë¡¤ë§. ê·¸ëŸ¬ë‚˜ GitHubì™€ ê°™ì€ í¬ë¡¤ë§ ê°€ëŠ¥í•œ ë§ì€ ì›¹ ì‚¬ì´íŠ¸ì˜ ê²½ìš°ì—ë„ ë‹¤ìš´ë¡œë“œí•œ í˜ì´ì§€ì—ëŠ” ì‹¤ì œë¡œ ì˜ë¯¸ ìˆê²Œ í‘œì‹œëœ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ìŠµë‹ˆë‹¤. ê·¸ê²ƒì€ ëª¨ë‘ ê±°ëŒ€í•˜ê²Œ ë‚´ì¥ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤. ë¬¸ìì—´ì€ ì•„ë§ˆë„ ìš°ë¦¬ê°€ â€œê²½ëŸ‰â€ JS ìŠ¤í¬ë¦½íŠ¸ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆëŠ” ê²ƒì— ì˜í•´ í´ë¼ì´ì–¸íŠ¸ ì¸¡ì—ì„œ ë Œë”ë§ë  ê²ƒì…ë‹ˆë‹¤. í¥ë¯¸ë¡œìš´ ë¯¸ë˜ ì‘ì—…ì—ëŠ” ì´ ì½”ë¼ë¦¬ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì´ í¬í•¨ë  ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ í˜ì´ì§€ë¥¼ ë™ì ìœ¼ë¡œ ë Œë”ë§í•´ì•¼ í•  ë•Œ ëŒ€ê·œëª¨ í¬ë¡¤ë§ì€ ì–´ë–»ê²Œ ë³´ì¼ê¹Œìš”? ê°™ì€ ê·œëª¨ê°€ í›¨ì”¬ ë” ë¹„ìŒ€ ê²ƒ ê°™ì•„ìš”.</p>
<p>Another question is: what do the shape and distribution of the billion pages I crawled look like? I kept a sample but havenâ€™t gotten the time to run any analytics. It will be interesting to know some basic facts about metadata, such as how many crawled URLs were actually alive vs. dead, how many were of an HTML content type vs. multimedia, etc.
ë˜ ë‹¤ë¥¸ ì§ˆë¬¸ì€ ë‚´ê°€ í¬ë¡¤ë§í•œ 10ì–µ í˜ì´ì§€ì˜ ëª¨ì–‘ê³¼ ë¶„í¬ëŠ” ì–´ë–»ê²Œ ìƒê²¼ëŠ”ê°€ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ìƒ˜í”Œì„ ë³´ê´€í–ˆì§€ë§Œ ë¶„ì„ì„ ì‹¤í–‰í•  ì‹œê°„ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ ì‚´ì•„ ìˆëŠ” URLê³¼ ì£½ì€ URLì˜ ìˆ˜, HTML ì½˜í…ì¸  ìœ í˜• ë° ë©€í‹°ë¯¸ë””ì–´ ë“± ë©”íƒ€ë°ì´í„°ì— ëŒ€í•œ ëª‡ ê°€ì§€ ê¸°ë³¸ ì‚¬ì‹¤ì„ ì•„ëŠ” ê²ƒì€ í¥ë¯¸ë¡œìš¸ ê²ƒì…ë‹ˆë‹¤.</p>
<p>Finally, this post covered some of the larger ways that the web has changed over the last decade, but the landscape is shifting yet again. Aggressive crawling/scraping backed by massive resources isnâ€™t new (Facebook previously ran into hot water for <a href="https://news.ycombinator.com/item?id=23490367">OpenGraph scraping</a>), but has been intensified with AI. I took politeness very seriously, following conventions like robots.txt and more, but many crawlers donâ€™t and the internet is starting to develop defenses. Cloudflareâ€™s experimental <a href="https://blog.cloudflare.com/introducing-pay-per-crawl/">pay-per-crawl</a> feature is a new offering from the market that could help a lot.
ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ ê²Œì‹œë¬¼ì—ì„œëŠ” ì§€ë‚œ 10ë…„ ë™ì•ˆ ì›¹ì´ ë³€í™”í–ˆì§€ë§Œ í™˜ê²½ì´ ë‹¤ì‹œ ë³€í™”í•˜ê³  ìˆëŠ” ë” í° ë°©ë²• ì¤‘ ì¼ë¶€ë¥¼ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ë°©ëŒ€í•œ ë¦¬ì†ŒìŠ¤ë¡œ ë’·ë°›ì¹¨ë˜ëŠ” ê³µê²©ì ì¸ í¬ë¡¤ë§/ìŠ¤í¬ë˜í•‘ì€ ìƒˆë¡œìš´ ê²ƒì´ ì•„ë‹ˆì§€ë§Œ(Facebookì€ ì´ì „ì— <a href="https://news.ycombinator.com/item?id=23490367">OpenGraph ìŠ¤í¬ë˜í•‘</a>ìœ¼ë¡œ ì¸í•´ ëœ¨ê±°ìš´ ë¬¼ì— ë¹ ì¡ŒìŠµë‹ˆë‹¤) AIë¡œ ì¸í•´ ë”ìš± ê°•í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚˜ëŠ” ë§¤ìš° ê³µì†í•˜ê²Œ ë°›ì•„ë“¤ì˜€ìŠµë‹ˆë‹¤ ì§„ì§€í•˜ê²Œ, robots.txt ë“±ê³¼ ê°™ì€ ê·œì¹™ì„ ë”°ë¥´ì§€ë§Œ ë§ì€ í¬ë¡¤ëŸ¬ëŠ” ê·¸ë ‡ì§€ ì•Šìœ¼ë©° ì¸í„°ë„·ì€ ë°©ì–´ ì²´ê³„ë¥¼ ê°œë°œí•˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤. Cloudflareì˜ ì‹¤í—˜ì  <a href="https://blog.cloudflare.com/introducing-pay-per-crawl/">í¬ë¡¤ë§ë‹¹ ì§€ë¶ˆ</a> ê¸°ëŠ¥ì€ ë§ì€ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ì‹œì¥ì˜ ìƒˆë¡œìš´ ì œí’ˆì…ë‹ˆë‹¤.</p> </article> </div> <script type="module">
      // ëª©ì : index.jsonì—ì„œ í˜„ì¬ ê¸€ ë©”íƒ€/ì¸ë„¤ì¼ì„ ì°¾ì•„ ìƒì„¸ í™”ë©´ì— ë°˜ì˜í•œë‹¤.
      async function hydrateMeta() {
        try {
          const BASE = import.meta.env.BASE_URL;
          const slug = decodeURIComponent(location.pathname.replace(/.*\/post\//,'').replace(/\/?$/,''));
          const res = await fetch(`${BASE}index.json`);
          const data = await res.json();
          const items = (data && data.items) || [];
          const item = items.find((i) => i.slug === slug);
          if (!item) return;

          const hero = document.getElementById('hero');
          const heroImg = document.getElementById('heroImg');
          const source = document.getElementById('source');
          const cta = document.getElementById('ctaSource');
          if (item.thumbnail && hero && heroImg) {
            heroImg.setAttribute('src', item.thumbnail);
            hero.style.display = 'block';
          }
          if (item.source_url && source && cta) {
            source.setAttribute('href', item.source_url);
            cta.setAttribute('href', item.source_url);
            source.style.display='inline-block';
            cta.style.display='inline-block';
          }
        } catch {}
      }
      hydrateMeta();

      // ë³µì‚¬ ë²„íŠ¼ ì œê±°ë¨ â€” ìƒë‹¨ì— ì›ë¬¸ ë³´ê¸° ë²„íŠ¼ë§Œ ìœ ì§€
    </script> </body> </html>