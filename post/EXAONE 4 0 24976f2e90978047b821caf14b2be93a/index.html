<!DOCTYPE html><html lang="ko"> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>상세</title><link rel="icon" href="/favicon.svg" type="image/svg+xml"><link rel="icon" href="/favicon-32x32.png" sizes="32x32"><link rel="apple-touch-icon" href="/apple-touch-icon.png" sizes="180x180"></head> <body class="container" style="padding:24px;max-width:900px"> <a href="/" aria-label="홈으로">← 홈</a> <h1 style="margin:8px 0 4px"></h1>  <article style="margin-top:16px"> <h1 id="exaone-40">EXAONE 4.0</h1>
<p>발견일: 2025/07/16
분류: 오픈소스
즐겨찾기: No</p>
<h1 id="exaone-40-모델-특징">EXAONE 4.0 모델 특징</h1>
<p><strong>EXAONE 4.0</strong>은 LG AI Research에서 개발한 오픈소스 대형 언어 모델(LLM)로, <strong>EXpert AI for EveryONE</strong>의 약자로 모든 이를 위한 전문 AI를 목표로 합니다. <a href="https://huggingface.co/collections/LGAI-EXAONE/exaone-40-686b2e0069800c835ed48375">Hugging Face 컬렉션</a>에서 2025년 7월 15일 공개되었으며, 영어, 한국어, 스페인어를 지원하는 다국어 모델입니다.</p>
<h2 id="주요-특징">주요 특징</h2>
<h3 id="1-모델-구성">1. 모델 구성</h3>
<ul>
<li><strong>두 가지 크기</strong>:
<ul>
<li><strong>32B(320억 파라미터)</strong>: 복잡한 추론 및 대규모 데이터 처리에 최적.</li>
<li><strong>1.2B(12억 파라미터)</strong>: 온디바이스용 경량 모델, 자원 제약 환경에 적합.</li>
</ul>
</li>
<li><strong>긴 컨텍스트</strong>: 최대 <strong>131,072 토큰</strong> 지원, 긴 문서 및 다중 턴 대화 가능.</li>
</ul>
<h3 id="2-하이브리드-추론">2. 하이브리드 추론</h3>
<ul>
<li><strong>비추론 모드</strong>: 빠른 응답, 일반 대화 및 간단한 작업에 적합.</li>
<li><strong>추론 모드</strong>: <code>\\n</code> 태그로 사고 과정 명시, 수학/코딩/문제 해결에 특화.</li>
<li><strong>하이브리드 어텐션</strong>: 로컬(슬라이딩 윈도우) 및 글로벌 어텐션(3:1) 결합.</li>
</ul>
<h3 id="3-다국어-지원">3. 다국어 지원</h3>
<ul>
<li>영어, 한국어, 스페인어 처리. 예: “Explain how wonderful you are” (영어), “너가 얼마나 대단한지 설명해 봐” (한국어).</li>
<li>다국어 대화, 문서 요약 가능.</li>
</ul>
<h3 id="4-에이전트-기능">4. 에이전트 기능</h3>
<ul>
<li>API 호출, 데이터 분석, 코드 생성 등 수행.</li>
<li>활용: 코딩, 데이터 시각화, 워크플로우 자동화.</li>
</ul>
<h3 id="5-성능">5. 성능</h3>
<ul>
<li><strong>벤치마크</strong>:
<ul>
<li><strong>MATH-500</strong>: 95.7% (EXAONE Deep 32B).</li>
<li><strong>Live Code Bench</strong>: 59.5% 성공률, OpenAI o1-mini 및 DeepSeek R1 상회.</li>
<li><strong>AIME</strong>: 고급 수학 문제 풀이 강력.</li>
</ul>
</li>
<li>긴 컨텍스트(32K~131K 토큰)로 복잡한 작업 우수.</li>
</ul>
<h3 id="6-기술적-특징">6. 기술적 특징</h3>
<ul>
<li><strong>아키텍처</strong>: 그룹 쿼리 어텐션(40 Q-heads, 8 KV-heads).</li>
<li><strong>사전 학습</strong>: 8T 토큰으로 일반화 성능 강화.</li>
<li><strong>최적화</strong>: AWQ, GGUF 양자화로 메모리 효율성 향상.</li>
</ul>
<h2 id="접근-방법">접근 방법</h2>
<ul>
<li>
<p><strong>Hugging Face</strong>: <a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B">EXAONE-4.0-32B</a>, <a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B">EXAONE-4.0-1.2B</a>.</p>
</li>
<li>
<p><strong>설치</strong>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> git+https://github.com/lgai-exaone/transformers@add-exaone4</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
</li>
<li>
<p><strong>예제 코드</strong>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> transformers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"><span style="color:#E1E4E8">model_name </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> "LGAI-EXAONE/EXAONE-4.0-32B"</span></span>
<span class="line"><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoModelForCausalLM.from_pretrained(model_name, </span><span style="color:#FFAB70">torch_dtype</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"bfloat16"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">device_map</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">tokenizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span class="line"><span style="color:#E1E4E8">prompt </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> "너가 얼마나 대단한지 설명해 봐"</span></span>
<span class="line"><span style="color:#E1E4E8">messages </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: prompt}]</span></span>
<span class="line"><span style="color:#E1E4E8">input_ids </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer.apply_chat_template(messages, </span><span style="color:#FFAB70">tokenize</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">add_generation_prompt</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">return_tensors</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"pt"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generate(input_ids.to(model.device), </span><span style="color:#FFAB70">max_new_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">128</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">do_sample</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(tokenizer.decode(output[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]))</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
</li>
<li>
<p><strong>Ollama</strong>: GGUF 형식으로 실행 가능.</p>
</li>
<li>
<p><strong>로컬 실행</strong>: llama.cpp, TensorRT-LLM 지원(vLLM/SGLang 미지원).</p>
</li>
</ul>
<h2 id="활용-사례">활용 사례</h2>
<ul>
<li><strong>코딩</strong>: 알고리즘, 디버깅, GitHub 이슈 해결.</li>
<li><strong>수학</strong>: AIME, MATH-500 문제 풀이.</li>
<li><strong>데이터 분석</strong>: 데이터셋 처리, 시각화.</li>
<li><strong>다국어</strong>: 영어/한국어/스페인어 챗봇, 문서 요약.</li>
</ul>
<h2 id="장점">장점</h2>
<ul>
<li>오픈소스: 연구용 무료, 상업용은 <a href="mailto:contact_us@lgresearch.ai">contact_us@lgresearch.ai</a> 문의.</li>
<li>동급 모델(OpenAI o1-mini) 대비 경쟁력.</li>
<li>온디바이스(1.2B) 및 고성능(32B) 지원.</li>
<li>131K 토큰으로 장문 처리 강력.</li>
</ul>
<h2 id="한계">한계</h2>
<ul>
<li>텍스트 중심, 이미지/오디오 미지원.</li>
<li>vLLM, SGLang 미지원(2025년 7월 기준).</li>
<li>32B 모델은 고성능 GPU 필요.</li>
<li>학습 데이터 기반 잠재적 편향 가능.</li>
</ul>
<h2 id="시작하기">시작하기</h2>
<ul>
<li><strong>다운로드</strong>: <a href="https://huggingface.co/collections/LGAI-EXAONE/exaone-40-686b2e0069800c835ed48375">Hugging Face EXAONE 4.0</a>.</li>
<li><strong>문서</strong>: <a href="https://github.com/LG-AI-EXAONE/EXAONE-4.0">LG AI Research GitHub</a>, <a href="https://arxiv.org/">arXiv</a>.</li>
<li><strong>문의</strong>: <a href="mailto:contact_us@lgresearch.ai">contact_us@lgresearch.ai</a>.</li>
</ul>
<p><strong>EXAONE 4.0</strong>은 코딩, 수학, 다국어 작업에서 강력한 오픈소스 LLM으로, AI 발전에 기여합니다. <a href="https://huggingface.co/LGAI-EXAONE">Hugging Face</a>에서 확인하세요!</p> </article> </body></html>