# ì˜ìƒì„ 4Dë¡œ EX-4D ì˜¤í”ˆì†ŒìŠ¤ ê³µê°œ

ë°œê²¬ì¼: 2025/07/08
ì›ë¬¸ URL: https://github.com/tau-yihouxiang/EX-4D
ë¶„ë¥˜: ì˜¤í”ˆì†ŒìŠ¤
ì›ë¬¸ Source: ğŸ”—github
ì¦ê²¨ì°¾ê¸°: No

[](https://opengraph.githubassets.com/2f3fe93325ec8e6d2c7151c068a08c6b7ac6b334b4b5e5dce97b699b839e68ca/tau-yihouxiang/EX-4D)

# EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh
EX-4D: ì‹¬ë„ ë°©ìˆ˜ ë©”ì‹œë¥¼ í†µí•œ EXtreme Viewpoint 4D ë¹„ë””ì˜¤ í•©ì„±

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/docs/Logo.png)

[ğŸ“„ Paper](https://arxiv.org/abs/2506.05554) | [ğŸ¥ Homepage](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html) | [ğŸ’» Code](https://github.com/tau-yihouxiang/EX-4D)
[ğŸ“„ ì¢…ì´](https://arxiv.org/abs/2506.05554) | [ğŸ¥ í™ˆí˜ì´ì§€](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html) | [ğŸ’» ì½”ë“œ](https://github.com/tau-yihouxiang/EX-4D)

## ğŸŒŸ Highlights ğŸŒŸ í•˜ì´ë¼ì´íŠ¸

- **ğŸ¯ Extreme Viewpoint Synthesis**: Generate high-quality 4D videos with camera movements ranging from -90Â° to 90Â°
**ğŸ¯ ê·¹í•œì˜ ì‹œì  í•©ì„±**: -90Â°ì—ì„œ 90Â° ë²”ìœ„ì˜ ì¹´ë©”ë¼ ì›€ì§ì„ìœ¼ë¡œ ê³ í’ˆì§ˆ 4D ë¹„ë””ì˜¤ ìƒì„±
- **ğŸ”§ Depth Watertight Mesh**: Novel geometric representation that models both visible and occluded regions
**ğŸ”§ Depth Watertight Mesh**: ê°€ì‹œ ì˜ì—­ê³¼ ê°€ë ¤ì§„ ì˜ì—­ì„ ëª¨ë‘ ëª¨ë¸ë§í•˜ëŠ” ìƒˆë¡œìš´ ê¸°í•˜í•™ì  í‘œí˜„
- **âš¡ Lightweight Architecture**: Only 1% trainable parameters (140M) of the 14B video diffusion backbone
**âš¡ ê²½ëŸ‰ ì•„í‚¤í…ì²˜**: 14B ë¹„ë””ì˜¤ í™•ì‚° ë°±ë³¸ì˜ 1% í•™ìŠµ ê°€ëŠ¥ ë§¤ê°œë³€ìˆ˜(140M)ë§Œ
- **ğŸ­ No Multi-view Training**: Innovative masking strategy eliminates the need for expensive multi-view datasets
**ğŸ­ ë©€í‹°ë·° êµìœ¡ ì—†ìŒ**: í˜ì‹ ì ì¸ ë§ˆìŠ¤í‚¹ ì „ëµìœ¼ë¡œ ê°’ë¹„ì‹¼ ë©€í‹°ë·° ë°ì´í„° ì„¸íŠ¸ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- **ğŸ† State-of-the-art Performance**: Outperforms existing methods, especially on extreme camera angles
**ğŸ† ìµœì²¨ë‹¨ ì„±ëŠ¥**: íŠ¹íˆ ê·¹í•œì˜ ì¹´ë©”ë¼ ê°ë„ì—ì„œ ê¸°ì¡´ ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.

## ğŸ¬ Demo Results ğŸ¬ ë°ëª¨ ê²°ê³¼

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/docs/teaser.png)

*EX-4D transforms monocular videos into camera-controllable 4D experiences with physically consistent results under extreme viewpoints.*
*EX-4DëŠ” ë‹¨ì•ˆ ë¹„ë””ì˜¤ë¥¼ ì¹´ë©”ë¼ë¡œ ì œì–´í•  ìˆ˜ ìˆëŠ” 4D ê²½í—˜ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê·¹í•œì˜ ì‹œì ì—ì„œ ë¬¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ëœ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.*

## ğŸ—ï¸ Framework Overview ğŸ—ï¸ í”„ë ˆì„ì›Œí¬ ê°œìš”

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/docs/overview.png)

Our framework consists of three key components:
ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

1. **ğŸ”º Depth Watertight Mesh Construction**: Creates a robust geometric prior that explicitly models both visible and occluded regions
**ğŸ”º Depth Watertight Mesh Construction**: ê°€ì‹œ ì˜ì—­ê³¼ ê°€ë ¤ì§„ ì˜ì—­ì„ ëª¨ë‘ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ê°•ë ¥í•œ ê¸°í•˜í•™ì  ì‚¬ì „ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤
2. **ğŸ­ Simulated Masking Strategy**: Generates effective training data from monocular videos without multi-view datasets
**ğŸ­ ì‹œë®¬ë ˆì´ì…˜ëœ ë§ˆìŠ¤í‚¹ ì „ëµ**: ë‹¤ì¤‘ ë·° ë°ì´í„° ì„¸íŠ¸ ì—†ì´ ë‹¨ì•ˆ ë¹„ë””ì˜¤ì—ì„œ íš¨ê³¼ì ì¸ í›ˆë ¨ ë°ì´í„° ìƒì„±
3. **âš™ï¸ Lightweight LoRA Adapter**: Efficiently integrates geometric information with pre-trained video diffusion models
**âš™ï¸ ê²½ëŸ‰ LoRA ì–´ëŒ‘í„°**: ê¸°í•˜í•™ì  ì •ë³´ë¥¼ ì‚¬ì „ í›ˆë ¨ëœ ë¹„ë””ì˜¤ í™•ì‚° ëª¨ë¸ê³¼ íš¨ìœ¨ì ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.

## ğŸš€ Quick Start ğŸš€ ë¹ ë¥¸ ì‹œì‘

### Installation ì„¤ì¹˜

```
# Clone the repository
git clone https://github.com/tau-yihouxiang/EX-4D.git
cd EX-4D
# Create conda environment
conda create -n ex4d python=3.10
conda activate ex4d
# Install PyTorch (2.x recommended)
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124
# Install Nvdiffrast
pip install git+https://github.com/NVlabs/nvdiffrast.git
# Install dependencies and diffsynth
pip install -e .
# Install depthcrafter for depth estimation. (Follow DepthCrafter's installing instruction for checkpoints preparation.)
git clone https://github.com/Tencent/DepthCrafter.git
```

### Download Pretrained Model

ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI
huggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D
```

### Example Usage ì‚¬ìš© ì˜ˆ

### 1. DW-Mesh Reconstruction

1. DW-ë©”ì‰¬ ì¬êµ¬ì„±

```
# --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )
python recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh
```

### 2. EX-4D Generation (48GB VRAM required)

2. EX-4D ì„¸ëŒ€(48GB VRAM í•„ìš”)

```
python generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4
```

**Input Video**

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/examples/flower/input.gif)

âœ

**Output Video**

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/examples/flower/output.gif)

### User Study Results

- **70.7%** of participants preferred EX-4D over baseline methods
- Superior performance in physical consistency and extreme viewpoint quality
- Significant improvement as camera angles become more extreme

## ğŸ¯ Applications

- **ğŸ® Gaming**: Create immersive 3D game cinematics from 2D footage
- **ğŸ¬ Film Production**: Generate novel camera angles for post-production
- **ğŸ¥½ VR/AR**: Create free-viewpoint video experiences
- **ğŸ“± Social Media**: Generate dynamic camera movements for content creation
- **ğŸ¢ Architecture**: Visualize spaces from multiple viewpoints

## âš ï¸ Limitations

- **Depth Dependency**: Performance relies on monocular depth estimation quality
- **Computational Cost**: Requires significant computation for high-resolution videos
- **Reflective Surfaces**: Challenges with reflective or transparent materials

## ğŸ”® Future Work

- Real-time inference optimization (3DGS / 4DGS)
- Support for higher resolutions (1K, 2K)
- Neural mesh refinement techniques

## ğŸ™ Acknowledgments

We would like to thank the [DiffSynth-Studio v1.1.1](https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1) project for providing the foundational diffusion framework.

## ğŸ“š Citation

If you find our work useful, please consider citing:

```
@misc{hu2025ex4dextremeviewpoint4d,
      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, 
      author={Tao Hu and Haoyang Peng and Xiao Liu and Yuewen Ma},
      year={2025},
      eprint={2506.05554},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05554}, 
}
```