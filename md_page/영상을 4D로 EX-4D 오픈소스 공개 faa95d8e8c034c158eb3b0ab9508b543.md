# 영상을 4D로 EX-4D 오픈소스 공개

발견일: 2025/07/08
원문 URL: https://github.com/tau-yihouxiang/EX-4D
분류: 오픈소스
원문 Source: 🔗github
즐겨찾기: No

[](https://opengraph.githubassets.com/2f3fe93325ec8e6d2c7151c068a08c6b7ac6b334b4b5e5dce97b699b839e68ca/tau-yihouxiang/EX-4D)

# EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh
EX-4D: 심도 방수 메시를 통한 EXtreme Viewpoint 4D 비디오 합성

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/docs/Logo.png)

[📄 Paper](https://arxiv.org/abs/2506.05554) | [🎥 Homepage](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html) | [💻 Code](https://github.com/tau-yihouxiang/EX-4D)
[📄 종이](https://arxiv.org/abs/2506.05554) | [🎥 홈페이지](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html) | [💻 코드](https://github.com/tau-yihouxiang/EX-4D)

## 🌟 Highlights 🌟 하이라이트

- **🎯 Extreme Viewpoint Synthesis**: Generate high-quality 4D videos with camera movements ranging from -90° to 90°
**🎯 극한의 시점 합성**: -90°에서 90° 범위의 카메라 움직임으로 고품질 4D 비디오 생성
- **🔧 Depth Watertight Mesh**: Novel geometric representation that models both visible and occluded regions
**🔧 Depth Watertight Mesh**: 가시 영역과 가려진 영역을 모두 모델링하는 새로운 기하학적 표현
- **⚡ Lightweight Architecture**: Only 1% trainable parameters (140M) of the 14B video diffusion backbone
**⚡ 경량 아키텍처**: 14B 비디오 확산 백본의 1% 학습 가능 매개변수(140M)만
- **🎭 No Multi-view Training**: Innovative masking strategy eliminates the need for expensive multi-view datasets
**🎭 멀티뷰 교육 없음**: 혁신적인 마스킹 전략으로 값비싼 멀티뷰 데이터 세트가 필요하지 않습니다.
- **🏆 State-of-the-art Performance**: Outperforms existing methods, especially on extreme camera angles
**🏆 최첨단 성능**: 특히 극한의 카메라 각도에서 기존 방법을 능가합니다.

## 🎬 Demo Results 🎬 데모 결과

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/docs/teaser.png)

*EX-4D transforms monocular videos into camera-controllable 4D experiences with physically consistent results under extreme viewpoints.*
*EX-4D는 단안 비디오를 카메라로 제어할 수 있는 4D 경험으로 변환하여 극한의 시점에서 물리적으로 일관된 결과를 제공합니다.*

## 🏗️ Framework Overview 🏗️ 프레임워크 개요

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/docs/overview.png)

Our framework consists of three key components:
우리의 프레임워크는 세 가지 주요 구성 요소로 구성됩니다.

1. **🔺 Depth Watertight Mesh Construction**: Creates a robust geometric prior that explicitly models both visible and occluded regions
**🔺 Depth Watertight Mesh Construction**: 가시 영역과 가려진 영역을 모두 명시적으로 모델링하는 강력한 기하학적 사전 설정을 생성합니다
2. **🎭 Simulated Masking Strategy**: Generates effective training data from monocular videos without multi-view datasets
**🎭 시뮬레이션된 마스킹 전략**: 다중 뷰 데이터 세트 없이 단안 비디오에서 효과적인 훈련 데이터 생성
3. **⚙️ Lightweight LoRA Adapter**: Efficiently integrates geometric information with pre-trained video diffusion models
**⚙️ 경량 LoRA 어댑터**: 기하학적 정보를 사전 훈련된 비디오 확산 모델과 효율적으로 통합합니다.

## 🚀 Quick Start 🚀 빠른 시작

### Installation 설치

```
# Clone the repository
git clone https://github.com/tau-yihouxiang/EX-4D.git
cd EX-4D
# Create conda environment
conda create -n ex4d python=3.10
conda activate ex4d
# Install PyTorch (2.x recommended)
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124
# Install Nvdiffrast
pip install git+https://github.com/NVlabs/nvdiffrast.git
# Install dependencies and diffsynth
pip install -e .
# Install depthcrafter for depth estimation. (Follow DepthCrafter's installing instruction for checkpoints preparation.)
git clone https://github.com/Tencent/DepthCrafter.git
```

### Download Pretrained Model

사전 학습된 모델 다운로드

```
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI
huggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D
```

### Example Usage 사용 예

### 1. DW-Mesh Reconstruction

1. DW-메쉬 재구성

```
# --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )
python recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh
```

### 2. EX-4D Generation (48GB VRAM required)

2. EX-4D 세대(48GB VRAM 필요)

```
python generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4
```

**Input Video**

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/examples/flower/input.gif)

➜

**Output Video**

![](https://github.com/tau-yihouxiang/EX-4D/raw/main/examples/flower/output.gif)

### User Study Results

- **70.7%** of participants preferred EX-4D over baseline methods
- Superior performance in physical consistency and extreme viewpoint quality
- Significant improvement as camera angles become more extreme

## 🎯 Applications

- **🎮 Gaming**: Create immersive 3D game cinematics from 2D footage
- **🎬 Film Production**: Generate novel camera angles for post-production
- **🥽 VR/AR**: Create free-viewpoint video experiences
- **📱 Social Media**: Generate dynamic camera movements for content creation
- **🏢 Architecture**: Visualize spaces from multiple viewpoints

## ⚠️ Limitations

- **Depth Dependency**: Performance relies on monocular depth estimation quality
- **Computational Cost**: Requires significant computation for high-resolution videos
- **Reflective Surfaces**: Challenges with reflective or transparent materials

## 🔮 Future Work

- Real-time inference optimization (3DGS / 4DGS)
- Support for higher resolutions (1K, 2K)
- Neural mesh refinement techniques

## 🙏 Acknowledgments

We would like to thank the [DiffSynth-Studio v1.1.1](https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1) project for providing the foundational diffusion framework.

## 📚 Citation

If you find our work useful, please consider citing:

```
@misc{hu2025ex4dextremeviewpoint4d,
      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, 
      author={Tao Hu and Haoyang Peng and Xiao Liu and Yuewen Ma},
      year={2025},
      eprint={2506.05554},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05554}, 
}
```