# Appleì˜ DiffuCoder í™•ì‚° ëª¨ë¸ì„ ì‚¬ìš©í•œ ì½”ë“œ ìƒì„±

ë°œê²¬ì¼: 2025/07/09
ì›ë¬¸ URL: https://github.com/apple/ml-diffucoder
ë¶„ë¥˜: ì˜¤í”ˆì†ŒìŠ¤
ì›ë¬¸ Source: ğŸ”—github
ì¦ê²¨ì°¾ê¸°: No

## Masked Diffusion Models for Code Generation
ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ë§ˆìŠ¤í‚¹ëœ í™•ì‚° ëª¨ë¸

    

[](https://camo.githubusercontent.com/f191a33a959369475e8c3743c17bdd2d069fdaee1abd8bf3cf042ebd1844e5ea/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d41727869762532304c696e6b2d677265656e)

[](https://camo.githubusercontent.com/14e900c44228cffa80904c2e05a53ca33d84a63d198c411a9f8e67b008594322/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170706c652d626c7565)

[](https://camo.githubusercontent.com/92d7302321a11da62ca18a3261e59653a8bd21c98a0519205c134ba22eaa95c2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48756767696e67253230466163652d4469666675436f6465725f426173652d464645423342)

[](https://camo.githubusercontent.com/29eaa740d50677ab100f9430003418fc85f2ff2c9d98c1f756c72f7269d2c15b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48756767696e67253230466163652d4469666675436f6465725f496e7374727563742d464645423342)

[](https://camo.githubusercontent.com/6bae1026ea3606363463b7109ae30993b6bcac1f610a9709a5ec05d0179892f0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48756767696e67253230466163652d4469666675436f6465725f63704752504f2d464645423342)

This software project accompanies the research paper, [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639).
ì´ ì†Œí”„íŠ¸ì›¨ì–´ í”„ë¡œì íŠ¸ëŠ” ì—°êµ¬ ë…¼ë¬¸ì¸ [DiffuCoder: ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ë§ˆìŠ¤í‚¹ í™•ì‚° ëª¨ë¸ ì´í•´ ë° ê°œì„ ](https://arxiv.org/abs/2506.20639)ê³¼ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤.

### Updates ì—…ë°ì´íŠ¸

**MLX support on Apple Silicon is in progress. We will make necessary updates to the repository once it is available.**
**Apple Siliconì—ì„œ MLX ì§€ì›ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì €ì¥ì†Œê°€ ì‚¬ìš© ê°€ëŠ¥í•´ì§€ë©´ í•„ìš”í•œ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤.**

- July 8, 2025. We are still training for 14B models, to be updated.
2025ë…„ 7ì›” 8ì¼. ìš°ë¦¬ëŠ” ì—¬ì „íˆ ì—…ë°ì´íŠ¸ë  14B ëª¨ë¸ì— ëŒ€í•œ í›ˆë ¨ ì¤‘ì…ë‹ˆë‹¤.
- July 7, 2025. [MLX community implementation of DiffuCoder 8bit model](https://huggingface.co/mlx-community/DiffuCoder-7B-cpGRPO-8bit)
2025ë…„ 7ì›” 7ì¼. [DiffuCoder 8ë¹„íŠ¸ ëª¨ë¸ì˜ MLX ì»¤ë®¤ë‹ˆí‹° êµ¬í˜„](https://huggingface.co/mlx-community/DiffuCoder-7B-cpGRPO-8bit)
- July 4, 2025. [MLX](https://github.com/ml-explore/mlx) support in progress. To preview or contribute, please check out this PR started by @Goekdeniz-Guelmez:
2025ë…„ 7ì›” 4ì¼. [MLX](https://github.com/ml-explore/mlx) ì§€ì›ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ë¯¸ë¦¬ ë³´ê±°ë‚˜ ê¸°ì—¬í•˜ë ¤ë©´ @Goekdeniz-Guelmezê°€ ì‹œì‘í•œ PRì„ í™•ì¸í•˜ì„¸ìš”.[this PR](https://github.com/ml-explore/mlx-lm/pull/270)
- July 4, 2025. Update inference usage/examples/demo.
2025ë…„ 7ì›” 4ì¼. ì¶”ë¡  ì‚¬ìš©/ì˜ˆì œ/ë°ëª¨ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
- July 2, 2025. Models are available on Huggingface.
2025ë…„ 7ì›” 2ì¼. ëª¨ë¸ì€ Huggingfaceì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- July 1, 2025. Code is available.
2025ë…„ 7ì›” 1ì¼. ì½”ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![](https://github.com/apple/ml-diffucoder/raw/main/figs/teaser.png)

### Motivation ë™ê¸°

Scaling upon Masked Denoising Models (MDMs), diffusion LLMs (dLLMs) such as [LLaDA](https://ml-gsai.github.io/LLaDA-demo/) and [Dream](https://hkunlp.github.io/blog/2025/dream/) have achieved performance on par with similarly sized autoregressive (AR) LLMs across many benchmarks. Recent commercial-scale dLLMs like [Mercury](https://chat.inceptionlabs.ai/) and [Gemini](https://deepmind.google/models/gemini-diffusion/) further demonstrate that diffusion-based code generators can rival top AR code models on programming tasks while offering faster text generation.
ë§ˆìŠ¤í‚¹ ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸(MDM), [LLaDA](https://ml-gsai.github.io/LLaDA-demo/) ë° [Dream](https://hkunlp.github.io/blog/2025/dream/)ê³¼ ê°™ì€ í™•ì‚° LLM(dLLM)ì— ëŒ€í•œ í™•ì¥ì€ ë§ì€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìœ ì‚¬í•œ í¬ê¸°ì˜ ìê°€íšŒê·€(AR) LLMê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. [Mercury](https://chat.inceptionlabs.ai/) ë° [Gemini](https://deepmind.google/models/gemini-diffusion/)ì™€ ê°™ì€ ìµœê·¼ì˜ ìƒìš© ê·œëª¨ dLLMì€ í™•ì‚° ê¸°ë°˜ ì½”ë“œ ìƒì„±ê¸°ê°€ í”„ë¡œê·¸ë˜ë° ì‘ì—…ì—ì„œ ìµœê³ ì˜ AR ì½”ë“œ ëª¨ë¸ê³¼ ê²½ìŸí•˜ëŠ” ë™ì‹œì— ë” ë¹ ë¥¸ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì œê³µí•  ìˆ˜ ìˆìŒì„ ë”ìš± ë³´ì—¬ì¤ë‹ˆë‹¤.

However, the generation pattern and post-training strategies of dLLMs remain under-explored. In this work, we investigate the following questions:
ê·¸ëŸ¬ë‚˜ dLLMì˜ ìƒì„± íŒ¨í„´ê³¼ í›ˆë ¨ í›„ ì „ëµì€ ì•„ì§ ì—°êµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì—ì„œëŠ” ë‹¤ìŒ ì§ˆë¬¸ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤.

- **How does the generation pattern of dLLMs differ from AR models?**
**dLLMì˜ ìƒì„± íŒ¨í„´ì€ AR ëª¨ë¸ê³¼ ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€ìš”?**
- **What is the difference in modeling different data modalities, such as code vs. math?**
**ì½”ë“œì™€ ìˆ˜í•™ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ë°ì´í„° ì–‘ì‹ì„ ëª¨ë¸ë§í•  ë•Œ ì–´ë–¤ ì°¨ì´ì ì´ ìˆë‚˜ìš”?**
- **How diverse can dLLMs be, and how should post-training be designed?**
**dLLMì€ ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬í›„ êµìœ¡ì€ ì–´ë–»ê²Œ ì„¤ê³„í•´ì•¼ í•©ë‹ˆê¹Œ?**

![](https://github.com/apple/ml-diffucoder/raw/main/figs/pipeline.png)

We train **DiffuCoder** using the adaptation approach in [DiffuLLaMA](https://github.com/HKUNLP/DiffuLLaMA) and introduce a new metric â€” **autoregressiveness score** â€” to quantify the causal pattern during dLLM generation. The key findings are listed below.
[DiffuLLaMA](https://github.com/HKUNLP/DiffuLLaMA)ì˜ ì ì‘ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ **DiffuCoder**ë¥¼ í›ˆë ¨í•˜ê³  dLLM ìƒì„± ì¤‘ ì¸ê³¼ íŒ¨í„´ì„ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì§€í‘œì¸ **ìê¸° íšŒê·€ ì ìˆ˜**ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ì£¼ìš” ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

### Findings ê²°ê³¼

- dLLMs still exhibit a left-to-right bias due to the nature of text, but they can also break this strict order in AR models.
dLLMì€ í…ìŠ¤íŠ¸ì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ì—¬ì „íˆ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í¸í–¥ë˜ì§€ë§Œ AR ëª¨ë¸ì—ì„œ ì´ëŸ¬í•œ ì—„ê²©í•œ ìˆœì„œë¥¼ ê¹¨ëœ¨ë¦´ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
- After pre-training, we show that code tasks induce less global AR-ness than math.
ì‚¬ì „ í›ˆë ¨ í›„ ì½”ë“œ ì‘ì—…ì´ ìˆ˜í•™ë³´ë‹¤ ê¸€ë¡œë²Œ ARì„±ì„ ëœ ìœ ë„í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- In dLLMs, changing the sampling temperature not only affects sampled tokens (as in AR models), but also alters the generation order itself.
dLLMì—ì„œ ìƒ˜í”Œë§ ì˜¨ë„ë¥¼ ë³€ê²½í•˜ë©´ ìƒ˜í”Œë§ëœ í† í°(AR ëª¨ë¸ì—ì„œì™€ ê°™ì´ )ì— ì˜í–¥ì„ ë¯¸ì¹  ë¿ë§Œ ì•„ë‹ˆë¼ ìƒì„± ìˆœì„œ ìì²´ë„ ë³€ê²½ë©ë‹ˆë‹¤.

For more interesting findings, please refer to our original paper!
ë” í¥ë¯¸ë¡œìš´ ê²°ê³¼ë¥¼ ì›í•˜ì‹œë©´ ì›ë³¸ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ì„¸ìš”!

We propose **Coupled-GRPO**, a post-training method to improve DiffuCoder's performance.
DiffuCoderì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì‚¬í›„ í•™ìŠµ ë°©ë²•ì¸ **Coupled-GRPO**ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

---

### Coupled-GRPO ê²°í•©-GRPO

In diffusion LLMs, the per-timestep loss L t typically computes log-probabilities only at masked token positions, which leads to inefficiency and high variance when sampling is limited. To address this, **Coupled-GRPO** introduces a *coupled-sampling* scheme:
í™•ì‚° LLMì—ì„œ ì‹œê°„ë‹¨ê³„ ì†ì‹¤ L t ì€ ì¼ë°˜ì ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í° ìœ„ì¹˜ì—ì„œë§Œ ë¡œê·¸ í™•ë¥ ì„ ê³„ì‚°í•˜ë¯€ë¡œ ìƒ˜í”Œë§ì´ ì œí•œë  ë•Œ ë¹„íš¨ìœ¨ì„±ê³¼ ë†’ì€ ë¶„ì‚°ì´ ë°œìƒí•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Coupled-GRPO**ëŠ” *ê²°í•© ìƒ˜í”Œë§* ì²´ê³„ë¥¼ ë„ì…í•©ë‹ˆë‹¤.

- For each training example, we select Î» pairs of timesteps ( t , t ^ ) such that t + t ^ = T .
ê° í›ˆë ¨ ì˜ˆì œì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ì‹œê°„ ë‹¨ê³„ ( t , t ) ìŒì„ ì„ íƒí•©ë‹ˆë‹¤ Î» . t + t = T
- We apply two complementary token masks â€” each mask hides part of the tokens, and together they cover the entire set of target tokens.
ë‘ ê°œì˜ ë³´ì™„ì ì¸ í† í° ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•©ë‹ˆë‹¤ â€” ê° ë§ˆìŠ¤í¬ëŠ” í† í°ì˜ ì¼ë¶€ë¥¼ ìˆ¨ê¸°ê³  í•¨ê»˜ ì „ì²´ ëŒ€ìƒ í† í° ì„¸íŠ¸ë¥¼ ë®ìŠµë‹ˆë‹¤.
- As a result, every token is unmasked in exactly one of the two forward passes.
ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë“  í† í°ì€ ë‘ ê°œì˜ ì „ë‹¬ íŒ¨ìŠ¤ ì¤‘ ì •í™•íˆ í•˜ë‚˜ì—ì„œ ë§ˆìŠ¤í‚¹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

This ensures that: ì´ë¥¼ í†µí•´ ë‹¤ìŒì„ ë³´ì¥í•©ë‹ˆë‹¤.

1. Every token's log-probability is computed at least once, providing a non-zero learning signal for all tokens.
ëª¨ë“  í† í°ì˜ ë¡œê·¸ í™•ë¥ ì€ ì ì–´ë„ í•œ ë²ˆ ê³„ì‚°ë˜ì–´ ëª¨ë“  í† í°ì— ëŒ€í•´ 0ì´ ì•„ë‹Œ í•™ìŠµ ì‹ í˜¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
2. The probability estimates are more accurate, since each token is evaluated in a realistic partially-masked context (rather than always being fully masked).
í™•ë¥  ì¶”ì •ì¹˜ëŠ” ê° í† í°ì´ í•­ìƒ ì™„ì „íˆ ë§ˆìŠ¤í‚¹ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë¶€ë¶„ì ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ëœ í˜„ì‹¤ì ì¸ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í‰ê°€ë˜ê¸° ë•Œë¬¸ì— ë” ì •í™•í•©ë‹ˆë‹¤.
3. The scheme effectively uses 2 Î» times more sampling passes than the baseline (we choose Î» = 1 ), improving estimation with modest computational overhead.
ì´ ì²´ê³„ëŠ” ê¸°ì¤€ì„ (ìš°ë¦¬ê°€ ì„ íƒ) Î» = 1 ë³´ë‹¤ ëª‡ ë°° ë” ë§ì€ ìƒ˜í”Œë§ íŒ¨ìŠ¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ 2 Î» ì ë‹¹í•œ ê³„ì‚° ì˜¤ë²„í—¤ë“œë¡œ ì¶”ì •ì„ ê°œì„ í•©ë‹ˆë‹¤.

In this repository, we release our implementation of **Coupled-GRPO**, built upon [open-r1](https://github.com/huggingface/open-r1/blob/6a0cd5c8ad031fc75118a4ce7f42a4860c3d8dea/).
ì´ ì €ì¥ì†Œì—ì„œëŠ” [open-r1](https://github.com/huggingface/open-r1/blob/6a0cd5c8ad031fc75118a4ce7f42a4860c3d8dea/)ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ **Coupled-GRPO**ì˜ êµ¬í˜„ì„ ë¦´ë¦¬ìŠ¤í•©ë‹ˆë‹¤.

### Getting Started ì‹œì‘

```
â”œâ”€â”€ run.sh # start training
â”œâ”€â”€ setup.py # modified open-r1/setup.py
â”œâ”€â”€ src/open_r1/ #  our code based on open-r1
â”‚   â”œâ”€â”€ configs.py # with diffusion related params
â”‚   â”œâ”€â”€ coupled_grpo.py # inherits trl GRPOTrainer 
â”‚   â”œâ”€â”€ grpo.py # main training script
â”‚   â”œâ”€â”€ rewards.py # rewrite code reward and code_format reward 
â”‚   â”œâ”€â”€ utils/code_providers.py # rewrite pass rate extraction for E2B
â”œâ”€â”€ recipes/process_data.py # prepare grpo training data
â”œâ”€â”€ recipes/config_coupled_code.yaml # training config
â”œâ”€â”€ tests/test_code_reward.py # test sandbox execution for code
```

### 1. Prepare code and environment

1. ì½”ë“œ ë° í™˜ê²½ ì¤€ë¹„

Clone the source code of Open-R1 from `git clone https://github.com/huggingface/open-r1`. Merge and replace files between ours and Open-R1's (including `setup.py`).
ì—ì„œ Open-R1ì˜ ì†ŒìŠ¤ ì½”ë“œë¥¼ ë³µì œí•©ë‹ˆë‹¤ `git clone https://github.com/huggingface/open-r1` . ë‹¹ì‚¬ì™€ Open-R1(`setup.py` í¬í•¨) ê°„ì˜ íŒŒì¼ì„ ë³‘í•©í•˜ê³  êµì²´í•©ë‹ˆë‹¤.

Set up the environment and dependencies following Open-R1:
Open-R1 ë‹¤ìŒì— í™˜ê²½ ë° ì¢…ì†ì„±ì„ ì„¤ì •í•©ë‹ˆë‹¤.

```
env=openr1
conda create -n $env python=3.11 -y -c anaconda
conda activate $env
pip install vllm==0.8.4
pip install setuptools
pip install flash-attn==2.8.0.post1 --no-build-isolation
pip install -e ".[code]"
```

Prepare a code sandbox at [E2B](https://e2b.dev/). Export your E2B token to `E2B_API_KEY` environment variable. Log in to wandb and export your `WANDB_ENTITY`.
[E2B](https://e2b.dev/)ì—ì„œ ì½”ë“œ ìƒŒë“œë°•ìŠ¤ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. E2B í† í°ì„ `E2B_API_KEY` í™˜ê²½ ë³€ìˆ˜ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤. wandbì— ë¡œê·¸ì¸í•˜ê³  `WANDB_ENTITY` ë‚´ë³´ëƒ…ë‹ˆë‹¤.

### 2. Data preparation 2. ë°ì´í„° ì¤€ë¹„

We prepare a hard split of GRPO training data based on [AceCode-89k](https://huggingface.co/datasets/TIGER-Lab/AceCode-87K).
[AceCode-89k](https://huggingface.co/datasets/TIGER-Lab/AceCode-87K)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GRPO í›ˆë ¨ ë°ì´í„°ì˜ í•˜ë“œ ë¶„í• ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

```
cd recipes
python process_data.py --dataset_path "TIGER-Lab/AceCode-89K" --output_path "./acecode_hard.jsonl" --difficulty "hard"
```

### 3. Start GRPO training 3. GRPO êµìœ¡ ì‹œì‘

```
cd ..
bash run.sh 
# in `run.sh`, we start e2b server locally, but you can also run it on CPU clusters.
```

### Inference ì¶”ë¡ 

The DiffuCoder models ([Base](https://huggingface.co/apple/DiffuCoder-7B-Base), [Instruct](https://huggingface.co/apple/DiffuCoder-7B-Instruct), and [cpGRPO](https://huggingface.co/apple/DiffuCoder-7B-cpGRPO)) are now available on HuggingFace. Change `TOKEN_PER_STEP` to trade off between performance and speed.
ì´ì œ DiffuCoder ëª¨ë¸([Base](https://huggingface.co/apple/DiffuCoder-7B-Base), [Instruct](https://huggingface.co/apple/DiffuCoder-7B-Instruct) ë° [cpGRPO)](https://huggingface.co/apple/DiffuCoder-7B-cpGRPO)ì„ HuggingFaceì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ê³¼ ì†ë„ ì‚¬ì´ì—ì„œ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´ `TOKEN_PER_STEP`ë¥¼ ë³€ê²½í•˜ì‹­ì‹œì˜¤.

Usage for Base model (click to expand)
ê¸°ë³¸ ëª¨ë¸ì— ëŒ€í•œ ì‚¬ìš©(í™•ì¥í•˜ë ¤ë©´ í´ë¦­)

```
import torch
from transformers import AutoModel, AutoTokenizer
model_path = "apple/DiffuCoder-7B-Base"
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = model.to("cuda").eval()
prompt = """
from typing import List

def has_close_elements(numbers: List[float], threshold: float) -> bool:
    \"\"\"
    Check if in given list of numbers, are any two numbers closer to each other than given threshold.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    \"\"\"
"""
TOKEN_PER_STEP = 1 # diffusion timesteps * TOKEN_PER_STEP = total new tokens
inputs = tokenizer(prompt, return_tensors="pt")
input_ids = inputs.input_ids.to(device="cuda")
attention_mask = inputs.attention_mask.to(device="cuda")
output = model.diffusion_generate(
    input_ids,
    attention_mask=attention_mask,
    max_new_tokens=256,
    output_history=True,
    return_dict_in_generate=True,
    steps=256//TOKEN_PER_STEP,
    temperature=0.2,
    top_p=0.95,
    alg="entropy",
    alg_temp=0.,
)
generations = [
    tokenizer.decode(g[len(p) :].tolist())
    for p, g in zip(input_ids, output.sequences)
]
print(generations[0].split(tokenizer.eos_token)[0])
```

Output (click to expand)
ì¶œë ¥(í™•ì¥í•˜ë ¤ë©´ í´ë¦­)

```
    # Sort the list to make it easier to find close elements
    numbers.sort()
    # Iterate through the list, checking each adjacent pair
    for i in range(len(numbers) - 1):
        # If the difference between the current and next element is less than the threshold, return True
        if numbers[i + 1] - numbers[i] < threshold:
            return True
    # If no such pair is found, return False
    return False 
```

> Given an example input from the HumanEval test, the output of DiffuCoder-Base is a direct completion of the code snippet.
HumanEval í…ŒìŠ¤íŠ¸ì˜ ì˜ˆì œ ì…ë ¥ì´ ì£¼ì–´ì§€ë©´ DiffuCoder-Baseì˜ ì¶œë ¥ì€ ì½”ë“œ ì¡°ê°ì˜ ì§ì ‘ ì™„ì„±ì…ë‹ˆë‹¤.
> 

Usage for Instruct model (click to expand)
Instruct ëª¨ë¸ ì‚¬ìš©ë²•(í™•ì¥í•˜ë ¤ë©´ í´ë¦­)

```
import torch
from transformers import AutoModel, AutoTokenizer
model_path = "apple/DiffuCoder-7B-cpGRPO"
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = model.to("cuda").eval()
query = "Write a function to find the shared elements from the given two lists."
prompt = f"""<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{query.strip()}
<|im_end|>
<|im_start|>assistant
""" ## following the template of qwen; you can also use apply_chat_template function
TOKEN_PER_STEP = 1 # diffusion timesteps * TOKEN_PER_STEP = total new tokens
inputs = tokenizer(prompt, return_tensors="pt")
input_ids = inputs.input_ids.to(device="cuda")
attention_mask = inputs.attention_mask.to(device="cuda")
output = model.diffusion_generate(
    input_ids,
    attention_mask=attention_mask,
    max_new_tokens=256,
    output_history=True,
    return_dict_in_generate=True,
    steps=256//TOKEN_PER_STEP,
    temperature=0.4,
    top_p=0.95,
    alg="entropy",
    alg_temp=0.,
)
generations = [
    tokenizer.decode(g[len(p) :].tolist())
    for p, g in zip(input_ids, output.sequences)
]
print(generations[0].split('<|dlm_pad|>')[0])
```

Output (click to expand)
ì¶œë ¥(í™•ì¥í•˜ë ¤ë©´ í´ë¦­)

```
Here is the code to solve this problem: 
```python
def shared_elements(list1, list2): 
  return [value for value in list1 if value in  list2] 
```<|im_end|> 
```

> Given an example input from the MBPP test, the output of DiffuCoder-cpGRPO is a chat-based response.
MBPP í…ŒìŠ¤íŠ¸ì˜ ì˜ˆì œ ì…ë ¥ì´ ì£¼ì–´ì§€ë©´ DiffuCoder-cpGRPOì˜ ì¶œë ¥ì€ ì±„íŒ… ê¸°ë°˜ ì‘ë‹µì…ë‹ˆë‹¤.
> 

### Demo ë°ëª¨

ğŸš€ Start the demo and enter any prompt you want!
ğŸš€ ë°ëª¨ë¥¼ ì‹œì‘í•˜ê³  ì›í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”!

```
python inference_demo.py
```

### Evaluation í‰ê°€

The diffusion inference algorithm is based on Dream-7B. The code evaluation is based on [Qwen2.5-Coder](https://github.com/QwenLM/Qwen2.5-Coder/tree/main/qwencoder-eval).
í™•ì‚° ì¶”ë¡  ì•Œê³ ë¦¬ì¦˜ì€ Dream-7Bë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì½”ë“œ í‰ê°€ëŠ” [Qwen2.5-Coder](https://github.com/QwenLM/Qwen2.5-Coder/tree/main/qwencoder-eval)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

## Acknowledgments ìŠ¹ì¸ì„

We sincerely appreciate the following works for DiffuCoder:
DiffuCoderì— ëŒ€í•œ ë‹¤ìŒ ì‘ì—…ì— ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.

- Our data used in pre-training/mid-training/instruction tuning are from [OpenCoder](https://huggingface.co/OpenCoder-LLM).
ì‚¬ì „ í•™ìŠµ/ì¤‘ê°„ í•™ìŠµ/ëª…ë ¹ì–´ íŠœë‹ì— ì‚¬ìš©ëœ ë°ì´í„°ëŠ” [OpenCoder](https://huggingface.co/OpenCoder-LLM)ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤.
- Our instruction tuning code is based on [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).
ìš°ë¦¬ì˜ ëª…ë ¹ì–´ íŠœë‹ ì½”ë“œëŠ” [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.
- Our coupled-GRPO is built upon [Open-R1](https://github.com/huggingface/open-r1) and [d1](https://github.com/dllm-reasoning/d1).
ë‹¹ì‚¬ì˜ ê²°í•© GRPOëŠ” [Open-R1](https://github.com/huggingface/open-r1) ë° [d1](https://github.com/dllm-reasoning/d1)ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.
- Our evaluation is built upon [Dream](https://github.com/HKUNLP/Dream) and [Qwen2.5-Coder](https://github.com/QwenLM/Qwen2.5-Coder/tree/main/qwencoder-eval).
ìš°ë¦¬ì˜ í‰ê°€ëŠ” [Dream](https://github.com/HKUNLP/Dream)ê³¼ [Qwen2.5-Coder](https://github.com/QwenLM/Qwen2.5-Coder/tree/main/qwencoder-eval)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

## Citation ì¸ìš©

```
@article{gong2025diffucoder,
  title={DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation},
  author={Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang},
  year={2025},
  eprint={2506.20639},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2506.20639},
}
```