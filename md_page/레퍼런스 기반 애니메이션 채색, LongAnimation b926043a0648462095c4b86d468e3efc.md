# ë ˆí¼ëŸ°ìŠ¤ ê¸°ë°˜ ì• ë‹ˆë©”ì´ì…˜ ì±„ìƒ‰, LongAnimation

ë°œê²¬ì¼: 2025/07/07
ì›ë¬¸ URL: https://github.com/CN-makers/LongAnimation
ë¶„ë¥˜: ì˜¤í”ˆì†ŒìŠ¤
ì›ë¬¸ Source: ğŸ”—github
ì¦ê²¨ì°¾ê¸°: No

[](https://opengraph.githubassets.com/86cfe0b61cbdf16e0bba9a8f0fe420241a3e3ee6a657e5e2d3978b5e0108e81e/CN-makers/LongAnimation)

# LongAnimation: Long Animation Generation with Dynamic Global-Local Memory
LongAnimation: ë™ì  ì „ì—­-ë¡œì»¬ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•œ ê¸´ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±

  

[](https://camo.githubusercontent.com/127d20f716b7227446bc8af440e5a6b695d1f6e87e4c1dd3c33098969a2306ca/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d50726f6a656374266d6573736167653d5765627369746526636f6c6f723d626c7565)

[](https://camo.githubusercontent.com/4721e7c059058929d64409fc70e8c46871098ff44e8d0a4e147c7c5e082cbc81/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323035372e30313934352d6233316231622e737667)

[](https://camo.githubusercontent.com/41acc901d5ed8336e63cb54e3895314b14b3c136badab81b8a09d580d32e3b09/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368652d79656c6c6f77)

video.mp4

> [**LongAnimation: Long Animation Generation with Dynamic Global-Local Memory**](https://cn-makers.github.io/long_animation_web/)
[**LongAnimation: ë™ì  ì „ì—­-ë¡œì»¬ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•œ ê¸´ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±**](https://cn-makers.github.io/long_animation_web/)
> 

[Nan Chen](https://openreview.net/profile?id=~Nan_Chen13)1, [Mengqi Huang](https://corleone-huang.github.io/)1, [Yihao Meng](https://yihao-meng.github.io/)2, [Zhendong Mao](https://faculty.ustc.edu.cn/maozhendong/en/index.htm)â€ ,1
[ë‚œ ì²¸](https://openreview.net/profile?id=~Nan_Chen13)1, [ë©©ì¹˜ í™©](https://corleone-huang.github.io/)1, [ì´í•˜ì˜¤ ë©©](https://yihao-meng.github.io/)2, [ì  ë™ ë§ˆì˜¤](https://faculty.ustc.edu.cn/maozhendong/en/index.htm)â€ ,1
1USTC 2HKUST â€ corresponding author
1 USTC 2HKUST â€ êµì‹ ì €ì

> Existing studies are limited to short-term colorization by fusing overlapping features to achieve smooth transitions, which fails to maintain long-term color consistency. In this study, we propose a dynamic global-local paradigm to achieve ideal long-term color consistency by dynamically extracting global color-consistent features relevant to the current generation.
ê¸°ì¡´ ì—°êµ¬ëŠ” ë¶€ë“œëŸ¬ìš´ ì „í™˜ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ê²¹ì¹˜ëŠ” íŠ¹ì§•ì„ ìœµí•©í•˜ì—¬ ë‹¨ê¸°ì ì¸ ìƒ‰ìƒí™”ì— êµ­í•œë˜ì–´ ì¥ê¸°ì ì¸ ìƒ‰ìƒ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” í˜„ì¬ ì„¸ëŒ€ì™€ ê´€ë ¨ëœ ê¸€ë¡œë²Œ ìƒ‰ìƒ ì¼ê´€ì„± íŠ¹ì§•ì„ ë™ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ì´ìƒì ì¸ ì¥ê¸° ìƒ‰ìƒ ì¼ê´€ì„±ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë™ì  ê¸€ë¡œë²Œ-ë¡œì»¬ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•©ë‹ˆë‹¤.
> 

ğŸ‰ğŸ‰ Our paper, â€œLongAnimation: Long Animation Generation with Dynamic Global-Local Memoryâ€ accepted by ICCV 2025! Looking forward to seeing you at ICCV then! **Strongly recommend seeing our [demo page](https://cn-makers.github.io/long_animation_web/).**
ğŸ‰ğŸ‰ ìš°ë¦¬ì˜ ë…¼ë¬¸ "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory"ê°€ ICCV 2025ì— ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤! ê·¸ë•Œ ICCVì—ì„œ ëµ™ê¸°ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤! [**ë°ëª¨ í˜ì´ì§€ë¥¼](https://cn-makers.github.io/long_animation_web/) ì°¸ì¡°í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.**

## Showcase ì‡¼ì¼€ì´ìŠ¤

showcase_1.mp4 showcase_2.mp4 showcase_3.mp4

## Creative usage ì°½ì˜ì ì¸ ì‚¬ìš©

### Text-guided Background Generation

í…ìŠ¤íŠ¸ ì•ˆë‚´ ë°°ê²½ ìƒì„±

text_1.mp4 text_2.mp4 text_3.mp4

*A boy and a girl in different environment.*
*ë‹¤ë¥¸ í™˜ê²½ì— ìˆëŠ” ì†Œë…„ê³¼ ì†Œë…€.*

## TODO List ALL ëª©ë¡

- Release the paper and demo page. Visit [https://cn-makers.github.io/long_animation_web/](https://cn-makers.github.io/long_animation_web/)
ë…¼ë¬¸ ë° ë°ëª¨ í˜ì´ì§€ë¥¼ ê³µê°œí•©ë‹ˆë‹¤. [ë°©ë¬¸ https://cn-makers.github.io/long_animation_web/](https://cn-makers.github.io/long_animation_web/)
- Release the code.
ì½”ë“œë¥¼ í•´ì œí•©ë‹ˆë‹¤.

## Requirements ìš”êµ¬ ì‚¬í•­

The training is conducted on 6 A100 GPUs (80GB VRAM), the inference is tested on 1 A100 GPU.
í›ˆë ¨ì€ 6ê°œì˜ A100 GPU(80GB VRAM)ì—ì„œ ìˆ˜í–‰ë˜ë©° ì¶”ë¡ ì€ 1ê°œì˜ A100 GPUì—ì„œ í…ŒìŠ¤íŠ¸ë©ë‹ˆë‹¤.

## Setup ì„¤ì¹˜

```
git clone https://github.com/CN-makers/LongAnimation
cd LongAnimation
```

## Environment í™˜ê²½

All the tests are conducted in Linux. We suggest running our code in Linux. To set up our environment in Linux, please run:
ëª¨ë“  í…ŒìŠ¤íŠ¸ëŠ” Linuxì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤. Linuxì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. Linuxì—ì„œ í™˜ê²½ì„ ì„¤ì •í•˜ë ¤ë©´ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤.

```
conda create -n LongAnimation python=3.10 -y
conda activate LongAnimation
bash install.sh
```

## Checkpoints ê²€ì‚¬ì 

1. please download the pre-trained CogVideoX-1.5 I2V checkpoints from [here](https://huggingface.co/THUDM/CogVideoX1.5-5B-I2V), and put the whole folder under `pretrained_weight`, it should look like `./pretrained_weights/CogVideoX1.5-5B-I2V`
[ì—¬ê¸°ì—ì„œ](https://huggingface.co/THUDM/CogVideoX1.5-5B-I2V) ì‚¬ì „ í›ˆë ¨ëœ CogVideoX-1.5 I2V ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²´ í´ë”ë¥¼ `pretrained_weight` ì•„ë˜ì— ë„£ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. `./pretrained_weights/CogVideoX1.5-5B-I2V`
2. please download the pre-trained long video understanding model Video-XL checkpoints from [here](https://huggingface.co/sy1998/Video_XL/tree/main), and put the whole folder under `pretrained_weight`, it should look like `./pretrained_weights/videoxl`
[ì—¬ê¸°ì—ì„œ](https://huggingface.co/sy1998/Video_XL/tree/main) ì‚¬ì „ í›ˆë ¨ëœ ê¸´ ë¹„ë””ì˜¤ ì´í•´ ëª¨ë¸ Video-XL ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²´ í´ë”ë¥¼ `pretrained_weight` ì•„ë˜ì— ë†“ìœ¼ë©´ `./pretrained_weights/videoXLê³¼` ê°™ì´ í‘œì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
3. please download the checkpoint for our SketchDiT and DGLM model from [here](https://huggingface.co/CNcreator0331/LongAnimation/tree/main), and put the whole folder as `./pretrained_weights/longanimation`.
ì—¬ê¸°ì—ì„œ SketchDiT ë° DGLM ëª¨ë¸ì— ëŒ€í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²´ í´ë”ë¥¼ `./pretrained_weights/longanimation` .

## Generate Your Animation! ì• ë‹ˆë©”ì´ì…˜ì„ ìƒì„±í•˜ì„¸ìš”!

To colorize the target lineart sequence with a specific character design, you can run the following command:
íŠ¹ì • ìºë¦­í„° ë””ìì¸ìœ¼ë¡œ ëŒ€ìƒ ì„ í™” ì‹œí€€ìŠ¤ë¥¼ ìƒ‰ì¹ í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
bash  long_animation_inference.sh
```

We provide some test cases in `test_json` folder. You can also try our model with your own data. You can change the lineart sequence and corresponding character design in the script `Long_animation_inference.sh`.
í´ë”ì— ëª‡ ê°€ì§€ í…ŒìŠ¤íŠ¸ ì‚¬ë¡€`test_json` ì œê³µí•©ë‹ˆë‹¤. ìì‹ ì˜ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•´ ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ `Long_animation_inference.sh`ì—ì„œ ì„ í™” ìˆœì„œì™€ í•´ë‹¹ ìºë¦­í„° ë””ìì¸ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

During the official training, the --height and --weight we used were 576 and 1024 respectively. Additionally, the model can also be compatible with resolutions of 768 in length and 1360 in width respectively.
ê³µì‹ í›ˆë ¨ ê¸°ê°„ ë™ì•ˆ ìš°ë¦¬ê°€ ì‚¬ìš©í•œ --height ë° --weightëŠ” ê°ê° 576ê³¼ 1024ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ ì´ ëª¨ë¸ì€ ê°ê° ê¸¸ì´ 768, ë„ˆë¹„ 1360ì˜ í•´ìƒë„ì™€ë„ í˜¸í™˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Citation:

Don't forget to cite this source if it proves useful in your research!

```
@misc{chen2025longanimationlonganimationgeneration,
      title={LongAnimation: Long Animation Generation with Dynamic Global-Local Memory}, 
      author={Nan Chen and Mengqi Huang and Yihao Meng and Zhendong Mao},
      year={2025},
      eprint={2507.01945},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.01945}, 
}
```

## Star History

[](https://camo.githubusercontent.com/f6a2989efdd2c8720f169f8390ae37a6b3ca24045d4dd136f55cdebd3e8a0d6c/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d434e2d6d616b6572732f4c6f6e67416e696d6174696f6e26747970653d44617465)