# YUME

발견일: 2025/07/31
원문 URL: https://github.com/stdstu12/YUME
분류: 오픈소스
원문 Source: 🔗github
즐겨찾기: No

[](https://opengraph.githubassets.com/ee8fb59693d5b98fb2d7880b4535566592364f09002d20d0e2d9adca1b8cee21/stdstu12/YUME)

![](https://github.com/stdstu12/YUME/raw/main/assets/yume.png)

# YUME: 대화형 세계 생성 모델

## 개요

YUME는 상하이 AI 연구소, 푸단 대학교, 상하이 혁신 연구소의 연구진이 개발한 프로젝트입니다. 이미지, 텍스트, 비디오와 같은 입력을 통해 대화형, 현실적이며 동적인 가상 세계를 생성하며, 키보드와 같은 주변 기기나 뉴럴 신호를 통해 탐색 및 제어가 가능하도록 설계되었습니다. 현재 미리보기 버전은 이미지로부터 동적인 세계를 생성하고 키보드로 탐색할 수 있는 기능을 제공합니다.

- **협업**: 팀은 협업을 환영하며 자기주도적인 인턴을 모집합니다. 연락처: [zhangkaipeng@pjlab.org.cn](mailto:zhangkaipeng@pjlab.org.cn)
- **자원**: 모든 데이터, 코드베이스, 모델 가중치는 [GitHub](https://github.com/stdstu12/YUME)에서 확인 가능합니다.
- **업데이트**: YUME는 완전한 대화형 세계 생성 목표를 향해 매달 업데이트됩니다.

## 기술 프레임워크

YUME는 고품질의 대화형 비디오 세계 생성을 위해 4개의 핵심 구성 요소로 이루어진 프레임워크를 사용합니다:

1. **카메라 모션 양자화 (QCM)**
    - 카메라 궤적을 직관적인 키보드 제어(예: 앞으로, 뒤로, 좌우 이동, 회전, 상하 틸트)로 변환합니다.
    - 추가 학습 모듈 없이 시공간적 맥락을 제어 신호에 포함하여 안정적인 훈련과 사용자 친화적인 상호작용을 지원합니다.
2. **비디오 생성 아키텍처**
    - *마스크드 비디오 디퓨전 트랜스포머 (MVDT)**와 프레임 메모리 모듈을 활용합니다.
    - 텍스트 기반 제어의 한계를 극복하고 긴 시퀀스에서 일관성을 유지하며 무한 오토리그레시브 비디오 생성을 가능하게 합니다.
3. **향상된 샘플링 메커니즘**
    - **아티팩트 방지 메커니즘 (AAM)**: 학습 없이 잠재 표현을 개선하여 시각적 세부 사항을 강화합니다.
    - **시간 여행 SDE (TTS-SDE)**: 미래 프레임 가이던스를 활용한 확률적 미분방정식 기반 샘플링으로 시간적 일관성을 유지합니다.
4. **최적화 가속**
    - 적대적 증류와 캐싱 메커니즘을 결합한 시너지 최적화로 샘플링 효율성을 3배 향상시키며 시각적 충실도를 유지합니다.

## 학습 및 성능

- **데이터셋**: 고품질 **Sekai** 세계 탐험 데이터셋으로 학습되었습니다.
- **결과**: 다양한 장면과 응용 분야에서 뛰어난 성능을 달성했습니다.

## 자원

- **GitHub 저장소**: [https://github.com/stdstu12/YUME](https://github.com/stdstu12/YUME) (데이터, 코드베이스, 모델 가중치 포함).
- **프로젝트 웹사이트**: [https://stdstu12.github.io/YUME-Project/](https://stdstu12.github.io/YUME-Project/).

## 인용

```
@article{mao2025yume,
    title={Yume: 대화형 세계 생성 모델},
    author={Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang},
    journal={arXiv preprint arXiv:2507.17744},
    year={2025}
}

```